{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of Synthetic Data with Latitude and Longitude from a GP with ARD\n",
    "\n",
    "Training and testing data generated by R script are merged in inferring latent theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PACKAGES\n",
    "import warnings\n",
    "import torch\n",
    "from gpytorch.distributions import base_distributions\n",
    "from gpytorch.functions import log_normal_cdf\n",
    "from gpytorch.likelihoods.likelihood import _OneDimensionalLikelihood\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "import gpytorch\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement BinomialLikelihood\n",
    "class BinomialLikelihood(_OneDimensionalLikelihood):\n",
    "    r\"\"\"\n",
    "    Implements the Binomial likelihood for count data y between 1 and m. \n",
    "    The Binomial distribution is parameterized by :math:`m > 0`. \n",
    "    We can write the likelihood as:\n",
    "\n",
    "    .. math::\n",
    "        \\begin{equation*}\n",
    "            p(Y=y|f,m)=\\phi(f)^y(1-\\phi(f))^{(m-y)}\n",
    "        \\end{equation*}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_trials):\n",
    "        super().__init__()\n",
    "        self.n_trials = n_trials\n",
    "\n",
    "    def forward(self, function_samples, **kwargs):\n",
    "        output_probs = base_distributions.Normal(0, 1).cdf(function_samples)\n",
    "        print(output_probs.size())\n",
    "        return base_distributions.Binomial(total_count=self.n_trials, probs=output_probs)\n",
    "\n",
    "    def log_marginal(self, observations, function_dist, *args, **kwargs):\n",
    "        marginal = self.marginal(function_dist, *args, **kwargs)\n",
    "        return marginal.log_prob(observations)\n",
    "\n",
    "    def marginal(self, function_dist, **kwargs):\n",
    "        mean = function_dist.mean\n",
    "        var = function_dist.variance\n",
    "        link = mean.div(torch.sqrt(1 + var))\n",
    "        output_probs = base_distributions.Normal(0, 1).cdf(link)\n",
    "        return base_distributions.Binomial(total_count=self.num_data, probs=output_probs)\n",
    "\n",
    "    def expected_log_prob(self, observations, function_dist, *params, **kwargs):\n",
    "        if torch.any(torch.logical_or(observations.le(-1), observations.ge(self.n_trials+1))):\n",
    "            # Remove after 1.0\n",
    "            warnings.warn(\n",
    "                \"BinomialLikelihood.expected_log_prob expects observations with labels in [0, m]. \"\n",
    "                \"Observations <0 or >m are not allowed.\",\n",
    "                DeprecationWarning,\n",
    "            )\n",
    "        else:\n",
    "            for i in range(observations.size(0)):\n",
    "                observations[i] = torch.clamp(observations[i],0,self.n_trials[i])\n",
    "\n",
    "        # Custom function here so we can use log_normal_cdf rather than Normal.cdf\n",
    "        # This is going to be less prone to overflow errors\n",
    "        log_prob_lambda = lambda function_samples: self.n_trials*log_normal_cdf(-function_samples) + \\\n",
    "                observations.mul(log_normal_cdf(function_samples)-log_normal_cdf(-function_samples))\n",
    "        log_prob = self.quadrature(log_prob_lambda, function_dist)\n",
    "        return log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement GP class\n",
    "class BinomialGPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, train_x):\n",
    "        variational_distribution = CholeskyVariationalDistribution(train_x.size(0))\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self, train_x, variational_distribution, learn_inducing_locations=False\n",
    "        )\n",
    "        super(BinomialGPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.LinearMean(input_size=train_x.size(1))\n",
    "        # ARD kernel for covariate, geospatial and time confounding\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(ard_num_dims=train_x.size(1)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data):\n",
    "    n = data.shape[0]\n",
    "    x = np.zeros((n,11))\n",
    "    y = np.zeros((n,))\n",
    "    theta = np.zeros((n,))\n",
    "    N = np.zeros((n,))\n",
    "    x[:,0] = data[\"latitude\"].to_numpy()\n",
    "    x[:,1] = data[\"longitude\"].to_numpy()\n",
    "    x[:,2] = (data[\"gender\"].to_numpy()==\"Male\")\n",
    "    x[:,3] = (data[\"gender\"].to_numpy()==\"Female\")\n",
    "    x[:,4] = (data[\"gender\"].to_numpy()==\"Non-binary\")\n",
    "    x[:,5] = (data[\"race\"].to_numpy()==\"White\")\n",
    "    x[:,6] = (data[\"race\"].to_numpy()==\"Black\")\n",
    "    x[:,7] = (data[\"race\"].to_numpy()==\"Hispanic\")\n",
    "    x[:,8] = (data[\"race\"].to_numpy()==\"Asian\")\n",
    "    x[:,9] = (data[\"race\"].to_numpy()==\"Other\")\n",
    "    x[:,10] = data[\"year\"].to_numpy()\n",
    "    theta = data[\"theta\"].to_numpy()\n",
    "    y = data[\"Y\"].to_numpy()\n",
    "    N = data[\"n\"].to_numpy()\n",
    "\n",
    "    return torch.from_numpy(x).double(), torch.from_numpy(y).double(),\\\n",
    "            torch.from_numpy(N).double(), theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yahoo/anaconda3/lib/python3.7/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2189.)\n",
      "  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/200 - Loss: 124.942\n",
      "Iter 2/200 - Loss: 248.292\n",
      "Iter 3/200 - Loss: 90.733\n",
      "Iter 4/200 - Loss: 94.112\n",
      "Iter 5/200 - Loss: 118.883\n",
      "Iter 6/200 - Loss: 104.181\n",
      "Iter 7/200 - Loss: 74.668\n",
      "Iter 8/200 - Loss: 53.423\n",
      "Iter 9/200 - Loss: 46.561\n",
      "Iter 10/200 - Loss: 49.427\n",
      "Iter 11/200 - Loss: 54.538\n",
      "Iter 12/200 - Loss: 57.239\n",
      "Iter 13/200 - Loss: 56.460\n",
      "Iter 14/200 - Loss: 53.308\n",
      "Iter 15/200 - Loss: 49.506\n",
      "Iter 16/200 - Loss: 46.327\n",
      "Iter 17/200 - Loss: 44.201\n",
      "Iter 18/200 - Loss: 42.909\n",
      "Iter 19/200 - Loss: 42.019\n",
      "Iter 20/200 - Loss: 41.215\n",
      "Iter 21/200 - Loss: 40.399\n",
      "Iter 22/200 - Loss: 39.636\n",
      "Iter 23/200 - Loss: 39.036\n",
      "Iter 24/200 - Loss: 38.660\n",
      "Iter 25/200 - Loss: 38.484\n",
      "Iter 26/200 - Loss: 38.407\n",
      "Iter 27/200 - Loss: 38.313\n",
      "Iter 28/200 - Loss: 38.121\n",
      "Iter 29/200 - Loss: 37.807\n",
      "Iter 30/200 - Loss: 37.400\n",
      "Iter 31/200 - Loss: 36.956\n",
      "Iter 32/200 - Loss: 36.538\n",
      "Iter 33/200 - Loss: 36.192\n",
      "Iter 34/200 - Loss: 35.937\n",
      "Iter 35/200 - Loss: 35.764\n",
      "Iter 36/200 - Loss: 35.643\n",
      "Iter 37/200 - Loss: 35.544\n",
      "Iter 38/200 - Loss: 35.441\n",
      "Iter 39/200 - Loss: 35.326\n",
      "Iter 40/200 - Loss: 35.203\n",
      "Iter 41/200 - Loss: 35.085\n",
      "Iter 42/200 - Loss: 34.982\n",
      "Iter 43/200 - Loss: 34.899\n",
      "Iter 44/200 - Loss: 34.831\n",
      "Iter 45/200 - Loss: 34.773\n",
      "Iter 46/200 - Loss: 34.714\n",
      "Iter 47/200 - Loss: 34.650\n",
      "Iter 48/200 - Loss: 34.579\n",
      "Iter 49/200 - Loss: 34.504\n",
      "Iter 50/200 - Loss: 34.433\n",
      "Iter 51/200 - Loss: 34.369\n",
      "Iter 52/200 - Loss: 34.317\n",
      "Iter 53/200 - Loss: 34.277\n",
      "Iter 54/200 - Loss: 34.245\n",
      "Iter 55/200 - Loss: 34.219\n",
      "Iter 56/200 - Loss: 34.193\n",
      "Iter 57/200 - Loss: 34.164\n",
      "Iter 58/200 - Loss: 34.131\n",
      "Iter 59/200 - Loss: 34.096\n",
      "Iter 60/200 - Loss: 34.058\n",
      "Iter 61/200 - Loss: 34.021\n",
      "Iter 62/200 - Loss: 33.987\n",
      "Iter 63/200 - Loss: 33.956\n",
      "Iter 64/200 - Loss: 33.928\n",
      "Iter 65/200 - Loss: 33.902\n",
      "Iter 66/200 - Loss: 33.878\n",
      "Iter 67/200 - Loss: 33.855\n",
      "Iter 68/200 - Loss: 33.832\n",
      "Iter 69/200 - Loss: 33.809\n",
      "Iter 70/200 - Loss: 33.786\n",
      "Iter 71/200 - Loss: 33.764\n",
      "Iter 72/200 - Loss: 33.742\n",
      "Iter 73/200 - Loss: 33.720\n",
      "Iter 74/200 - Loss: 33.698\n",
      "Iter 75/200 - Loss: 33.676\n",
      "Iter 76/200 - Loss: 33.655\n",
      "Iter 77/200 - Loss: 33.634\n",
      "Iter 78/200 - Loss: 33.615\n",
      "Iter 79/200 - Loss: 33.597\n",
      "Iter 80/200 - Loss: 33.580\n",
      "Iter 81/200 - Loss: 33.563\n",
      "Iter 82/200 - Loss: 33.546\n",
      "Iter 83/200 - Loss: 33.528\n",
      "Iter 84/200 - Loss: 33.511\n",
      "Iter 85/200 - Loss: 33.494\n",
      "Iter 86/200 - Loss: 33.477\n",
      "Iter 87/200 - Loss: 33.461\n",
      "Iter 88/200 - Loss: 33.445\n",
      "Iter 89/200 - Loss: 33.430\n",
      "Iter 90/200 - Loss: 33.415\n",
      "Iter 91/200 - Loss: 33.400\n",
      "Iter 92/200 - Loss: 33.386\n",
      "Iter 93/200 - Loss: 33.372\n",
      "Iter 94/200 - Loss: 33.358\n",
      "Iter 95/200 - Loss: 33.344\n",
      "Iter 96/200 - Loss: 33.331\n",
      "Iter 97/200 - Loss: 33.317\n",
      "Iter 98/200 - Loss: 33.304\n",
      "Iter 99/200 - Loss: 33.291\n",
      "Iter 100/200 - Loss: 33.279\n",
      "Iter 101/200 - Loss: 33.266\n",
      "Iter 102/200 - Loss: 33.254\n",
      "Iter 103/200 - Loss: 33.242\n",
      "Iter 104/200 - Loss: 33.230\n",
      "Iter 105/200 - Loss: 33.219\n",
      "Iter 106/200 - Loss: 33.207\n",
      "Iter 107/200 - Loss: 33.196\n",
      "Iter 108/200 - Loss: 33.185\n",
      "Iter 109/200 - Loss: 33.174\n",
      "Iter 110/200 - Loss: 33.164\n",
      "Iter 111/200 - Loss: 33.153\n",
      "Iter 112/200 - Loss: 33.143\n",
      "Iter 113/200 - Loss: 33.133\n",
      "Iter 114/200 - Loss: 33.123\n",
      "Iter 115/200 - Loss: 33.113\n",
      "Iter 116/200 - Loss: 33.103\n",
      "Iter 117/200 - Loss: 33.094\n",
      "Iter 118/200 - Loss: 33.084\n",
      "Iter 119/200 - Loss: 33.075\n",
      "Iter 120/200 - Loss: 33.066\n",
      "Iter 121/200 - Loss: 33.057\n",
      "Iter 122/200 - Loss: 33.048\n",
      "Iter 123/200 - Loss: 33.040\n",
      "Iter 124/200 - Loss: 33.031\n",
      "Iter 125/200 - Loss: 33.023\n",
      "Iter 126/200 - Loss: 33.015\n",
      "Iter 127/200 - Loss: 33.007\n",
      "Iter 128/200 - Loss: 32.999\n",
      "Iter 129/200 - Loss: 32.991\n",
      "Iter 130/200 - Loss: 32.983\n",
      "Iter 131/200 - Loss: 32.975\n",
      "Iter 132/200 - Loss: 32.968\n",
      "Iter 133/200 - Loss: 32.961\n",
      "Iter 134/200 - Loss: 32.953\n",
      "Iter 135/200 - Loss: 32.946\n",
      "Iter 136/200 - Loss: 32.939\n",
      "Iter 137/200 - Loss: 32.932\n",
      "Iter 138/200 - Loss: 32.925\n",
      "Iter 139/200 - Loss: 32.919\n",
      "Iter 140/200 - Loss: 32.912\n",
      "Iter 141/200 - Loss: 32.905\n",
      "Iter 142/200 - Loss: 32.899\n",
      "Iter 143/200 - Loss: 32.893\n",
      "Iter 144/200 - Loss: 32.887\n",
      "Iter 145/200 - Loss: 32.880\n",
      "Iter 146/200 - Loss: 32.874\n",
      "Iter 147/200 - Loss: 32.869\n",
      "Iter 148/200 - Loss: 32.863\n",
      "Iter 149/200 - Loss: 32.857\n",
      "Iter 150/200 - Loss: 32.851\n",
      "Iter 151/200 - Loss: 32.846\n",
      "Iter 152/200 - Loss: 32.840\n",
      "Iter 153/200 - Loss: 32.835\n",
      "Iter 154/200 - Loss: 32.830\n",
      "Iter 155/200 - Loss: 32.825\n",
      "Iter 156/200 - Loss: 32.819\n",
      "Iter 157/200 - Loss: 32.814\n",
      "Iter 158/200 - Loss: 32.809\n",
      "Iter 159/200 - Loss: 32.805\n",
      "Iter 160/200 - Loss: 32.800\n",
      "Iter 161/200 - Loss: 32.795\n",
      "Iter 162/200 - Loss: 32.791\n",
      "Iter 163/200 - Loss: 32.786\n",
      "Iter 164/200 - Loss: 32.782\n",
      "Iter 165/200 - Loss: 32.777\n",
      "Iter 166/200 - Loss: 32.773\n",
      "Iter 167/200 - Loss: 32.769\n",
      "Iter 168/200 - Loss: 32.764\n",
      "Iter 169/200 - Loss: 32.760\n",
      "Iter 170/200 - Loss: 32.756\n",
      "Iter 171/200 - Loss: 32.752\n",
      "Iter 172/200 - Loss: 32.748\n",
      "Iter 173/200 - Loss: 32.744\n",
      "Iter 174/200 - Loss: 32.741\n",
      "Iter 175/200 - Loss: 32.737\n",
      "Iter 176/200 - Loss: 32.733\n",
      "Iter 177/200 - Loss: 32.730\n",
      "Iter 178/200 - Loss: 32.726\n",
      "Iter 179/200 - Loss: 32.723\n",
      "Iter 180/200 - Loss: 32.719\n",
      "Iter 181/200 - Loss: 32.716\n",
      "Iter 182/200 - Loss: 32.713\n",
      "Iter 183/200 - Loss: 32.709\n",
      "Iter 184/200 - Loss: 32.706\n",
      "Iter 185/200 - Loss: 32.703\n",
      "Iter 186/200 - Loss: 32.700\n",
      "Iter 187/200 - Loss: 32.697\n",
      "Iter 188/200 - Loss: 32.694\n",
      "Iter 189/200 - Loss: 32.691\n",
      "Iter 190/200 - Loss: 32.688\n",
      "Iter 191/200 - Loss: 32.685\n",
      "Iter 192/200 - Loss: 32.682\n",
      "Iter 193/200 - Loss: 32.679\n",
      "Iter 194/200 - Loss: 32.677\n",
      "Iter 195/200 - Loss: 32.674\n",
      "Iter 196/200 - Loss: 32.671\n",
      "Iter 197/200 - Loss: 32.669\n",
      "Iter 198/200 - Loss: 32.666\n",
      "Iter 199/200 - Loss: 32.664\n",
      "Iter 200/200 - Loss: 32.661\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv(\"data.csv\", index_col=0)\n",
    "\n",
    "# transform data\n",
    "train_x, train_y, train_N, true_theta = transform_data(data)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = BinomialLikelihood(n_trials=train_N)\n",
    "model = BinomialGPModel(train_x=train_x).double()\n",
    "\n",
    "training_iterations = 200\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1) \n",
    "\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, train_y.numel())\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(training_iterations):\n",
    "    # Zero backpropped gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Get predictive output\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "    optimizer.step()\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3888, 0.1177, 0.3172, 0.3295, 0.2884, 0.2938, 0.3095, 0.3148, 0.2801,\n",
      "         0.3222, 0.1701]], dtype=torch.float64, grad_fn=<SoftplusBackward0>)\n",
      "tensor(0.2123, dtype=torch.float64, grad_fn=<SoftplusBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.covar_module.base_kernel.lengthscale)\n",
    "print(model.covar_module.outputscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained for 0 min 37 sec for 200 iterations\n"
     ]
    }
   ],
   "source": [
    "print(\"trained for {:d} min {:d} sec for {:d} iterations\".format(int(end-start)//60,int(end-start)%60,training_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'est theta')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    f_pred = model(train_x)\n",
    "    mu = f_pred.mean.numpy()\n",
    "\n",
    "plt.scatter(true_theta,mu/np.std(mu))\n",
    "plt.xlabel(\"true theta\")\n",
    "plt.ylabel(\"est theta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg 95% Coverage: 0.952\n",
      "Avg RMSE: 0.412\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\"true_theta\": true_theta, \"est_mean\": mu/np.std(mu)})\n",
    "results['est_std'] = np.sqrt(f_pred.variance.numpy())/np.std(mu)\n",
    "results.to_csv(\"GPR_result.csv\")\n",
    "\n",
    "lower = results['est_mean'] - 2*results['est_std']\n",
    "upper = results['est_mean'] + 2*results['est_std']\n",
    "print(\"Avg 95% Coverage: {:.3f}\".format(np.mean(np.logical_and(lower<=true_theta, upper>=true_theta))))\n",
    "print(\"Avg RMSE: {:.3f}\".format(np.sqrt(np.mean((true_theta-results['est_mean'])**2))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
