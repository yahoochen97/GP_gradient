{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of GPR for infant mortality data in Miller (2015) with proposed accountability measure in LLM (2020)\n",
    "\n",
    "Miller, Michael K. \"Electoral authoritarianism and human development.\" Comparative Political Studies 48.12 (2015): 1526-1562.\n",
    "\n",
    "LÃœHRMANN, A., MARQUARDT, K., & MECHKOVA, V. (2020). Constraining Governments: New Indices of Vertical, Horizontal, and Diagonal Accountability. American Political Science Review, 114(3), 811-820."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import gpytorch\n",
    "from typing import Optional, Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.means import LinearMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement constant mean module and mask mean module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantVectorMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(self, d=1, prior=None, batch_shape=torch.Size(), **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_shape = batch_shape\n",
    "        self.register_parameter(name=\"constantvector\",\\\n",
    "                 parameter=torch.nn.Parameter(torch.zeros(*batch_shape, d)))\n",
    "        if prior is not None:\n",
    "            self.register_prior(\"mean_prior\", prior, \"constantvector\")\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.constantvector[input.int().reshape((-1,)).tolist()]\n",
    "    \n",
    "class MaskMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_mean: gpytorch.means.mean.Mean,\n",
    "        active_dims: Optional[Tuple[int, ...]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if active_dims is not None and not torch.is_tensor(active_dims):\n",
    "            active_dims = torch.tensor(active_dims, dtype=torch.long)\n",
    "        self.active_dims = active_dims\n",
    "        self.base_mean = base_mean\n",
    "    \n",
    "    def forward(self, x, **params):\n",
    "        return self.base_mean.forward(x.index_select(-1, self.active_dims), **params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and drop NA\n",
    "data = pd.read_csv(\"accountability_data_regressions.csv\", index_col=[0])\n",
    "# data = data[~data.infant.isna()]\n",
    "data = data[[\"infant\", \"Accountability\", \"aid\", \"loggdp\", \"gdp_grow\",\\\n",
    "            \"resourcesdep_hm\", \"gini2\", \"lnpop\", \"urban_cow\", \"country_name\", \\\n",
    "            \"violence_domestic\", \"communist\", \"rx_infant\", \"v2x_corr\", \"Vertical\", \"Horizontal\"]]\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "countries = sorted(data.country_name.unique())\n",
    "data[\"year\"] = data.index.values\n",
    "years = data.index.unique()\n",
    "n = len(countries)\n",
    "m = len(years)\n",
    "\n",
    "# build data\n",
    "country_dict = dict(zip(countries, range(n)))\n",
    "year_dict = dict(zip(years, range(m)))\n",
    "\n",
    "# x is:\n",
    "# 0: year number\n",
    "# 1: country id\n",
    "# 2: Accountability (regressor of interests)\n",
    "# 3: aid\n",
    "# 4: loggdp\n",
    "# 5: gdp_grow\n",
    "# 6: resourcesdep_hm\n",
    "# 7: gini2\n",
    "# 8: lnpop\n",
    "# 9: urban_cow\n",
    "# 10: rx_infant\n",
    "# 11: v2x_corr \n",
    "# 12: violence_domestic\n",
    "# 13: communist\n",
    "# 14: Vertical\n",
    "# 15: Horizontal\n",
    "\n",
    "x = torch.zeros(data.shape[0], 16)\n",
    "x[:,0] = torch.as_tensor(list(map(year_dict.get, data.index)))\n",
    "x[:,1] = torch.as_tensor(list(map(country_dict.get, data.country_name)))\n",
    "x[:,2] = torch.as_tensor(data.Accountability.to_numpy())\n",
    "x[:,3] = torch.as_tensor(data.aid.to_numpy())\n",
    "x[:,4] = torch.as_tensor(data.loggdp.to_numpy())\n",
    "x[:,5] = torch.as_tensor(data.gdp_grow.to_numpy())\n",
    "x[:,6] = torch.as_tensor(data.resourcesdep_hm.to_numpy())\n",
    "x[:,7] = torch.as_tensor(data.gini2.to_numpy())\n",
    "x[:,8] = torch.as_tensor(data.lnpop.to_numpy())\n",
    "x[:,9] = torch.as_tensor(data.urban_cow.to_numpy())\n",
    "x[:,10] = torch.as_tensor(data.rx_infant.to_numpy())\n",
    "x[:,11] = torch.as_tensor(data.v2x_corr.to_numpy())\n",
    "x[:,12] = torch.as_tensor(data.violence_domestic.to_numpy())\n",
    "x[:,13] = torch.as_tensor(data.communist.to_numpy())\n",
    "x[:,14] = torch.as_tensor(data.Vertical.to_numpy())\n",
    "x[:,15] = torch.as_tensor(data.Horizontal.to_numpy())\n",
    "y = torch.as_tensor(data.infant.to_numpy()).float()\n",
    "\n",
    "train_x, train_y = x.float(), y.float()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build GPR model with unit trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model specification: gp model with unit trends\n",
    "# infant ~ Accountability + aid + loggdp + gdp_grow +\n",
    "#    resourcesdep_hm + gini2 + lnpop + urban_cow + \n",
    "#    violence_domestic + communist + rx_infant + v2x_corr + u_i(t)\n",
    "# u_i(t) ~ GP(b_i, K)\n",
    "# K = K_x + K_t*K_{spatial}\n",
    "\n",
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, n_country):\n",
    "        # variational_distribution = CholeskyVariationalDistribution(train_x.size()[0])\n",
    "        # variational_strategy = VariationalStrategy(\n",
    "        #     self, train_x, variational_distribution, learn_inducing_locations=False\n",
    "        # )\n",
    "        # super(GPModel, self).__init__(variational_strategy)\n",
    "        super(GPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        # unit mean by country\n",
    "        self.unit_mean_module = MaskMean(active_dims=1, base_mean=ConstantVectorMean(d=n_country))\n",
    "        # year kernel * country kernel\n",
    "        self.unit_covar_module = ScaleKernel(RBFKernel(active_dims=0)*RBFKernel(active_dims=[14,15]))\n",
    "        # adopt a product kernel of continuous inputs and categorical inputs\n",
    "        self.x_covar_module = ScaleKernel(RBFKernel(active_dims=[2,3,4,5,6,7,8,9,10,11], ard_num_dims=10))\n",
    "        # linear mean for continuous covariates\n",
    "        self.x_mean_module = MaskMean(active_dims=[2,3,4,5,6,7,8,9,10,11], base_mean=LinearMean(input_size=10, bias=False))\n",
    "        # dummy mean for categorical covariates\n",
    "        self.categorical_mean_module = torch.nn.modules.ModuleList([MaskMean(active_dims=12, base_mean=ConstantVectorMean(d=11)),\\\n",
    "                                                                    MaskMean(active_dims=13, base_mean=ConstantVectorMean(d=2))])\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.unit_mean_module(x) + self.x_mean_module(x)\n",
    "        for i in range(len(self.categorical_mean_module)):\n",
    "            mean_x += self.categorical_mean_module[i](x)\n",
    "        # K = K_country * K_year + K_continuous \n",
    "        covar_x = self.x_covar_module(x) + self.unit_covar_module(x)\n",
    "        \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a 2FE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "\n",
    "lm = sm.ols('infant~ Accountability + aid + loggdp + gdp_grow + \\\n",
    "                        resourcesdep_hm + gini2 + lnpop + urban_cow + \\\n",
    "                        C(country_name) + C(year)  + C(violence_domestic) + C(communist) + \\\n",
    "                        + rx_infant + v2x_corr - 1', data).fit()\n",
    "# print(lm.summary())\n",
    "coefs = lm.params.to_dict()\n",
    "year_effects = np.zeros((m,))\n",
    "country_effects = np.zeros((n,))\n",
    "violence_effects = np.zeros((11,))\n",
    "communist_effects = np.zeros((2,))\n",
    "for i in range(1,m):\n",
    "    year_effects[i] = coefs['C(year)[T.' + str(years[i]) + ']']\n",
    "for i in range(n):\n",
    "    country_effects[i] = coefs['C(country_name)[' + countries[i] + ']']\n",
    "for i in range(1,11):\n",
    "    violence_effects[i] = coefs['C(violence_domestic)[T.' + str(i) + '.0]']\n",
    "communist_effects[1] = coefs['C(communist)[T.1.0]']\n",
    "noise_var = lm.scale\n",
    "year_os = np.max(np.abs(year_effects))**2\n",
    "\n",
    "covariate_names = [\"Accountability\", \"aid\", \"loggdp\",\"gdp_grow\", \\\n",
    "                        \"resourcesdep_hm\",\"gini2\",\"lnpop\",\"urban_cow\", \\\n",
    "                       \"rx_infant\", \"v2x_corr\"]\n",
    "x_weights = list(map(coefs.get, covariate_names))\n",
    "x_os = np.max(np.abs(list(map(lm.bse.to_dict().get, covariate_names))))**2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPModel(\n",
       "  (likelihood): GaussianLikelihood(\n",
       "    (noise_covar): HomoskedasticNoise(\n",
       "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "    )\n",
       "  )\n",
       "  (unit_mean_module): MaskMean(\n",
       "    (base_mean): ConstantVectorMean()\n",
       "  )\n",
       "  (unit_covar_module): ScaleKernel(\n",
       "    (base_kernel): ProductKernel(\n",
       "      (kernels): ModuleList(\n",
       "        (0): RBFKernel(\n",
       "          (raw_lengthscale_constraint): Positive()\n",
       "        )\n",
       "        (1): RBFKernel(\n",
       "          (raw_lengthscale_constraint): Positive()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (raw_outputscale_constraint): Positive()\n",
       "  )\n",
       "  (x_covar_module): ScaleKernel(\n",
       "    (base_kernel): RBFKernel(\n",
       "      (raw_lengthscale_constraint): Positive()\n",
       "    )\n",
       "    (raw_outputscale_constraint): Positive()\n",
       "  )\n",
       "  (x_mean_module): MaskMean(\n",
       "    (base_mean): LinearMean()\n",
       "  )\n",
       "  (categorical_mean_module): ModuleList(\n",
       "    (0): MaskMean(\n",
       "      (base_mean): ConstantVectorMean()\n",
       "    )\n",
       "    (1): MaskMean(\n",
       "      (base_mean): ConstantVectorMean()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood = GaussianLikelihood()\n",
    "likelihood.noise = 4 \n",
    "# N = train_x.size()[0]\n",
    "# inducing_points = train_x[torch.randperm(N)[:(N//20)]]\n",
    "model = GPModel(train_x, train_y, likelihood, n)\n",
    "\n",
    "# initialize model parameters\n",
    "hypers = {\n",
    "    'unit_mean_module.base_mean.constantvector': torch.tensor(country_effects),\n",
    "    'unit_covar_module.base_kernel.kernels.0.lengthscale': torch.tensor(10),\n",
    "    'unit_covar_module.base_kernel.kernels.1.lengthscale': torch.tensor(1),\n",
    "    'unit_covar_module.outputscale': torch.tensor(year_os),\n",
    "    'x_mean_module.base_mean.weights': torch.tensor(x_weights),\n",
    "    'x_covar_module.outputscale': torch.tensor(x_os),\n",
    "    'categorical_mean_module.0.base_mean.constantvector': torch.tensor(violence_effects),\n",
    "    'categorical_mean_module.1.base_mean.constantvector': torch.tensor(communist_effects)\n",
    "}    \n",
    "\n",
    "model.initialize(**hypers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model by optimizing hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 - Loss: 5.814 \n",
      "Iter 5 - Loss: 5.697 \n",
      "Iter 10 - Loss: 5.149 \n",
      "Iter 15 - Loss: 4.657 \n",
      "Iter 20 - Loss: 4.273 \n",
      "Iter 25 - Loss: 4.038 \n",
      "Iter 30 - Loss: 3.833 \n",
      "Iter 35 - Loss: 3.658 \n",
      "Iter 40 - Loss: 3.536 \n",
      "Iter 45 - Loss: 3.437 \n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "# freeze length scale in the country component in unit covar\n",
    "# freeze constant unit means\n",
    "all_params = set(model.parameters())\n",
    "final_params = list(all_params - \\\n",
    "            {model.unit_covar_module.base_kernel.kernels[1].raw_lengthscale})\n",
    "\n",
    "# optimizer = torch.optim.Adam([\n",
    "#     {'params': final_params},\n",
    "#     {'params': likelihood.parameters()},\n",
    "# ], lr=0.1)\n",
    "optimizer = torch.optim.Adam(final_params, lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "# loss_diff = 1\n",
    "# last_loss = 200\n",
    "# iter = 0\n",
    "# while(loss_diff>=1e-2):\n",
    "for iter in range(50):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    if iter % 5 == 0:\n",
    "        print('Iter %d - Loss: %.3f '  % (\n",
    "            iter, loss.item()\n",
    "        ))\n",
    "    # loss_diff = np.abs(last_loss-loss.item())\n",
    "    # last_loss = loss.item()\n",
    "    optimizer.step() \n",
    "    # iter += 1\n",
    "\n",
    "torch.save(model.state_dict(), \"AIM_GPR_model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate posterior; compute RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yahoo/anaconda3/lib/python3.7/site-packages/gpytorch/models/exact_gp.py:275: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  GPInputWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3345288778754743\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihood(model(train_x))\n",
    "\n",
    "RMSE = np.square((observed_pred.mean - train_y).detach().numpy()).mean()**0.5\n",
    "print(RMSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use autogradient to generate posterior variance of marginal effects by small batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "df_std = np.zeros((train_x.size(0),train_x.size(1)))\n",
    "x_grad = np.zeros((train_x.size(0),train_x.size(1)))\n",
    "evidence_gp = 0\n",
    "# small batches of size 100\n",
    "for i in range(train_x.size(0)//100):\n",
    "    with gpytorch.settings.fast_pred_var():\n",
    "        test_x = train_x[(i*100):(i*100+100)].clone().detach().requires_grad_(True)\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "        dydtest_x = torch.autograd.grad(observed_pred.mean.sum(), test_x, retain_graph=True)[0]\n",
    "        x_grad[(i*100):(i*100+100)] = dydtest_x\n",
    "        loss = mll(observed_pred, train_y[(i*100):(i*100+100)])\n",
    "        evidence_gp += loss.item() * observed_pred.mean.size(0)\n",
    "\n",
    "        n_samples = 100\n",
    "        sampled_pred = observed_pred.rsample(torch.Size([n_samples]))\n",
    "        sampled_dydtest_x = torch.stack([torch.autograd.grad(pred.sum(), test_x, retain_graph=True)[0] for pred in sampled_pred])\n",
    "        df_std[(i*100):(i*100+100)] = sampled_dydtest_x.std(0)\n",
    "        \n",
    "# last 100 rows\n",
    "with gpytorch.settings.fast_pred_var():\n",
    "    test_x = train_x[(100*i+100):].clone().detach().requires_grad_(True)\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "    dydtest_x = torch.autograd.grad(observed_pred.mean.sum(), test_x, retain_graph=True)[0]\n",
    "    x_grad[(100*i+100):] = dydtest_x\n",
    "    loss = mll(observed_pred, train_y[(100*i+100):])\n",
    "    evidence_gp += loss.item() * observed_pred.mean.size(0)\n",
    "\n",
    "    n_samples = 100\n",
    "    sampled_pred = observed_pred.rsample(torch.Size([n_samples]))\n",
    "    sampled_dydtest_x = torch.stack([torch.autograd.grad(pred.sum(), test_x, retain_graph=True)[0] for pred in sampled_pred])\n",
    "    df_std[(100*i+100):] = sampled_dydtest_x.std(0)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accesse marginal effects of regressors including accountability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 x   est_mean  est_std           t        pvalue\n",
      "0   Accountability  -4.292333    0.029 -148.011497  0.000000e+00\n",
      "1              aid  -0.037023    0.076   -0.487144  3.130781e-01\n",
      "2           loggdp -10.252744    1.955   -5.244370  7.840853e-08\n",
      "3         gdp_grow   0.021200    0.246    0.086179  5.343378e-01\n",
      "4  resourcesdep_hm   0.009873    0.096    0.102845  5.409572e-01\n",
      "5            gini2  -0.001887    0.208   -0.009071  4.963813e-01\n",
      "6            lnpop -17.964225    1.285  -13.979942  1.033286e-44\n",
      "7        urban_cow  -0.075183    0.178   -0.422375  3.363757e-01\n",
      "8        rx_infant   0.634666    0.487    1.303216  9.037495e-01\n",
      "9         v2x_corr  -3.723241    1.052   -3.539202  2.006689e-04\n"
     ]
    }
   ],
   "source": [
    "est_std = np.square(df_std).mean(axis=0).round(decimals=3)**0.5\n",
    "est_std = (df_std).mean(axis=0).round(decimals=3)\n",
    "results = pd.DataFrame({\"x\": covariate_names, \\\n",
    "                        'est_mean': x_grad.mean(axis=0)[2:12],\n",
    "                        'est_std': est_std[2:12]})\n",
    "results[\"t\"] = results['est_mean'].values/results['est_std'].values\n",
    "results[\"pvalue\"] = norm.cdf(results[\"t\"].values)\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare model evidence and BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm evidence: -15435.67\n",
      "\n",
      "gpm evidence: -10960.33\n",
      "\n",
      "lm bic: 32664.42\n",
      "\n",
      "gpm bic: 1536.82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evidence_lm = lm.llf\n",
    "\n",
    "num_params = 0\n",
    "for p in final_params:\n",
    "    num_params += p.reshape((-1,1)).size(0)\n",
    "bic_gp = num_params*np.log(train_x.size(0)) + 2*loss.item()\n",
    "bic_lm = lm.bic\n",
    "\n",
    "print(\"lm evidence: {:.2f}\\n\".format(evidence_lm))\n",
    "print(\"gpm evidence: {:.2f}\\n\".format(evidence_gp))\n",
    "print(\"lm bic: {:.2f}\\n\".format(bic_lm))\n",
    "print(\"gpm bic: {:.2f}\\n\".format(bic_gp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.3811]], grad_fn=<SoftplusBackward0>)\n",
      "tensor([[1.]], grad_fn=<SoftplusBackward0>)\n",
      "tensor(41.7539, grad_fn=<SoftplusBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-4.2979e+00],\n",
      "        [-2.9468e-02],\n",
      "        [-1.0524e+01],\n",
      "        [ 6.7370e-03],\n",
      "        [ 1.4616e-02],\n",
      "        [ 7.7441e-03],\n",
      "        [-1.7884e+01],\n",
      "        [-7.9762e-02],\n",
      "        [ 6.1208e-01],\n",
      "        [-3.3652e+00]], requires_grad=True)\n",
      "tensor(7.2707, grad_fn=<SoftplusBackward0>)\n",
      "tensor([[3.9983, 5.0333, 0.4293, 5.0150, 4.9576, 4.8648, 0.3747, 4.8716, 4.8539,\n",
      "         0.2553]], grad_fn=<SoftplusBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.unit_covar_module.base_kernel.kernels[0].lengthscale) # temporal cov ls\n",
    "print(model.unit_covar_module.base_kernel.kernels[1].lengthscale) # spatial cov ls\n",
    "print(model.unit_covar_module.outputscale) # spatial temporal cov os\n",
    "print(model.x_mean_module.base_mean.weights) # covariate linear mean effect\n",
    "print(model.x_covar_module.outputscale) # covariate kernel os\n",
    "print(model.x_covar_module.base_kernel.lengthscale) # covariate kernel os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
