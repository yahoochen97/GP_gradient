{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of GPR for PIRI data with unit trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "# load packages\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gpytorch\n",
    "print(gpytorch.__version__)\n",
    "from typing import Optional, Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "from gpytorch.means import LinearMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement constant mean module and mask mean module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantVectorMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(self, d=1, prior=None, batch_shape=torch.Size(), **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_shape = batch_shape\n",
    "        self.register_parameter(name=\"constantvector\",\\\n",
    "                 parameter=torch.nn.Parameter(torch.zeros(*batch_shape, d)))\n",
    "        if prior is not None:\n",
    "            self.register_prior(\"mean_prior\", prior, \"constantvector\")\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.constantvector[input.int().reshape((-1,)).tolist()]\n",
    "    \n",
    "class MaskMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_mean: gpytorch.means.mean.Mean,\n",
    "        active_dims: Optional[Tuple[int, ...]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if active_dims is not None and not torch.is_tensor(active_dims):\n",
    "            active_dims = torch.tensor(active_dims, dtype=torch.long)\n",
    "        self.active_dims = active_dims\n",
    "        self.base_mean = base_mean\n",
    "    \n",
    "    def forward(self, x, **params):\n",
    "        return self.base_mean.forward(x.index_select(-1, self.active_dims), **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data with last 5 years splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_PIRI_data():\n",
    "    # read data\n",
    "    data = pd.read_csv(\"hb_data_complete.csv\", index_col=[0])\n",
    "\n",
    "    # all zero PIRI for new zealand and netherland\n",
    "    data = data.loc[~data['country'].isin(['N-ZEAL','NETHERL'])]\n",
    "\n",
    "    countries = sorted(data.country.unique())\n",
    "    years = data.year.unique()\n",
    "    n = len(countries)\n",
    "    m = len(years)\n",
    "\n",
    "    # build data\n",
    "    country_dict = dict(zip(countries, range(n)))\n",
    "    year_dict = dict(zip(years, range(m)))\n",
    "\n",
    "    # x is:\n",
    "    # 1: year number\n",
    "    # 2: country id\n",
    "    # 3: AIShame (treatment indicator)\n",
    "    # 4: cat_rat\n",
    "    # 5: ccpr_rat\n",
    "    # 6: democratic\n",
    "    # 7: log(gdppc)\n",
    "    # 8: log(pop)\n",
    "    # 9: Civilwar2\n",
    "    # 10: War\n",
    "    x = torch.zeros(data.shape[0], 10)\n",
    "    x[:,0] = torch.as_tensor(list(map(year_dict.get, data.year)))\n",
    "    x[:,1] = torch.as_tensor(list(map(country_dict.get, data.country)))\n",
    "    x[:,2] = torch.as_tensor(data.AIShame.to_numpy())\n",
    "    x[:,3] = torch.as_tensor(data.cat_rat.to_numpy())\n",
    "    x[:,4] = torch.as_tensor(data.ccpr_rat.to_numpy())\n",
    "    x[:,5] = torch.as_tensor(data.democratic.to_numpy())\n",
    "    x[:,6] = torch.as_tensor(data.log_gdppc.to_numpy())\n",
    "    x[:,7] = torch.as_tensor(data.log_pop.to_numpy())\n",
    "    x[:,8] = torch.as_tensor(data.Civilwar2.to_numpy())\n",
    "    x[:,9] = torch.as_tensor(data.War.to_numpy())\n",
    "    y = torch.as_tensor(data.PIRI.to_numpy()).double()\n",
    "\n",
    "    # split data into training and testing by last 5 years\n",
    "    train_mask = x[:,0] < (m-5)\n",
    "    train_x = x[train_mask]\n",
    "    test_x = x[~train_mask]\n",
    "    train_y = y[train_mask]\n",
    "    test_y = y[~train_mask]\n",
    "\n",
    "    unit_means = torch.zeros(n,)\n",
    "    for i in range(n):\n",
    "        unit_means[i] = train_y[train_x[:,1]==i].mean()\n",
    "\n",
    "    return train_x.double(), train_y.double(), test_x.double(), test_y.double(), unit_means.double(), data, countries, years\n",
    "\n",
    "train_x, train_y, test_x, test_y, unit_means, data, countries, years = load_PIRI_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build GPR model with unit trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model specification: PIRI gp model with unit trends\n",
    "# PIRI ~ AIShame + u_i(t) + cat_rat + ccpr_rat \n",
    "#            + democratic + log(gdppc) + log(pop) \n",
    "#            + Civilwar2 + War\n",
    "# u_i(t) ~ GP(b_i, K_t)\n",
    "\n",
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, ard_num_dim=None):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = MaskMean(active_dims=1, \\\n",
    "                base_mean=ConstantVectorMean(d=train_x[:,1].unique().size()[0]))\n",
    "        # year kernel * country kernel\n",
    "        self.unit_covar_module = ScaleKernel(RBFKernel(active_dims=0)*RBFKernel(active_dims=1))\n",
    "        self.x_covar_module = torch.nn.ModuleList([ScaleKernel(RBFKernel(\\\n",
    "            active_dims=(i))) for i in [6,7]])\n",
    "        self.binary_covar_module = torch.nn.ModuleList([ScaleKernel(RBFKernel(\\\n",
    "            active_dims=(i))) for i in [3,4,5,8,9]])\n",
    "        self.effect_covar_module = ScaleKernel(RBFKernel(active_dims=2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        unit_covar_x = self.unit_covar_module(x)\n",
    "        effect_covar_x = self.effect_covar_module(x)\n",
    "        covar_x = unit_covar_x + effect_covar_x\n",
    "        for i, _ in enumerate(self.x_covar_module):\n",
    "            covar_x += self.x_covar_module[i](x)\n",
    "        for i, _ in enumerate(self.binary_covar_module):\n",
    "            covar_x += self.binary_covar_module[i](x)\n",
    "        \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPModel(\n",
       "  (likelihood): GaussianLikelihood(\n",
       "    (noise_covar): HomoskedasticNoise(\n",
       "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "    )\n",
       "  )\n",
       "  (mean_module): MaskMean(\n",
       "    (base_mean): ConstantVectorMean()\n",
       "  )\n",
       "  (unit_covar_module): ScaleKernel(\n",
       "    (base_kernel): ProductKernel(\n",
       "      (kernels): ModuleList(\n",
       "        (0): RBFKernel(\n",
       "          (raw_lengthscale_constraint): Positive()\n",
       "        )\n",
       "        (1): RBFKernel(\n",
       "          (raw_lengthscale_constraint): Positive()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (raw_outputscale_constraint): Positive()\n",
       "  )\n",
       "  (x_covar_module): ModuleList(\n",
       "    (0): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (1): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "  )\n",
       "  (binary_covar_module): ModuleList(\n",
       "    (0): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (1): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (2): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (3): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (4): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "  )\n",
       "  (effect_covar_module): ScaleKernel(\n",
       "    (base_kernel): RBFKernel(\n",
       "      (raw_lengthscale_constraint): Positive()\n",
       "    )\n",
       "    (raw_outputscale_constraint): Positive()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood = GaussianLikelihood()\n",
    "model = GPModel(train_x, train_y, likelihood).double()\n",
    "\n",
    "# initialize model parameters\n",
    "hypers = {\n",
    "    'mean_module.base_mean.constantvector': unit_means,\n",
    "    'likelihood.noise_covar.noise': torch.tensor(0.25),\n",
    "    'unit_covar_module.base_kernel.kernels.0.lengthscale': torch.tensor(4),\n",
    "    'unit_covar_module.base_kernel.kernels.1.lengthscale': torch.tensor(0.01),\n",
    "    'unit_covar_module.outputscale': torch.tensor(0.25),\n",
    "    'x_covar_module.0.outputscale': torch.tensor(0.25),\n",
    "    'x_covar_module.1.outputscale': torch.tensor(0.25),\n",
    "    'binary_covar_module.0.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.1.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.2.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.3.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.4.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'effect_covar_module.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'effect_covar_module.outputscale': torch.tensor(0.25)\n",
    "}    \n",
    "\n",
    "model.initialize(**hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model by optimizing hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/100 - Loss: 2.358 \n",
      "Iter 2/100 - Loss: 2.252 \n",
      "Iter 3/100 - Loss: 2.132 \n",
      "Iter 4/100 - Loss: 2.029 \n",
      "Iter 5/100 - Loss: 1.956 \n",
      "Iter 6/100 - Loss: 1.887 \n",
      "Iter 7/100 - Loss: 1.835 \n",
      "Iter 8/100 - Loss: 1.787 \n",
      "Iter 9/100 - Loss: 1.752 \n",
      "Iter 10/100 - Loss: 1.717 \n",
      "Iter 11/100 - Loss: 1.691 \n",
      "Iter 12/100 - Loss: 1.665 \n",
      "Iter 13/100 - Loss: 1.645 \n",
      "Iter 14/100 - Loss: 1.639 \n",
      "Iter 15/100 - Loss: 1.619 \n",
      "Iter 16/100 - Loss: 1.607 \n",
      "Iter 17/100 - Loss: 1.605 \n",
      "Iter 18/100 - Loss: 1.605 \n",
      "Iter 19/100 - Loss: 1.601 \n",
      "Iter 20/100 - Loss: 1.586 \n",
      "Iter 21/100 - Loss: 1.589 \n",
      "Iter 22/100 - Loss: 1.593 \n",
      "Iter 23/100 - Loss: 1.588 \n",
      "Iter 24/100 - Loss: 1.586 \n",
      "Iter 25/100 - Loss: 1.588 \n",
      "Iter 26/100 - Loss: 1.592 \n",
      "Iter 27/100 - Loss: 1.588 \n",
      "Iter 28/100 - Loss: 1.590 \n",
      "Iter 29/100 - Loss: 1.591 \n",
      "Iter 30/100 - Loss: 1.593 \n",
      "Iter 31/100 - Loss: 1.592 \n",
      "Iter 32/100 - Loss: 1.592 \n",
      "Iter 33/100 - Loss: 1.593 \n",
      "Iter 34/100 - Loss: 1.594 \n",
      "Iter 35/100 - Loss: 1.591 \n",
      "Iter 36/100 - Loss: 1.589 \n",
      "Iter 37/100 - Loss: 1.583 \n",
      "Iter 38/100 - Loss: 1.591 \n",
      "Iter 39/100 - Loss: 1.586 \n",
      "Iter 40/100 - Loss: 1.587 \n",
      "Iter 41/100 - Loss: 1.591 \n",
      "Iter 42/100 - Loss: 1.589 \n",
      "Iter 43/100 - Loss: 1.593 \n",
      "Iter 44/100 - Loss: 1.589 \n",
      "Iter 45/100 - Loss: 1.589 \n",
      "Iter 46/100 - Loss: 1.589 \n",
      "Iter 47/100 - Loss: 1.592 \n",
      "Iter 48/100 - Loss: 1.587 \n",
      "Iter 49/100 - Loss: 1.583 \n",
      "Iter 50/100 - Loss: 1.585 \n",
      "Iter 51/100 - Loss: 1.586 \n",
      "Iter 52/100 - Loss: 1.589 \n",
      "Iter 53/100 - Loss: 1.579 \n",
      "Iter 54/100 - Loss: 1.580 \n",
      "Iter 55/100 - Loss: 1.585 \n",
      "Iter 56/100 - Loss: 1.586 \n",
      "Iter 57/100 - Loss: 1.583 \n",
      "Iter 58/100 - Loss: 1.584 \n",
      "Iter 59/100 - Loss: 1.585 \n",
      "Iter 60/100 - Loss: 1.580 \n",
      "Iter 61/100 - Loss: 1.580 \n",
      "Iter 62/100 - Loss: 1.584 \n",
      "Iter 63/100 - Loss: 1.580 \n",
      "Iter 64/100 - Loss: 1.583 \n",
      "Iter 65/100 - Loss: 1.579 \n",
      "Iter 66/100 - Loss: 1.580 \n",
      "Iter 67/100 - Loss: 1.577 \n",
      "Iter 68/100 - Loss: 1.579 \n",
      "Iter 69/100 - Loss: 1.583 \n",
      "Iter 70/100 - Loss: 1.583 \n",
      "Iter 71/100 - Loss: 1.579 \n",
      "Iter 72/100 - Loss: 1.577 \n",
      "Iter 73/100 - Loss: 1.582 \n",
      "Iter 74/100 - Loss: 1.584 \n",
      "Iter 75/100 - Loss: 1.580 \n",
      "Iter 76/100 - Loss: 1.584 \n",
      "Iter 77/100 - Loss: 1.587 \n",
      "Iter 78/100 - Loss: 1.583 \n",
      "Iter 79/100 - Loss: 1.579 \n",
      "Iter 80/100 - Loss: 1.583 \n",
      "Iter 81/100 - Loss: 1.587 \n",
      "Iter 82/100 - Loss: 1.579 \n",
      "Iter 83/100 - Loss: 1.583 \n",
      "Iter 84/100 - Loss: 1.578 \n",
      "Iter 85/100 - Loss: 1.584 \n",
      "Iter 86/100 - Loss: 1.588 \n",
      "Iter 87/100 - Loss: 1.581 \n",
      "Iter 88/100 - Loss: 1.584 \n",
      "Iter 89/100 - Loss: 1.584 \n",
      "Iter 90/100 - Loss: 1.583 \n",
      "Iter 91/100 - Loss: 1.579 \n",
      "Iter 92/100 - Loss: 1.588 \n",
      "Iter 93/100 - Loss: 1.583 \n",
      "Iter 94/100 - Loss: 1.580 \n",
      "Iter 95/100 - Loss: 1.578 \n",
      "Iter 96/100 - Loss: 1.578 \n",
      "Iter 97/100 - Loss: 1.575 \n",
      "Iter 98/100 - Loss: 1.586 \n",
      "Iter 99/100 - Loss: 1.578 \n",
      "Iter 100/100 - Loss: 1.580 \n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "# freeze length scale in the country component in unit covar\n",
    "# freeze constant unit means\n",
    "all_params = set(model.parameters())\n",
    "final_params = list(all_params - \\\n",
    "            {model.unit_covar_module.base_kernel.kernels[1].raw_lengthscale, \\\n",
    "            model.mean_module.base_mean.constantvector, \\\n",
    "        #   model.x_covar_module[0].raw_outputscale,\n",
    "        #   model.x_covar_module[1].raw_outputscale,\n",
    "            model.binary_covar_module[0].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[1].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[2].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[3].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[4].base_kernel.raw_lengthscale,\n",
    "            model.effect_covar_module.base_kernel.raw_lengthscale})\n",
    "        #   model.effect_covar_module.raw_outputscale})\n",
    "optimizer = torch.optim.Adam(final_params, lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 100\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f '  % (\n",
    "        i + 1, training_iter, loss.item()\n",
    "    ))\n",
    "    optimizer.step()\n",
    "\n",
    "torch.save(model.state_dict(), \"PIRI_GPR_model_5yr.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate posterior of PIRI effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effect: 0.335 +- 0.152\n",
      "\n",
      "model evidence: -2331.979 \n",
      "\n",
      "BIC: 4758.820 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('PIRI_GPR_model_5yr.pth'))\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out = likelihood(model(train_x))\n",
    "    mu_f = out.mean\n",
    "    V = out.covariance_matrix\n",
    "    L = torch.linalg.cholesky(V, upper=False)\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    model.unit_covar_module.outputscale = 0\n",
    "    for i,_ in enumerate(model.x_covar_module):\n",
    "        model.x_covar_module[i].outputscale = 0\n",
    "    for i,_ in enumerate(model.binary_covar_module):\n",
    "        model.binary_covar_module[i].outputscale = 0\n",
    "    effect_covar = model(train_x).covariance_matrix\n",
    "\n",
    "# get posterior effect mean\n",
    "alpha = torch.linalg.solve(L.t(),torch.linalg.solve(L,train_y-mu_f))\n",
    "tmp = torch.linalg.solve(L, effect_covar)\n",
    "post_effect_mean = effect_covar @ alpha\n",
    "# get posterior effect covariance\n",
    "post_effect_covar = effect_covar - tmp.t() @ tmp\n",
    "\n",
    "effect = post_effect_mean[train_x[:,2]==1].mean() - post_effect_mean[train_x[:,2]==0].mean()\n",
    "effect_std = post_effect_covar.diag().mean().sqrt()\n",
    "BIC = (2+4+6+1)*torch.log(torch.tensor(train_x.size()[0])) + 2*loss*train_x.size()[0]\n",
    "print(\"effect: {:0.3f} +- {:0.3f}\\n\".format(effect, effect_std))\n",
    "print(\"model evidence: {:0.3f} \\n\".format(-loss*train_x.size()[0]))\n",
    "print(\"BIC: {:0.3f} \\n\".format(BIC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Durbin Watson tests for autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 out of 138 residuals are positively correlated.\n",
      "\n",
      "0 out of 138 residuals are negatively correlated.\n",
      "\n",
      "133 out of 138 residuals are not correlated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get unit trend wo AIShame\n",
    "model.load_state_dict(torch.load('PIRI_GPR_model_5yr.pth'))\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    model.effect_covar_module.outputscale = 0\n",
    "    unit_covar = likelihood(model(train_x)).covariance_matrix\n",
    "\n",
    "# get posterior unit trend mean\n",
    "alpha = torch.linalg.solve(L.t(),torch.linalg.solve(L,train_y-mu_f))\n",
    "tmp = torch.linalg.solve(L, unit_covar)\n",
    "post_unit_mean = mu_f + unit_covar @ alpha + post_effect_mean\n",
    "\n",
    "# DW-test for sample size = 13 and 8 regressors.\n",
    "dL = 0.147\n",
    "dU = 3.266\n",
    "n = len(countries)\n",
    "DW_results = np.zeros((n,))\n",
    "for i in range(n):\n",
    "    mask = (train_x[:,1]==i).numpy()\n",
    "    res = train_y[mask] - post_unit_mean[mask]\n",
    "    DW_results[i] = durbin_watson(res.detach().numpy())\n",
    "\n",
    "print(\"{} out of {} residuals are positively correlated.\\n\".format(np.sum(DW_results<=dL),n))\n",
    "print(\"{} out of {} residuals are negatively correlated.\\n\".format(np.sum(DW_results>=dU),n))\n",
    "print(\"{} out of {} residuals are not correlated.\\n\".format(np.sum((DW_results>dL) & (DW_results<dU)),n))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    y_pred = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_np = y_pred.mean.numpy() #convert to Numpy array\n",
    "\n",
    "df = pd.DataFrame(y_pred_np) #convert to a dataframe\n",
    "df['true_y'] = test_y\n",
    "df.to_csv(\"predicted_5_year.csv\",index=False) #save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fd2df426198>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(test_y.numpy(),y_pred_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
