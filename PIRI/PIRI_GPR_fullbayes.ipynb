{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration of Full Bayesian Gaussian Process Regression (GPR) with multiple-units time series data\n",
    "\n",
    "The dataset we are going to use in this demo is the physical integrity rights index (PIRI) data, which comes from Strezhnev, Anton, Judith G Kelley and Beth A Simmons. 2021. “Testing for Negative Spillovers: Is Promoting Human Rights Really Part of the “Problem”?” International\n",
    "Organization 75(1):71–102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going Fully Bayesian - Sampling Hyperparamters with NUTS\n",
    "\n",
    "So far our inference relies on a single set of hyperparameters optimized using empirical Bayes. However, we have not really accounted for *uncertainty associated with hyperparameters* that might lead to model misspecification. Hence, we may adopt a fully Bayesian inference strategy that adds another layer of prior structures on the hyperparameters, where the parameters specifying shapes of those priors are sometimes referred as *hyper-hyperparameters*.\n",
    "\n",
    "Gpytorch has integrated pyro, a probabilistic programming language specifically designed to reason probability and uncertainty in large-scale machine learning research, for sampling GP hyperparameters and performing fully Bayesian inference. Here we follow [gpytorch doc](https://docs.gpytorch.ai/en/stable/examples/01_Exact_GPs/GP_Regression_Fully_Bayesian.html) to demonstrate how to sample hyperparameters with No-U-Turn Sampler ([NUTS](https://www.jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gpytoch and other packages\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gpytorch\n",
    "from scipy.stats import norm\n",
    "from typing import Optional, Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "from gpytorch.means import LinearMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.priors import GammaPrior, LogNormalPrior\n",
    "\n",
    "# load pyro packages\n",
    "import pyro\n",
    "from pyro.infer.mcmc import NUTS, MCMC\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "num_samples = 2 if smoke_test else 100\n",
    "warmup_steps = 2 if smoke_test else 100\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_PIRI_data():\n",
    "    # read data\n",
    "    url = \"https://raw.githubusercontent.com/yahoochen97/GP_gradient/main/hb_data_complete.csv\"\n",
    "    data = pd.read_csv(url, index_col=[0])\n",
    "\n",
    "    # all zero PIRI for new zealand and netherland\n",
    "    data = data.loc[~data['country'].isin(['N-ZEAL','NETHERL'])]\n",
    "\n",
    "    countries = sorted(data.country.unique())\n",
    "    years = data.year.unique()\n",
    "    n = len(countries)\n",
    "    m = len(years)\n",
    "\n",
    "    # build data\n",
    "    country_dict = dict(zip(countries, range(n)))\n",
    "    year_dict = dict(zip(years, range(m)))\n",
    "\n",
    "    # x is:\n",
    "    # 1: year number\n",
    "    # 2: country id\n",
    "    # 3: AIShame (treatment indicator)\n",
    "    # 4: cat_rat\n",
    "    # 5: ccpr_rat\n",
    "    # 6: democratic\n",
    "    # 7: log(gdppc)\n",
    "    # 8: log(pop)\n",
    "    # 9: Civilwar2\n",
    "    # 10: War\n",
    "    x = torch.zeros(data.shape[0], 10)\n",
    "    x[:,0] = torch.as_tensor(list(map(year_dict.get, data.year)))\n",
    "    x[:,1] = torch.as_tensor(list(map(country_dict.get, data.country)))\n",
    "    x[:,2] = torch.as_tensor(data.AIShame.to_numpy())\n",
    "    x[:,3] = torch.as_tensor(data.cat_rat.to_numpy())\n",
    "    x[:,4] = torch.as_tensor(data.ccpr_rat.to_numpy())\n",
    "    x[:,5] = torch.as_tensor(data.democratic.to_numpy())\n",
    "    x[:,6] = torch.as_tensor(data.log_gdppc.to_numpy())\n",
    "    x[:,7] = torch.as_tensor(data.log_pop.to_numpy())\n",
    "    x[:,8] = torch.as_tensor(data.Civilwar2.to_numpy())\n",
    "    x[:,9] = torch.as_tensor(data.War.to_numpy())\n",
    "    # x[:,10] = torch.as_tensor(data.PIRI.to_numpy())\n",
    "    y = torch.as_tensor(data.PIRILead1.to_numpy()).double()\n",
    "\n",
    "    unit_means = torch.zeros(n,)\n",
    "    for i in range(n):\n",
    "        unit_means[i] = y[x[:,1]==i].mean()\n",
    "\n",
    "    return x.double(), y.double(), unit_means.double(), data, countries, years\n",
    "\n",
    "train_x, train_y, unit_means, data, countries, years = load_PIRI_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customization of mean and kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantVectorMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(self, d=1, prior=None, batch_shape=torch.Size(), **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_shape = batch_shape\n",
    "        self.register_parameter(name=\"constantvector\",\\\n",
    "                 parameter=torch.nn.Parameter(torch.zeros(*batch_shape, d)))\n",
    "        if prior is not None:\n",
    "            self.register_prior(\"constantvector_prior\", prior, \"constantvector\")\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.constantvector[input.int().reshape((-1,)).tolist()]\n",
    "    \n",
    "class MaskMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_mean: gpytorch.means.mean.Mean,\n",
    "        active_dims: Optional[Tuple[int, ...]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if active_dims is not None and not torch.is_tensor(active_dims):\n",
    "            active_dims = torch.tensor(active_dims, dtype=torch.long)\n",
    "        self.active_dims = active_dims\n",
    "        self.base_mean = base_mean\n",
    "    \n",
    "    def forward(self, x, **params):\n",
    "        return self.base_mean.forward(x.index_select(-1, self.active_dims), **params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build GPR model for multi-unit time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model specification: PIRI gp model with unit trends\n",
    "# x_it : AIShame + cat_rat + ccpr_rat \n",
    "#            + democratic + log(gdppc) + log(pop) \n",
    "#            + Civilwar2 + War \n",
    "# y_i(t) ~ u_i(t) + f(x_{it}) + ε\n",
    "# f(x_{it}) ~ GP(0, K_x)\n",
    "# u_i(t) ~ GP(b_i, K_t)\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "lm = sm.ols('PIRILead1 ~ AIShame  + cat_rat + ccpr_rat \\\n",
    "            + democratic + log_gdppc + log_pop \\\n",
    "            + Civilwar2 + War + C(year) + C(country) + PIRI', data).fit()\n",
    "\n",
    "coefs = lm.params.to_dict()\n",
    "covariate_names = [\"AIShame\" ,\"cat_rat\" , \"ccpr_rat\",\n",
    "           \"democratic\",  \"log_gdppc\", \"log_pop\",\n",
    "            \"Civilwar2\", \"War\"]\n",
    "x_weights = list(map(coefs.get, covariate_names))\n",
    "\n",
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.likelihood = likelihood\n",
    "\n",
    "        # constant country-level mean; fix; no prior\n",
    "        self.mean_module = MaskMean(active_dims=1, \\\n",
    "               base_mean=ConstantVectorMean(d=train_x[:,1].unique().size()[0]))\n",
    "        \n",
    "        # linear mean for continuous and binary covariates\n",
    "        self.x_mean_module = MaskMean(active_dims=[2,3,4,5,6,7,8,9], base_mean=LinearMean(input_size=8, bias=False))\n",
    "        \n",
    "        # unit level trend: year kernel * country kernel\n",
    "        self.unit_covar_module = ScaleKernel(RBFKernel(active_dims=0)*RBFKernel(active_dims=1))\n",
    "        self.x_covar_module = ScaleKernel(RBFKernel(active_dims=[2,3,4,5,6,7,8,9],ard_num_dims=8))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x) + self.x_mean_module(x)\n",
    "        unit_covar_x = self.unit_covar_module(x)\n",
    "        covar_x = unit_covar_x + self.x_covar_module(x)\n",
    "        \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning and training of GPR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = GaussianLikelihood()\n",
    "model = GPModel(train_x, train_y, likelihood).double()\n",
    "\n",
    "# initialize model parameters\n",
    "hypers = {\n",
    "    'mean_module.base_mean.constantvector': unit_means,\n",
    "    'x_mean_module.base_mean.weights': torch.tensor(x_weights),\n",
    "    'likelihood.noise_covar.noise': torch.tensor(0.5),\n",
    "    'unit_covar_module.base_kernel.kernels.0.lengthscale': torch.tensor(6.),\n",
    "    'unit_covar_module.base_kernel.kernels.1.lengthscale': torch.tensor(0.01),\n",
    "    'unit_covar_module.outputscale': torch.tensor(4.),\n",
    "    'x_covar_module.outputscale': torch.tensor(1.)\n",
    "}    \n",
    "\n",
    "model = model.initialize(**hypers)\n",
    "\n",
    "# fix constant prior mean\n",
    "model.mean_module.base_mean.constantvector.requires_grad = False\n",
    "model.unit_covar_module.base_kernel.kernels[1].raw_lengthscale.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we register hyperpriors to existing gp model and likelihood using `register_prior()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unit_covar_module.register_prior(\"outputscale_prior\", GammaPrior(1.0, 1.), \"outputscale\")\n",
    "model.unit_covar_module.base_kernel.kernels[0].register_prior(\"lengthscale_prior\", GammaPrior(2., 1.), \"lengthscale\")\n",
    "model.x_covar_module.base_kernel.register_prior(\"lengthscale_prior\", GammaPrior(1., 1.), \"lengthscale\")\n",
    "model.x_covar_module.register_prior(\"outputscale_prior\", GammaPrior(1., 1.), \"outputscale\")\n",
    "likelihood.register_prior(\"noise_prior\", GammaPrior(1., 1.0), \"noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the MAP estimate can be computed by minimizing the penalized marginalized log likelihood loss via built-in optimizers in pytorch, and serve as the initial values for MCMC sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0/100 - Loss: 1.887 \n",
      "Iter 20/100 - Loss: 1.668 \n",
      "Iter 40/100 - Loss: 1.633 \n",
      "Iter 60/100 - Loss: 1.604 \n",
      "Iter 80/100 - Loss: 1.575 \n"
     ]
    }
   ],
   "source": [
    "# Initialize with MAP\n",
    "model.train()\n",
    "likelihood.train()\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "# freeze length scale in the country component in unit covar\n",
    "# freeze constant unit means\n",
    "all_params = set(model.parameters())\n",
    "final_params = list(all_params - \\\n",
    "            {model.unit_covar_module.base_kernel.kernels[1].raw_lengthscale, \\\n",
    "            model.mean_module.base_mean.constantvector})\n",
    "optimizer = torch.optim.Adam(final_params, lr=0.05)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 100\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    if i % 20 == 0:\n",
    "        print('Iter %d/%d - Loss: %.3f '  % (\n",
    "            i , training_iter, loss.item()\n",
    "        ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch has a nice interface with pyro which performs MCMC sampling. We follow [the tutorial](https://docs.gpytorch.ai/en/v1.6.0/examples/01_Exact_GPs/GP_Regression_Fully_Bayesian.html) in defining pyro model and training NUTS sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading mcmc run from url...\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "def pyro_model(x, y):\n",
    "    sampled_model = model.pyro_sample_from_prior()\n",
    "    output = sampled_model.likelihood(sampled_model(x))\n",
    "    pyro.sample(\"obs\", output, obs=y)\n",
    "\n",
    "nuts_kernel = NUTS(pyro_model)  \n",
    "mcmc_run = MCMC(nuts_kernel, num_samples=num_samples,\\\n",
    "            warmup_steps=warmup_steps, disable_progbar=smoke_test,\\\n",
    "            num_chains=1)\n",
    "\n",
    "# we load trained pyro model as running full sampling takes time\n",
    "# set load_trained_pyro_model to False if you want to run MCMC sampling\n",
    "# on your own machine\n",
    "load_trained_pyro_model = True\n",
    "if load_trained_pyro_model==False:\n",
    "    mcmc_run.run(train_x, train_y)\n",
    "else:\n",
    "    print(\"loading mcmc run from url...\")\n",
    "    import dill\n",
    "    with open('./results/PIRI_GPR_fullbayes.pkl', 'rb') as f:\n",
    "        mcmc_run = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we can check convergence statistic like effective sample size and rhat using `mcmc_run.diagnostic()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               v_name                   n_eff  \\\n",
      "0                              likelihood.noise_prior                   [nan]   \n",
      "1                 unit_covar_module.outputscale_prior                     nan   \n",
      "2   unit_covar_module.base_kernel.kernels.0.length...  [[0.5025125628140703]]   \n",
      "3                    x_covar_module.outputscale_prior      0.5025125628140703   \n",
      "4      x_covar_module.base_kernel.lengthscale_prior_0                     NaN   \n",
      "5      x_covar_module.base_kernel.lengthscale_prior_1                     NaN   \n",
      "6      x_covar_module.base_kernel.lengthscale_prior_2                     NaN   \n",
      "7      x_covar_module.base_kernel.lengthscale_prior_3                     NaN   \n",
      "8      x_covar_module.base_kernel.lengthscale_prior_4                     NaN   \n",
      "9      x_covar_module.base_kernel.lengthscale_prior_5                     NaN   \n",
      "10     x_covar_module.base_kernel.lengthscale_prior_6                     NaN   \n",
      "11     x_covar_module.base_kernel.lengthscale_prior_7                     NaN   \n",
      "\n",
      "      r_hat  \n",
      "0     [nan]  \n",
      "1       nan  \n",
      "2   [[nan]]  \n",
      "3       nan  \n",
      "4       NaN  \n",
      "5       NaN  \n",
      "6       NaN  \n",
      "7       NaN  \n",
      "8       NaN  \n",
      "9       NaN  \n",
      "10      NaN  \n",
      "11      NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "results = pd.DataFrame(columns=['v_name','n_eff','r_hat'])\n",
    "\n",
    "for k,v in mcmc_run.diagnostics().items():\n",
    "    if k=='divergences' or k==\"acceptance rate\": continue\n",
    "    if k==\"x_covar_module.base_kernel.lengthscale_prior\":\n",
    "        for i in range(v['n_eff'].shape[1]):\n",
    "            results = results.append({'v_name':k+\"_\"+str(i), 'n_eff': v['n_eff'].numpy()[0,i],\\\n",
    "                             'r_hat': v['r_hat'].numpy()[0,i]}, ignore_index=True)\n",
    "    else:\n",
    "        results = results.append({'v_name':k, 'n_eff': v['n_eff'].numpy(), \\\n",
    "                              'r_hat': v['r_hat'].numpy()}, ignore_index=True)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we can computer the gradients iteratively over all samples and take average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-991c8077b37a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0msampled_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobserved_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             sampled_dydtest_x[:,(i*100):(i*100+100),:] = torch.stack([torch.autograd.grad(pred.sum(), \\\n\u001b[0;32m---> 33\u001b[0;31m                                         test_x, retain_graph=True)[0] for pred in sampled_pred])\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# last 100 rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-127-991c8077b37a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0msampled_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobserved_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             sampled_dydtest_x[:,(i*100):(i*100+100),:] = torch.stack([torch.autograd.grad(pred.sum(), \\\n\u001b[0;32m---> 33\u001b[0;31m                                         test_x, retain_graph=True)[0] for pred in sampled_pred])\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# last 100 rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    276\u001b[0m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    277\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# iterate over each mcmc sample\n",
    "est_means = np.zeros((num_samples, len(covariate_names)))\n",
    "est_stds = np.zeros((num_samples, len(covariate_names)))\n",
    "samples = mcmc_run.get_samples()\n",
    "for iter in range(num_samples):\n",
    "    one_sample = {}\n",
    "    for k,v in samples.items():\n",
    "        one_sample[k] = v[iter]\n",
    "    model.pyro_load_from_samples(one_sample)\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    df_std = np.zeros((train_x.size(0),train_x.size(1)))\n",
    "    x_grad = np.zeros((train_x.size(0),train_x.size(1)))\n",
    "\n",
    "    # number of empirically sample \n",
    "    n_samples = 100\n",
    "    sampled_dydtest_x = np.zeros((n_samples, train_x.size(0),train_x.size(1)))\n",
    "\n",
    "    # we proceed in small batches of size 100 for speed up\n",
    "\n",
    "    for i in range(train_x.size(0)//100):\n",
    "        with gpytorch.settings.fast_pred_var():\n",
    "            test_x = train_x[(i*100):(i*100+100)].clone().detach().requires_grad_(True)\n",
    "            observed_pred = model(test_x)\n",
    "            dydtest_x = torch.autograd.grad(observed_pred.mean.sum(), test_x, retain_graph=True)[0]\n",
    "            x_grad[(i*100):(i*100+100)] = dydtest_x\n",
    "\n",
    "            sampled_pred = observed_pred.rsample(torch.Size([n_samples]))\n",
    "            sampled_dydtest_x[:,(i*100):(i*100+100),:] = torch.stack([torch.autograd.grad(pred.sum(), \\\n",
    "                                        test_x, retain_graph=True)[0] for pred in sampled_pred])\n",
    "            \n",
    "    # last 100 rows\n",
    "    with gpytorch.settings.fast_pred_var():\n",
    "        test_x = train_x[(100*i+100):].clone().detach().requires_grad_(True)\n",
    "        observed_pred = model(test_x)\n",
    "        dydtest_x = torch.autograd.grad(observed_pred.mean.sum(), test_x, retain_graph=True)[0]\n",
    "        x_grad[(100*i+100):] = dydtest_x\n",
    "\n",
    "        sampled_pred = observed_pred.rsample(torch.Size([n_samples]))\n",
    "        sampled_dydtest_x[:,(100*i+100):,:] = torch.stack([torch.autograd.grad(pred.sum(),\\\n",
    "                                        test_x, retain_graph=True)[0] for pred in sampled_pred])\n",
    "        \n",
    "\n",
    "    est_std = np.sqrt(sampled_dydtest_x.mean(1).std(0)**2 + \\\n",
    "                    sampled_dydtest_x.std(1).mean(0)**2).round(decimals=5)\n",
    "    est_stds[iter] = est_std[2:10]\n",
    "    est_means[iter] = x_grad.mean(axis=0)[2:10]\n",
    "\n",
    "# print marginalized results\n",
    "results = pd.DataFrame({\"x\": covariate_names, \\\n",
    "                    'est_mean': est_means.mean(axis=0),\n",
    "                    'est_std': np.sqrt(np.var(est_means.mean(axis=0)) + np.power(est_stds, 2).mean(axis=0))})\n",
    "results[\"t\"] = results['est_mean'].values/results['est_std'].values\n",
    "results[\"pvalue\"] = 1 - norm.cdf(np.abs(results[\"t\"].values))\n",
    "results.to_csv(\"./results/PIRI_GPR_fullbayes.csv\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043],\n",
       "        [1.1043]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcmc_run.get_samples()[\"likelihood.noise_prior\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
