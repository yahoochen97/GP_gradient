{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of GPR for PIRI data with unit trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "# load packages\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gpytorch\n",
    "print(gpytorch.__version__)\n",
    "from typing import Optional, Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "from gpytorch.means import LinearMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement constant mean module and mask mean module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantVectorMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(self, d=1, prior=None, batch_shape=torch.Size(), **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_shape = batch_shape\n",
    "        self.register_parameter(name=\"constantvector\",\\\n",
    "                 parameter=torch.nn.Parameter(torch.zeros(*batch_shape, d)))\n",
    "        if prior is not None:\n",
    "            self.register_prior(\"mean_prior\", prior, \"constantvector\")\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.constantvector[input.int().reshape((-1,)).tolist()]\n",
    "    \n",
    "class MaskMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_mean: gpytorch.means.mean.Mean,\n",
    "        active_dims: Optional[Tuple[int, ...]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if active_dims is not None and not torch.is_tensor(active_dims):\n",
    "            active_dims = torch.tensor(active_dims, dtype=torch.long)\n",
    "        self.active_dims = active_dims\n",
    "        self.base_mean = base_mean\n",
    "    \n",
    "    def forward(self, x, **params):\n",
    "        return self.base_mean.forward(x.index_select(-1, self.active_dims), **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data with last 5 years splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996\n",
      " 1997 1998 1999 2000]\n"
     ]
    }
   ],
   "source": [
    "def load_PIRI_data():\n",
    "    # read data\n",
    "    data = pd.read_csv(\"hb_data_complete.csv\", index_col=[0])\n",
    "\n",
    "    # all zero PIRI for new zealand and netherland\n",
    "    data = data.loc[~data['country'].isin(['N-ZEAL','NETHERL'])]\n",
    "\n",
    "    countries = sorted(data.country.unique())\n",
    "    years = data.year.unique()\n",
    "    pred_countries  = ['MALAYSIA', 'CHINA', 'RUSSIA', 'KENYA', 'SPAIN']\n",
    "    pred_years = [1996, 1997, 1998, 1999, 2000]\n",
    "    n = len(countries)\n",
    "    m = len(years)\n",
    "    print(years)\n",
    "\n",
    "    # build data\n",
    "    country_dict = dict(zip(countries, range(n)))\n",
    "    year_dict = dict(zip(years, range(m)))\n",
    "\n",
    "    # x is:\n",
    "    # 1: year number\n",
    "    # 2: country id\n",
    "    # 3: AIShame (treatment indicator)\n",
    "    # 4: cat_rat\n",
    "    # 5: ccpr_rat\n",
    "    # 6: democratic\n",
    "    # 7: log(gdppc)\n",
    "    # 8: log(pop)\n",
    "    # 9: Civilwar2\n",
    "    # 10: War\n",
    "    x = torch.zeros(data.shape[0], 10)\n",
    "    x[:,0] = torch.as_tensor(list(map(year_dict.get, data.year)))\n",
    "    x[:,1] = torch.as_tensor(list(map(country_dict.get, data.country)))\n",
    "    x[:,2] = torch.as_tensor(data.AIShame.to_numpy())\n",
    "    x[:,3] = torch.as_tensor(data.cat_rat.to_numpy())\n",
    "    x[:,4] = torch.as_tensor(data.ccpr_rat.to_numpy())\n",
    "    x[:,5] = torch.as_tensor(data.democratic.to_numpy())\n",
    "    x[:,6] = torch.as_tensor(data.log_gdppc.to_numpy())\n",
    "    x[:,7] = torch.as_tensor(data.log_pop.to_numpy())\n",
    "    x[:,8] = torch.as_tensor(data.Civilwar2.to_numpy())\n",
    "    x[:,9] = torch.as_tensor(data.War.to_numpy())\n",
    "    y = torch.as_tensor(data.PIRI.to_numpy()).double()\n",
    "\n",
    "    # split data into training and testing by last 5 years\n",
    "    train_mask_year = torch.isin(x[:,0], torch.tensor(list(map(year_dict.get, pred_years))))\n",
    "    train_mask_country = torch.isin(x[:,1], torch.tensor(list(map(country_dict.get, pred_countries))))\n",
    "    train_mask = ~torch.logical_and(train_mask_year, train_mask_country)\n",
    "\n",
    "    train_x = x[train_mask]\n",
    "    test_x = x[~train_mask]\n",
    "    train_y = y[train_mask]\n",
    "    test_y = y[~train_mask]\n",
    "\n",
    "    unit_means = torch.zeros(n,)\n",
    "    for i in range(n):\n",
    "        unit_means[i] = train_y[train_x[:,1]==i].mean()\n",
    "\n",
    "    return train_x.double(), train_y.double(), test_x.double(), test_y.double(), unit_means.double(), data, countries, years\n",
    "\n",
    "train_x, train_y, test_x, test_y, unit_means, data, countries, years = load_PIRI_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build GPR model with unit trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model specification: PIRI gp model with unit trends\n",
    "# PIRI ~ AIShame + u_i(t) + cat_rat + ccpr_rat \n",
    "#            + democratic + log(gdppc) + log(pop) \n",
    "#            + Civilwar2 + War\n",
    "# u_i(t) ~ GP(b_i, K_t)\n",
    "\n",
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, ard_num_dim=None):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = MaskMean(active_dims=1, \\\n",
    "                base_mean=ConstantVectorMean(d=train_x[:,1].unique().size()[0]))\n",
    "        self.x_mean_module = MaskMean(active_dims=[2,3,4,5,6,7,8,9], \\\n",
    "                                      base_mean=LinearMean(input_size=8,bias=False))\n",
    "        # year kernel * country kernel\n",
    "        self.unit_covar_module = ScaleKernel(RBFKernel(active_dims=0)*RBFKernel(active_dims=1))\n",
    "        self.x_covar_module = torch.nn.ModuleList([ScaleKernel(RBFKernel(\\\n",
    "            active_dims=(i))) for i in [6,7]])\n",
    "        self.binary_covar_module = torch.nn.ModuleList([ScaleKernel(RBFKernel(\\\n",
    "            active_dims=(i))) for i in [3,4,5,8,9]])\n",
    "        self.effect_covar_module = ScaleKernel(RBFKernel(active_dims=2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x) + self.x_mean_module(x)\n",
    "        unit_covar_x = self.unit_covar_module(x)\n",
    "        effect_covar_x = self.effect_covar_module(x)\n",
    "        covar_x = unit_covar_x + effect_covar_x\n",
    "        for i, _ in enumerate(self.x_covar_module):\n",
    "            covar_x += self.x_covar_module[i](x)\n",
    "        for i, _ in enumerate(self.binary_covar_module):\n",
    "            covar_x += self.binary_covar_module[i](x)\n",
    "        \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPModel(\n",
       "  (likelihood): GaussianLikelihood(\n",
       "    (noise_covar): HomoskedasticNoise(\n",
       "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "    )\n",
       "  )\n",
       "  (mean_module): MaskMean(\n",
       "    (base_mean): ConstantVectorMean()\n",
       "  )\n",
       "  (x_mean_module): MaskMean(\n",
       "    (base_mean): LinearMean()\n",
       "  )\n",
       "  (unit_covar_module): ScaleKernel(\n",
       "    (base_kernel): ProductKernel(\n",
       "      (kernels): ModuleList(\n",
       "        (0): RBFKernel(\n",
       "          (raw_lengthscale_constraint): Positive()\n",
       "        )\n",
       "        (1): RBFKernel(\n",
       "          (raw_lengthscale_constraint): Positive()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (raw_outputscale_constraint): Positive()\n",
       "  )\n",
       "  (x_covar_module): ModuleList(\n",
       "    (0): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (1): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "  )\n",
       "  (binary_covar_module): ModuleList(\n",
       "    (0): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (1): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (2): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (3): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (4): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "  )\n",
       "  (effect_covar_module): ScaleKernel(\n",
       "    (base_kernel): RBFKernel(\n",
       "      (raw_lengthscale_constraint): Positive()\n",
       "    )\n",
       "    (raw_outputscale_constraint): Positive()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood = GaussianLikelihood()\n",
    "model = GPModel(train_x, train_y, likelihood).double()\n",
    "\n",
    "# initialize model parameters\n",
    "hypers = {\n",
    "    'mean_module.base_mean.constantvector': unit_means,\n",
    "    'likelihood.noise_covar.noise': torch.tensor(0.25),\n",
    "    'unit_covar_module.base_kernel.kernels.0.lengthscale': torch.tensor(4),\n",
    "    'unit_covar_module.base_kernel.kernels.1.lengthscale': torch.tensor(0.01),\n",
    "    'unit_covar_module.outputscale': torch.tensor(0.25),\n",
    "    'x_covar_module.0.outputscale': torch.tensor(0.25),\n",
    "    'x_covar_module.1.outputscale': torch.tensor(0.25),\n",
    "    'binary_covar_module.0.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.1.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.2.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.3.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.4.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'effect_covar_module.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'effect_covar_module.outputscale': torch.tensor(0.25)\n",
    "}    \n",
    "\n",
    "model.initialize(**hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model by optimizing hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/20 - Loss: 2.347 \n",
      "Iter 2/20 - Loss: 2.210 \n",
      "Iter 3/20 - Loss: 2.099 \n",
      "Iter 4/20 - Loss: 2.002 \n",
      "Iter 5/20 - Loss: 1.917 \n",
      "Iter 6/20 - Loss: 1.848 \n",
      "Iter 7/20 - Loss: 1.803 \n",
      "Iter 8/20 - Loss: 1.754 \n",
      "Iter 9/20 - Loss: 1.715 \n",
      "Iter 10/20 - Loss: 1.683 \n",
      "Iter 11/20 - Loss: 1.657 \n",
      "Iter 12/20 - Loss: 1.637 \n",
      "Iter 13/20 - Loss: 1.621 \n",
      "Iter 14/20 - Loss: 1.609 \n",
      "Iter 15/20 - Loss: 1.593 \n",
      "Iter 16/20 - Loss: 1.591 \n",
      "Iter 17/20 - Loss: 1.586 \n",
      "Iter 18/20 - Loss: 1.583 \n",
      "Iter 19/20 - Loss: 1.574 \n",
      "Iter 20/20 - Loss: 1.578 \n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "# freeze length scale in the country component in unit covar\n",
    "# freeze constant unit means\n",
    "all_params = set(model.parameters())\n",
    "final_params = list(all_params - \\\n",
    "            {model.unit_covar_module.base_kernel.kernels[1].raw_lengthscale, \\\n",
    "            model.mean_module.base_mean.constantvector, \\\n",
    "        #   model.x_covar_module[0].raw_outputscale,\n",
    "        #   model.x_covar_module[1].raw_outputscale,\n",
    "            model.binary_covar_module[0].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[1].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[2].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[3].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[4].base_kernel.raw_lengthscale,\n",
    "            model.effect_covar_module.base_kernel.raw_lengthscale})\n",
    "        #   model.effect_covar_module.raw_outputscale})\n",
    "optimizer = torch.optim.Adam(final_params, lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 20\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f '  % (\n",
    "        i + 1, training_iter, loss.item()\n",
    "    ))\n",
    "    optimizer.step()\n",
    "\n",
    "torch.save(model.state_dict(), \"PIRI_GPR_model_5yr.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate posterior of PIRI effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effect: 0.148 +- 0.501\n",
      "\n",
      "model evidence: -3332.483 \n",
      "\n",
      "BIC: 6764.486 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('PIRI_GPR_model_5yr.pth'))\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out = likelihood(model(train_x))\n",
    "    mu_f = out.mean\n",
    "    V = out.covariance_matrix\n",
    "    L = torch.linalg.cholesky(V, upper=False)\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    model.unit_covar_module.outputscale = 0\n",
    "    for i,_ in enumerate(model.x_covar_module):\n",
    "        model.x_covar_module[i].outputscale = 0\n",
    "    for i,_ in enumerate(model.binary_covar_module):\n",
    "        model.binary_covar_module[i].outputscale = 0\n",
    "    effect_covar = model(train_x).covariance_matrix\n",
    "\n",
    "# get posterior effect mean\n",
    "alpha = torch.linalg.solve(L.t(),torch.linalg.solve(L,train_y-mu_f))\n",
    "tmp = torch.linalg.solve(L, effect_covar)\n",
    "post_effect_mean = effect_covar @ alpha\n",
    "# get posterior effect covariance\n",
    "post_effect_covar = effect_covar - tmp.t() @ tmp\n",
    "\n",
    "effect = post_effect_mean[train_x[:,2]==1].mean() - post_effect_mean[train_x[:,2]==0].mean()\n",
    "effect_std = post_effect_covar.diag().mean().sqrt()\n",
    "BIC = (2+4+6+1)*torch.log(torch.tensor(train_x.size()[0])) + 2*loss*train_x.size()[0]\n",
    "print(\"effect: {:0.3f} +- {:0.3f}\\n\".format(effect, effect_std))\n",
    "print(\"model evidence: {:0.3f} \\n\".format(-loss*train_x.size()[0]))\n",
    "print(\"BIC: {:0.3f} \\n\".format(BIC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Durbin Watson tests for autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 138 residuals are positively correlated.\n",
      "\n",
      "1 out of 138 residuals are negatively correlated.\n",
      "\n",
      "137 out of 138 residuals are not correlated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get unit trend wo AIShame\n",
    "model.load_state_dict(torch.load('PIRI_GPR_model_5yr.pth'))\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    model.effect_covar_module.outputscale = 0\n",
    "    unit_covar = likelihood(model(train_x)).covariance_matrix\n",
    "\n",
    "# get posterior unit trend mean\n",
    "alpha = torch.linalg.solve(L.t(),torch.linalg.solve(L,train_y-mu_f))\n",
    "tmp = torch.linalg.solve(L, unit_covar)\n",
    "post_unit_mean = mu_f + unit_covar @ alpha + post_effect_mean\n",
    "\n",
    "# DW-test for sample size = 13 and 8 regressors.\n",
    "dL = 0.147\n",
    "dU = 3.266\n",
    "n = len(countries)\n",
    "DW_results = np.zeros((n,))\n",
    "for i in range(n):\n",
    "    mask = (train_x[:,1]==i).numpy()\n",
    "    res = train_y[mask] - post_unit_mean[mask]\n",
    "    DW_results[i] = durbin_watson(res.detach().numpy())\n",
    "\n",
    "print(\"{} out of {} residuals are positively correlated.\\n\".format(np.sum(DW_results<=dL),n))\n",
    "print(\"{} out of {} residuals are negatively correlated.\\n\".format(np.sum(DW_results>=dU),n))\n",
    "print(\"{} out of {} residuals are not correlated.\\n\".format(np.sum((DW_results>dL) & (DW_results<dU)),n))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('PIRI_GPR_model_5yr.pth'))\n",
    "\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    y_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_np = y_pred.mean.numpy() #convert to Numpy array\n",
    "\n",
    "df = pd.DataFrame(y_pred_np) #convert to a dataframe\n",
    "df['true_y'] = test_y\n",
    "df.to_csv(\"predicted_5_year.csv\",index=False) #save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f7e8e734630>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(test_y.numpy(),y_pred_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6889781667644885"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(test_y.numpy(),y_pred_np)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
