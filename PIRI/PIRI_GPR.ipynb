{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of GPR for PIRI data with unit trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gpytorch\n",
    "from scipy.stats import norm\n",
    "from typing import Optional, Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "from gpytorch.means import LinearMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from statsmodels.stats.stattools import durbin_watson"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement constant mean module and mask mean module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantVectorMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(self, d=1, prior=None, batch_shape=torch.Size(), **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_shape = batch_shape\n",
    "        self.register_parameter(name=\"constantvector\",\\\n",
    "                 parameter=torch.nn.Parameter(torch.zeros(*batch_shape, d)))\n",
    "        if prior is not None:\n",
    "            self.register_prior(\"mean_prior\", prior, \"constantvector\")\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.constantvector[input.int().reshape((-1,)).tolist()]\n",
    "    \n",
    "class MaskMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_mean: gpytorch.means.mean.Mean,\n",
    "        active_dims: Optional[Tuple[int, ...]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if active_dims is not None and not torch.is_tensor(active_dims):\n",
    "            active_dims = torch.tensor(active_dims, dtype=torch.long)\n",
    "        self.active_dims = active_dims\n",
    "        self.base_mean = base_mean\n",
    "    \n",
    "    def forward(self, x, **params):\n",
    "        return self.base_mean.forward(x.index_select(-1, self.active_dims), **params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_PIRI_data():\n",
    "    # read data\n",
    "    data = pd.read_csv(\"hb_data_complete.csv\", index_col=[0])\n",
    "\n",
    "    # all zero PIRI for new zealand and netherland\n",
    "    data = data.loc[~data['country'].isin(['N-ZEAL','NETHERL'])]\n",
    "\n",
    "    countries = sorted(data.country.unique())\n",
    "    years = data.year.unique()\n",
    "    n = len(countries)\n",
    "    m = len(years)\n",
    "\n",
    "    # build data\n",
    "    country_dict = dict(zip(countries, range(n)))\n",
    "    year_dict = dict(zip(years, range(m)))\n",
    "\n",
    "    # x is:\n",
    "    # 1: year number\n",
    "    # 2: country id\n",
    "    # 3: AIShame (treatment indicator)\n",
    "    # 4: cat_rat\n",
    "    # 5: ccpr_rat\n",
    "    # 6: democratic\n",
    "    # 7: log(gdppc)\n",
    "    # 8: log(pop)\n",
    "    # 9: Civilwar2\n",
    "    # 10: War\n",
    "    x = torch.zeros(data.shape[0], 10)\n",
    "    x[:,0] = torch.as_tensor(list(map(year_dict.get, data.year)))\n",
    "    x[:,1] = torch.as_tensor(list(map(country_dict.get, data.country)))\n",
    "    x[:,2] = torch.as_tensor(data.AIShame.to_numpy())\n",
    "    x[:,3] = torch.as_tensor(data.cat_rat.to_numpy())\n",
    "    x[:,4] = torch.as_tensor(data.ccpr_rat.to_numpy())\n",
    "    x[:,5] = torch.as_tensor(data.democratic.to_numpy())\n",
    "    x[:,6] = torch.as_tensor(data.log_gdppc.to_numpy())\n",
    "    x[:,7] = torch.as_tensor(data.log_pop.to_numpy())\n",
    "    x[:,8] = torch.as_tensor(data.Civilwar2.to_numpy())\n",
    "    x[:,9] = torch.as_tensor(data.War.to_numpy())\n",
    "    # x[:,10] = torch.as_tensor(data.PIRI.to_numpy())\n",
    "    y = torch.as_tensor(data.PIRILead1.to_numpy()).double()\n",
    "\n",
    "    unit_means = torch.zeros(n,)\n",
    "    for i in range(n):\n",
    "        unit_means[i] = y[x[:,1]==i].mean()\n",
    "\n",
    "    return x.double(), y.double(), unit_means.double(), data, countries, years\n",
    "\n",
    "train_x, train_y, unit_means, data, countries, years = load_PIRI_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build GPR model with unit trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model specification: PIRI gp model with unit trends\n",
    "# PIRI ~ AIShame + u_i(t) + cat_rat + ccpr_rat \n",
    "#            + democratic + log(gdppc) + log(pop) \n",
    "#            + Civilwar2 + War\n",
    "# u_i(t) ~ GP(b_i, K_t)\n",
    "\n",
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = MaskMean(active_dims=1, \\\n",
    "                base_mean=ConstantVectorMean(d=train_x[:,1].unique().size()[0]))\n",
    "        # linear mean for continuous covariates\n",
    "        self.x_mean_module = MaskMean(active_dims=[2,3,4,5,6,7,8,9], base_mean=LinearMean(input_size=8, bias=False))\n",
    "        # year kernel * country kernel\n",
    "        self.unit_covar_module = ScaleKernel(RBFKernel(active_dims=0)*RBFKernel(active_dims=1))\n",
    "        self.x_covar_module = torch.nn.ModuleList([ScaleKernel(RBFKernel(\\\n",
    "            active_dims=(i))) for i in [6,7]])\n",
    "        self.binary_covar_module = torch.nn.ModuleList([ScaleKernel(RBFKernel(\\\n",
    "            active_dims=(i))) for i in [3,4,5,8,9]])\n",
    "        self.effect_covar_module = ScaleKernel(RBFKernel(active_dims=2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.x_mean_module(x) + self.mean_module(x)\n",
    "        unit_covar_x = self.unit_covar_module(x)\n",
    "        effect_covar_x = self.effect_covar_module(x)\n",
    "        covar_x = unit_covar_x + effect_covar_x\n",
    "        for i, _ in enumerate(self.x_covar_module):\n",
    "            covar_x += self.x_covar_module[i](x)\n",
    "        for i, _ in enumerate(self.binary_covar_module):\n",
    "            covar_x += self.binary_covar_module[i](x)\n",
    "        \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              PIRILead1   R-squared:                       0.465\n",
      "Model:                            OLS   Adj. R-squared:                  0.463\n",
      "Method:                 Least Squares   F-statistic:                     231.0\n",
      "Date:                Mon, 21 Aug 2023   Prob (F-statistic):          2.60e-282\n",
      "Time:                        17:51:06   Log-Likelihood:                -4118.3\n",
      "No. Observations:                2137   AIC:                             8255.\n",
      "Df Residuals:                    2128   BIC:                             8306.\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.1340      0.453     -0.296      0.767      -1.022       0.754\n",
      "AIShame        0.8887      0.081     10.922      0.000       0.729       1.048\n",
      "cat_rat        0.1380      0.084      1.633      0.103      -0.028       0.304\n",
      "ccpr_rat       0.1286      0.087      1.476      0.140      -0.042       0.299\n",
      "democratic    -1.0142      0.088    -11.579      0.000      -1.186      -0.842\n",
      "log_gdppc     -0.4836      0.027    -17.897      0.000      -0.537      -0.431\n",
      "log_pop        0.4192      0.026     16.331      0.000       0.369       0.470\n",
      "Civilwar2      1.8976      0.139     13.691      0.000       1.626       2.169\n",
      "War            1.0251      0.643      1.594      0.111      -0.236       2.287\n",
      "==============================================================================\n",
      "Omnibus:                       52.218   Durbin-Watson:                   0.906\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               55.692\n",
      "Skew:                           0.393   Prob(JB):                     8.06e-13\n",
      "Kurtosis:                       2.919   Cond. No.                         319.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "\n",
    "lm = sm.ols('PIRILead1 ~ AIShame  + cat_rat + ccpr_rat \\\n",
    "            + democratic + log_gdppc + log_pop \\\n",
    "            + Civilwar2 + War', data).fit()\n",
    "print(lm.summary())\n",
    "\n",
    "coefs = lm.params.to_dict()\n",
    "covariate_names = [\"AIShame\" ,\"cat_rat\" , \"ccpr_rat\",\n",
    "           \"democratic\",  \"log_gdppc\", \"log_pop\",\n",
    "            \"Civilwar2\", \"War\"]\n",
    "x_weights = list(map(coefs.get, covariate_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPModel(\n",
       "  (likelihood): GaussianLikelihood(\n",
       "    (noise_covar): HomoskedasticNoise(\n",
       "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "    )\n",
       "  )\n",
       "  (mean_module): MaskMean(\n",
       "    (base_mean): ConstantVectorMean()\n",
       "  )\n",
       "  (x_mean_module): MaskMean(\n",
       "    (base_mean): LinearMean()\n",
       "  )\n",
       "  (unit_covar_module): ScaleKernel(\n",
       "    (base_kernel): ProductKernel(\n",
       "      (kernels): ModuleList(\n",
       "        (0): RBFKernel(\n",
       "          (raw_lengthscale_constraint): Positive()\n",
       "        )\n",
       "        (1): RBFKernel(\n",
       "          (raw_lengthscale_constraint): Positive()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (raw_outputscale_constraint): Positive()\n",
       "  )\n",
       "  (x_covar_module): ModuleList(\n",
       "    (0): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (1): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "  )\n",
       "  (binary_covar_module): ModuleList(\n",
       "    (0): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (1): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (2): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (3): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (4): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "  )\n",
       "  (effect_covar_module): ScaleKernel(\n",
       "    (base_kernel): RBFKernel(\n",
       "      (raw_lengthscale_constraint): Positive()\n",
       "    )\n",
       "    (raw_outputscale_constraint): Positive()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood = GaussianLikelihood()\n",
    "model = GPModel(train_x, train_y, likelihood).double()\n",
    "\n",
    "# initialize model parameters\n",
    "hypers = {\n",
    "    'mean_module.base_mean.constantvector': unit_means,\n",
    "    'x_mean_module.base_mean.weights': torch.tensor(x_weights),\n",
    "    'likelihood.noise_covar.noise': torch.tensor(0.25),\n",
    "    'unit_covar_module.base_kernel.kernels.0.lengthscale': torch.tensor(4),\n",
    "    'unit_covar_module.base_kernel.kernels.1.lengthscale': torch.tensor(0.01),\n",
    "    'unit_covar_module.outputscale': torch.tensor(1),\n",
    "    'x_covar_module.0.outputscale': torch.tensor(0.25),\n",
    "    'x_covar_module.1.outputscale': torch.tensor(0.25),\n",
    "    'binary_covar_module.0.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.1.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.2.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.3.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.4.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.0.outputscale': torch.tensor(0.25),\n",
    "    'binary_covar_module.1.outputscale': torch.tensor(0.25),\n",
    "    'binary_covar_module.2.outputscale': torch.tensor(0.25),\n",
    "    'binary_covar_module.3.outputscale': torch.tensor(0.25),\n",
    "    'binary_covar_module.4.outputscale': torch.tensor(0.25),\n",
    "    'effect_covar_module.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'effect_covar_module.outputscale': torch.tensor(0.25)\n",
    "}    \n",
    "\n",
    "model.initialize(**hypers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model by optimizing hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/5 - Loss: 2.175 \n",
      "Iter 2/5 - Loss: 2.073 \n",
      "Iter 3/5 - Loss: 1.987 \n",
      "Iter 4/5 - Loss: 1.919 \n",
      "Iter 5/5 - Loss: 1.842 \n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "# freeze length scale in the country component in unit covar\n",
    "# freeze constant unit means\n",
    "all_params = set(model.parameters())\n",
    "final_params = list(all_params - \\\n",
    "            {model.unit_covar_module.base_kernel.kernels[1].raw_lengthscale, \\\n",
    "        #    model.unit_covar_module.raw_outputscale, \\\n",
    "        #    model.mean_module.base_mean.constantvector, \\\n",
    "        #   model.x_covar_module[0].raw_outputscale,\n",
    "        #   model.x_covar_module[1].raw_outputscale,\n",
    "            model.binary_covar_module[0].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[1].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[2].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[3].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[4].base_kernel.raw_lengthscale,\n",
    "            model.effect_covar_module.base_kernel.raw_lengthscale})\n",
    "        #    model.effect_covar_module.raw_outputscale})\n",
    "optimizer = torch.optim.Adam(final_params, lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 5\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f '  % (\n",
    "        i + 1, training_iter, loss.item()\n",
    "    ))\n",
    "    optimizer.step()\n",
    "\n",
    "torch.save(model.state_dict(), \"PIRI_GPR_model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate posterior of PIRI effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model evidence: 3217.434 \n",
      "\n",
      "BIC: -6335.196 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('PIRI_GPR_model.pth'))\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out = likelihood(model(train_x))\n",
    "    mu_f = out.mean\n",
    "    V = out.covariance_matrix\n",
    "    L = torch.linalg.cholesky(V, upper=False)\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    model.unit_covar_module.outputscale = 0\n",
    "    for i,_ in enumerate(model.x_covar_module):\n",
    "        model.x_covar_module[i].outputscale = 0\n",
    "    for i,_ in enumerate(model.binary_covar_module):\n",
    "        model.binary_covar_module[i].outputscale = 0\n",
    "    effect_covar = model(train_x).covariance_matrix\n",
    "\n",
    "# get posterior effect mean\n",
    "alpha = torch.linalg.solve(L.t(),torch.linalg.solve(L,train_y-mu_f))\n",
    "tmp = torch.linalg.solve(L, effect_covar)\n",
    "post_effect_mean =   mu_f + effect_covar @ alpha\n",
    "# get posterior effect covariance\n",
    "post_effect_covar = effect_covar - tmp.t() @ tmp\n",
    "\n",
    "effect = post_effect_mean[train_x[:,2]==1].mean() - post_effect_mean[train_x[:,2]==0].mean()\n",
    "effect_std = post_effect_covar.diag().mean().sqrt()\n",
    "BIC = (2+4+6+1)*torch.log(torch.tensor(train_x.size()[0])) + 2*loss*train_x.size()[0]\n",
    "# print(\"effect: {:0.3f} +- {:0.3f}\\n\".format(effect, effect_std))\n",
    "print(\"model evidence: {:0.3f} \\n\".format(-loss*train_x.size()[0]))\n",
    "print(\"BIC: {:0.3f} \\n\".format(BIC))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Durbin Watson tests for autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 out of 138 residuals are positively correlated.\n",
      "\n",
      "0 out of 138 residuals are negatively correlated.\n",
      "\n",
      "8 out of 138 residuals are not correlated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get unit trend wo AIShame\n",
    "model.load_state_dict(torch.load('PIRI_GPR_model.pth'))\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    model.effect_covar_module.outputscale = 0\n",
    "    unit_covar = likelihood(model(train_x)).covariance_matrix\n",
    "\n",
    "# get posterior unit trend mean\n",
    "alpha = torch.linalg.solve(L.t(),torch.linalg.solve(L,train_y-mu_f))\n",
    "tmp = torch.linalg.solve(L, unit_covar)\n",
    "post_unit_mean = mu_f + unit_covar @ alpha + post_effect_mean\n",
    "\n",
    "# DW-test for sample size = 18 and 9 regressors.\n",
    "dL = 0.32\n",
    "dU = 2.87\n",
    "n = len(countries)\n",
    "DW_results = np.zeros((n,))\n",
    "for i in range(n):\n",
    "    mask = data.country==countries[i]\n",
    "    mask = mask.to_list()\n",
    "    res = train_y[mask] - post_unit_mean[mask]\n",
    "    DW_results[i] = durbin_watson(res.detach().numpy())\n",
    "\n",
    "print(\"{} out of {} residuals are positively correlated.\\n\".format(np.sum(DW_results<=dL),n))\n",
    "print(\"{} out of {} residuals are negatively correlated.\\n\".format(np.sum(DW_results>=dU),n))\n",
    "print(\"{} out of {} residuals are not correlated.\\n\".format(np.sum((DW_results>dL) & (DW_results<dU)),n))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot fitted mean trend and CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yahoo/anaconda3/lib/python3.7/site-packages/gpytorch/models/exact_gp.py:275: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  GPInputWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gpr_mean  true_y   gpr_lwr   gpr_upr  year country\n",
      "0  0.251448     0.0 -2.366495  2.869390  1983     USA\n",
      "1  0.313593     0.0 -2.286459  2.913645  1984     USA\n",
      "2  0.768128     1.0 -1.815695  3.351952  1985     USA\n",
      "3  0.465908     1.0 -2.100868  3.032684  1986     USA\n",
      "4  0.868379     0.0 -1.686243  3.423002  1987     USA\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('PIRI_GPR_model.pth'))\n",
    "\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out = likelihood(model(train_x))\n",
    "    mu_f = out.mean.numpy()\n",
    "    lower, upper = out.confidence_region()\n",
    "\n",
    "results = pd.DataFrame({\"gpr_mean\":mu_f})\n",
    "results['true_y'] = train_y\n",
    "results['gpr_lwr'] = lower\n",
    "results['gpr_upr'] = upper\n",
    "results['year'] = years[train_x[:,0].numpy().astype(int)]\n",
    "results['country'] = [countries[i] for i in train_x[:,1].numpy().astype(int)]\n",
    "print(results.head())\n",
    "results.to_csv(\"./results/PIRI_fitted_gpr.csv\",index=False) #save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpr_mean</th>\n",
       "      <th>true_y</th>\n",
       "      <th>gpr_lwr</th>\n",
       "      <th>gpr_upr</th>\n",
       "      <th>year</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>5.700728</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.082248</td>\n",
       "      <td>8.319207</td>\n",
       "      <td>1983</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>5.467938</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.881896</td>\n",
       "      <td>8.053980</td>\n",
       "      <td>1984</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>4.910411</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.352177</td>\n",
       "      <td>7.468645</td>\n",
       "      <td>1985</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>4.910006</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.374480</td>\n",
       "      <td>7.445531</td>\n",
       "      <td>1986</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>5.491105</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.970757</td>\n",
       "      <td>8.011453</td>\n",
       "      <td>1987</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>5.516754</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.010289</td>\n",
       "      <td>8.023218</td>\n",
       "      <td>1988</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>5.958619</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.464083</td>\n",
       "      <td>8.453156</td>\n",
       "      <td>1989</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>5.796347</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.313799</td>\n",
       "      <td>8.278894</td>\n",
       "      <td>1990</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>5.606905</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.134507</td>\n",
       "      <td>8.079302</td>\n",
       "      <td>1991</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>5.648193</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.178968</td>\n",
       "      <td>8.117417</td>\n",
       "      <td>1992</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>5.586728</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.111184</td>\n",
       "      <td>8.062273</td>\n",
       "      <td>1993</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>6.316937</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.837716</td>\n",
       "      <td>8.796159</td>\n",
       "      <td>1994</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>6.462571</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.976680</td>\n",
       "      <td>8.948462</td>\n",
       "      <td>1995</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>6.329591</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.832722</td>\n",
       "      <td>8.826460</td>\n",
       "      <td>1996</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>6.059123</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.543941</td>\n",
       "      <td>8.574305</td>\n",
       "      <td>1997</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>5.808005</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.266332</td>\n",
       "      <td>8.349679</td>\n",
       "      <td>1998</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>5.644892</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.068773</td>\n",
       "      <td>8.221011</td>\n",
       "      <td>1999</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>5.564956</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.947361</td>\n",
       "      <td>8.182552</td>\n",
       "      <td>2000</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gpr_mean  true_y   gpr_lwr   gpr_upr  year country\n",
       "1799  5.700728     6.0  3.082248  8.319207  1983   CHINA\n",
       "1800  5.467938     6.0  2.881896  8.053980  1984   CHINA\n",
       "1801  4.910411     4.0  2.352177  7.468645  1985   CHINA\n",
       "1802  4.910006     5.0  2.374480  7.445531  1986   CHINA\n",
       "1803  5.491105     4.0  2.970757  8.011453  1987   CHINA\n",
       "1804  5.516754     6.0  3.010289  8.023218  1988   CHINA\n",
       "1805  5.958619     8.0  3.464083  8.453156  1989   CHINA\n",
       "1806  5.796347     6.0  3.313799  8.278894  1990   CHINA\n",
       "1807  5.606905     4.0  3.134507  8.079302  1991   CHINA\n",
       "1808  5.648193     5.0  3.178968  8.117417  1992   CHINA\n",
       "1809  5.586728     5.0  3.111184  8.062273  1993   CHINA\n",
       "1810  6.316937     8.0  3.837716  8.796159  1994   CHINA\n",
       "1811  6.462571     8.0  3.976680  8.948462  1995   CHINA\n",
       "1812  6.329591     5.0  3.832722  8.826460  1996   CHINA\n",
       "1813  6.059123     6.0  3.543941  8.574305  1997   CHINA\n",
       "1814  5.808005     6.0  3.266332  8.349679  1998   CHINA\n",
       "1815  5.644892     6.0  3.068773  8.221011  1999   CHINA\n",
       "1816  5.564956     5.0  2.947361  8.182552  2000   CHINA"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results.country==\"CHINA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use autogradient to generate posterior variance of marginal effects in PIRI by small batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('PIRI_GPR_model.pth'))\n",
    "\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "df_std = np.zeros((train_x.size(0),train_x.size(1)))\n",
    "x_grad = np.zeros((train_x.size(0),train_x.size(1)))\n",
    "\n",
    "# small batches of size 100\n",
    "for i in range(train_x.size(0)//100):\n",
    "    with gpytorch.settings.fast_pred_var():\n",
    "        test_x = train_x[(i*100):(i*100+100)].clone().detach().requires_grad_(True)\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "        dydtest_x = torch.autograd.grad(observed_pred.mean.sum(), test_x, retain_graph=True)[0]\n",
    "        x_grad[(i*100):(i*100+100)] = dydtest_x\n",
    "        loss = mll(observed_pred, train_y[(i*100):(i*100+100)])\n",
    "\n",
    "        n_samples = 25\n",
    "        sampled_pred = observed_pred.rsample(torch.Size([n_samples]))\n",
    "        sampled_dydtest_x = torch.stack([torch.autograd.grad(pred.sum(), test_x, retain_graph=True)[0] for pred in sampled_pred])\n",
    "        df_std[(i*100):(i*100+100)] = sampled_dydtest_x.std(0)\n",
    "        \n",
    "# last 100 rows\n",
    "with gpytorch.settings.fast_pred_var():\n",
    "    test_x = train_x[(100*i+100):].clone().detach().requires_grad_(True)\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "    dydtest_x = torch.autograd.grad(observed_pred.mean.sum(), test_x, retain_graph=True)[0]\n",
    "    x_grad[(100*i+100):] = dydtest_x\n",
    "    loss = mll(observed_pred, train_y[(100*i+100):])\n",
    "\n",
    "    sampled_pred = observed_pred.rsample(torch.Size([n_samples]))\n",
    "    sampled_dydtest_x = torch.stack([torch.autograd.grad(pred.sum(), test_x, retain_graph=True)[0] for pred in sampled_pred])\n",
    "    df_std[(100*i+100):] = sampled_dydtest_x.std(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accesse marginal effects of regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            x  est_mean   est_std         t    pvalue\n",
      "0     AIShame  0.455517  0.000000       inf  0.000000\n",
      "1     cat_rat -0.177745  0.000000      -inf  0.000000\n",
      "2    ccpr_rat -0.125010  0.000000      -inf  0.000000\n",
      "3  democratic -1.027804  0.000000      -inf  0.000000\n",
      "4   log_gdppc -0.016395  0.064941 -0.252463  0.400342\n",
      "5     log_pop  0.042709  0.043206  0.988489  0.161457\n",
      "6   Civilwar2  1.439676  0.000000       inf  0.000000\n",
      "7         War  0.584848  0.000000       inf  0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yahoo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "est_std = (df_std).mean(axis=0).round(decimals=6)\n",
    "results = pd.DataFrame({\"x\": covariate_names, \\\n",
    "                        'est_mean': x_grad.mean(axis=0)[2:10],\n",
    "                        'est_std': est_std[2:10]})\n",
    "results[\"t\"] = results['est_mean'].values/results['est_std'].values\n",
    "results[\"pvalue\"] = 1 - norm.cdf(np.abs(results[\"t\"].values))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
