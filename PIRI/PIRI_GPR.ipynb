{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of GPR for PIRI data with unit trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gpytorch\n",
    "from typing import Optional, Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "from gpytorch.means import LinearMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from statsmodels.stats.stattools import durbin_watson"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement constant mean module and mask mean module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantVectorMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(self, d=1, prior=None, batch_shape=torch.Size(), **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_shape = batch_shape\n",
    "        self.register_parameter(name=\"constantvector\",\\\n",
    "                 parameter=torch.nn.Parameter(torch.zeros(*batch_shape, d)))\n",
    "        if prior is not None:\n",
    "            self.register_prior(\"mean_prior\", prior, \"constantvector\")\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.constantvector[input.int().reshape((-1,)).tolist()]\n",
    "    \n",
    "class MaskMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_mean: gpytorch.means.mean.Mean,\n",
    "        active_dims: Optional[Tuple[int, ...]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if active_dims is not None and not torch.is_tensor(active_dims):\n",
    "            active_dims = torch.tensor(active_dims, dtype=torch.long)\n",
    "        self.active_dims = active_dims\n",
    "        self.base_mean = base_mean\n",
    "    \n",
    "    def forward(self, x, **params):\n",
    "        return self.base_mean.forward(x.index_select(-1, self.active_dims), **params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_PIRI_data():\n",
    "    # read data\n",
    "    data = pd.read_csv(\"hb_data_complete.csv\", index_col=[0])\n",
    "\n",
    "    # all zero PIRI for new zealand and netherland\n",
    "    data = data.loc[~data['country'].isin(['N-ZEAL','NETHERL'])]\n",
    "\n",
    "    countries = sorted(data.country.unique())\n",
    "    years = data.year.unique()\n",
    "    n = len(countries)\n",
    "    m = len(years)\n",
    "\n",
    "    # build data\n",
    "    country_dict = dict(zip(countries, range(n)))\n",
    "    year_dict = dict(zip(years, range(m)))\n",
    "\n",
    "    # x is:\n",
    "    # 1: year number\n",
    "    # 2: country id\n",
    "    # 3: AIShame (treatment indicator)\n",
    "    # 4: cat_rat\n",
    "    # 5: ccpr_rat\n",
    "    # 6: democratic\n",
    "    # 7: log(gdppc)\n",
    "    # 8: log(pop)\n",
    "    # 9: Civilwar2\n",
    "    # 10: War\n",
    "    x = torch.zeros(data.shape[0], 10)\n",
    "    x[:,0] = torch.as_tensor(list(map(year_dict.get, data.year)))\n",
    "    x[:,1] = torch.as_tensor(list(map(country_dict.get, data.country)))\n",
    "    x[:,2] = torch.as_tensor(data.AIShame.to_numpy())\n",
    "    x[:,3] = torch.as_tensor(data.cat_rat.to_numpy())\n",
    "    x[:,4] = torch.as_tensor(data.ccpr_rat.to_numpy())\n",
    "    x[:,5] = torch.as_tensor(data.democratic.to_numpy())\n",
    "    x[:,6] = torch.as_tensor(data.log_gdppc.to_numpy())\n",
    "    x[:,7] = torch.as_tensor(data.log_pop.to_numpy())\n",
    "    x[:,8] = torch.as_tensor(data.Civilwar2.to_numpy())\n",
    "    x[:,9] = torch.as_tensor(data.War.to_numpy())\n",
    "    y = torch.as_tensor(data.PIRI.to_numpy()).double()\n",
    "\n",
    "    unit_means = torch.zeros(n,)\n",
    "    for i in range(n):\n",
    "        unit_means[i] = y[x[:,1]==i].mean()\n",
    "\n",
    "    return x.double(), y.double(), unit_means.double(), data, countries, years\n",
    "\n",
    "train_x, train_y, unit_means, data, countries, years = load_PIRI_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build GPR model with unit trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model specification: PIRI gp model with unit trends\n",
    "# PIRI ~ AIShame + u_i(t) + cat_rat + ccpr_rat \n",
    "#            + democratic + log(gdppc) + log(pop) \n",
    "#            + Civilwar2 + War\n",
    "# u_i(t) ~ GP(b_i, K_t)\n",
    "\n",
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, ard_num_dim=None):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = MaskMean(active_dims=1, \\\n",
    "                base_mean=ConstantVectorMean(d=train_x[:,1].unique().size()[0]))\n",
    "        # year kernel * country kernel\n",
    "        self.unit_covar_module = ScaleKernel(RBFKernel(active_dims=0)*RBFKernel(active_dims=1))\n",
    "        self.x_covar_module = torch.nn.ModuleList([ScaleKernel(RBFKernel(\\\n",
    "            active_dims=(i))) for i in [6,7]])\n",
    "        self.binary_covar_module = torch.nn.ModuleList([ScaleKernel(RBFKernel(\\\n",
    "            active_dims=(i))) for i in [3,4,5,8,9]])\n",
    "        self.effect_covar_module = ScaleKernel(RBFKernel(active_dims=2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        unit_covar_x = self.unit_covar_module(x)\n",
    "        effect_covar_x = self.effect_covar_module(x)\n",
    "        covar_x = unit_covar_x + effect_covar_x\n",
    "        for i, _ in enumerate(self.x_covar_module):\n",
    "            covar_x += self.x_covar_module[i](x)\n",
    "        for i, _ in enumerate(self.binary_covar_module):\n",
    "            covar_x += self.binary_covar_module[i](x)\n",
    "        \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPModel(\n",
       "  (likelihood): GaussianLikelihood(\n",
       "    (noise_covar): HomoskedasticNoise(\n",
       "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "    )\n",
       "  )\n",
       "  (mean_module): MaskMean(\n",
       "    (base_mean): ConstantVectorMean()\n",
       "  )\n",
       "  (unit_covar_module): ScaleKernel(\n",
       "    (base_kernel): ProductKernel(\n",
       "      (kernels): ModuleList(\n",
       "        (0): RBFKernel(\n",
       "          (raw_lengthscale_constraint): Positive()\n",
       "        )\n",
       "        (1): RBFKernel(\n",
       "          (raw_lengthscale_constraint): Positive()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (raw_outputscale_constraint): Positive()\n",
       "  )\n",
       "  (x_covar_module): ModuleList(\n",
       "    (0): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (1): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "  )\n",
       "  (binary_covar_module): ModuleList(\n",
       "    (0): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (1): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (2): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (3): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (4): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "  )\n",
       "  (effect_covar_module): ScaleKernel(\n",
       "    (base_kernel): RBFKernel(\n",
       "      (raw_lengthscale_constraint): Positive()\n",
       "    )\n",
       "    (raw_outputscale_constraint): Positive()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood = GaussianLikelihood()\n",
    "model = GPModel(train_x, train_y, likelihood).double()\n",
    "\n",
    "# initialize model parameters\n",
    "hypers = {\n",
    "    'mean_module.base_mean.constantvector': unit_means,\n",
    "    'likelihood.noise_covar.noise': torch.tensor(0.25),\n",
    "    'unit_covar_module.base_kernel.kernels.0.lengthscale': torch.tensor(4),\n",
    "    'unit_covar_module.base_kernel.kernels.1.lengthscale': torch.tensor(0.01),\n",
    "    'unit_covar_module.outputscale': torch.tensor(0.25),\n",
    "    'x_covar_module.0.outputscale': torch.tensor(0.25),\n",
    "    'x_covar_module.1.outputscale': torch.tensor(0.25),\n",
    "    'binary_covar_module.0.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.1.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.2.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.3.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.4.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'effect_covar_module.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'effect_covar_module.outputscale': torch.tensor(0.25)\n",
    "}    \n",
    "\n",
    "model.initialize(**hypers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model by optimizing hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yahoo/anaconda3/lib/python3.7/site-packages/gpytorch/functions/_pivoted_cholesky.py:118: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2189.)\n",
      "  [L, torch.triangular_solve(Krows[..., m:, :].transpose(-1, -2), L, upper=False)[0].transpose(-1, -2)],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 2.310 \n",
      "Iter 2/50 - Loss: 2.189 \n",
      "Iter 3/50 - Loss: 2.085 \n",
      "Iter 4/50 - Loss: 1.997 \n",
      "Iter 5/50 - Loss: 1.907 \n",
      "Iter 6/50 - Loss: 1.842 \n",
      "Iter 7/50 - Loss: 1.791 \n",
      "Iter 8/50 - Loss: 1.747 \n",
      "Iter 9/50 - Loss: 1.706 \n",
      "Iter 10/50 - Loss: 1.675 \n",
      "Iter 11/50 - Loss: 1.655 \n",
      "Iter 12/50 - Loss: 1.638 \n",
      "Iter 13/50 - Loss: 1.622 \n",
      "Iter 14/50 - Loss: 1.602 \n",
      "Iter 15/50 - Loss: 1.592 \n",
      "Iter 16/50 - Loss: 1.584 \n",
      "Iter 17/50 - Loss: 1.582 \n",
      "Iter 18/50 - Loss: 1.574 \n",
      "Iter 19/50 - Loss: 1.574 \n",
      "Iter 20/50 - Loss: 1.574 \n",
      "Iter 21/50 - Loss: 1.573 \n",
      "Iter 22/50 - Loss: 1.570 \n",
      "Iter 23/50 - Loss: 1.565 \n",
      "Iter 24/50 - Loss: 1.571 \n",
      "Iter 25/50 - Loss: 1.568 \n",
      "Iter 26/50 - Loss: 1.573 \n",
      "Iter 27/50 - Loss: 1.577 \n",
      "Iter 28/50 - Loss: 1.576 \n",
      "Iter 29/50 - Loss: 1.574 \n",
      "Iter 30/50 - Loss: 1.570 \n",
      "Iter 31/50 - Loss: 1.569 \n",
      "Iter 32/50 - Loss: 1.572 \n",
      "Iter 33/50 - Loss: 1.578 \n",
      "Iter 34/50 - Loss: 1.570 \n",
      "Iter 35/50 - Loss: 1.574 \n",
      "Iter 36/50 - Loss: 1.573 \n",
      "Iter 37/50 - Loss: 1.570 \n",
      "Iter 38/50 - Loss: 1.574 \n",
      "Iter 39/50 - Loss: 1.571 \n",
      "Iter 40/50 - Loss: 1.577 \n",
      "Iter 41/50 - Loss: 1.571 \n",
      "Iter 42/50 - Loss: 1.568 \n",
      "Iter 43/50 - Loss: 1.576 \n",
      "Iter 44/50 - Loss: 1.569 \n",
      "Iter 45/50 - Loss: 1.570 \n",
      "Iter 46/50 - Loss: 1.570 \n",
      "Iter 47/50 - Loss: 1.572 \n",
      "Iter 48/50 - Loss: 1.570 \n",
      "Iter 49/50 - Loss: 1.560 \n",
      "Iter 50/50 - Loss: 1.567 \n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "# freeze length scale in the country component in unit covar\n",
    "# freeze constant unit means\n",
    "all_params = set(model.parameters())\n",
    "final_params = list(all_params - \\\n",
    "            {model.unit_covar_module.base_kernel.kernels[1].raw_lengthscale, \\\n",
    "            model.mean_module.base_mean.constantvector, \\\n",
    "        #   model.x_covar_module[0].raw_outputscale,\n",
    "        #   model.x_covar_module[1].raw_outputscale,\n",
    "            model.binary_covar_module[0].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[1].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[2].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[3].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[4].base_kernel.raw_lengthscale,\n",
    "            model.effect_covar_module.base_kernel.raw_lengthscale})\n",
    "        #   model.effect_covar_module.raw_outputscale})\n",
    "optimizer = torch.optim.Adam(final_params, lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 50\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f '  % (\n",
    "        i + 1, training_iter, loss.item()\n",
    "    ))\n",
    "    optimizer.step()\n",
    "\n",
    "torch.save(model.state_dict(), \"PIRI_GPR_model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate posterior of PIRI effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effect: 0.350 +- 0.157\n",
      "\n",
      "model evidence: -3348.776 \n",
      "\n",
      "BIC: 6797.226 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('PIRI_GPR_model.pth'))\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out = likelihood(model(train_x))\n",
    "    mu_f = out.mean\n",
    "    V = out.covariance_matrix\n",
    "    L = torch.linalg.cholesky(V, upper=False)\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    model.unit_covar_module.outputscale = 0\n",
    "    for i,_ in enumerate(model.x_covar_module):\n",
    "        model.x_covar_module[i].outputscale = 0\n",
    "    for i,_ in enumerate(model.binary_covar_module):\n",
    "        model.binary_covar_module[i].outputscale = 0\n",
    "    effect_covar = model(train_x).covariance_matrix\n",
    "\n",
    "# get posterior effect mean\n",
    "alpha = torch.linalg.solve(L.t(),torch.linalg.solve(L,train_y-mu_f))\n",
    "tmp = torch.linalg.solve(L, effect_covar)\n",
    "post_effect_mean = effect_covar @ alpha\n",
    "# get posterior effect covariance\n",
    "post_effect_covar = effect_covar - tmp.t() @ tmp\n",
    "\n",
    "effect = post_effect_mean[train_x[:,2]==1].mean() - post_effect_mean[train_x[:,2]==0].mean()\n",
    "effect_std = post_effect_covar.diag().mean().sqrt()\n",
    "BIC = (2+4+6+1)*torch.log(torch.tensor(train_x.size()[0])) + 2*loss*train_x.size()[0]\n",
    "print(\"effect: {:0.3f} +- {:0.3f}\\n\".format(effect, effect_std))\n",
    "print(\"model evidence: {:0.3f} \\n\".format(-loss*train_x.size()[0]))\n",
    "print(\"BIC: {:0.3f} \\n\".format(BIC))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Durbin Watson tests for autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 138 residuals are positively correlated.\n",
      "\n",
      "2 out of 138 residuals are negatively correlated.\n",
      "\n",
      "136 out of 138 residuals are not correlated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get unit trend wo AIShame\n",
    "model.load_state_dict(torch.load('PIRI_GPR_model.pth'))\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    model.effect_covar_module.outputscale = 0\n",
    "    unit_covar = likelihood(model(train_x)).covariance_matrix\n",
    "\n",
    "# get posterior unit trend mean\n",
    "alpha = torch.linalg.solve(L.t(),torch.linalg.solve(L,train_y-mu_f))\n",
    "tmp = torch.linalg.solve(L, unit_covar)\n",
    "post_unit_mean = mu_f + unit_covar @ alpha + post_effect_mean\n",
    "\n",
    "# DW-test for sample size = 18 and 9 regressors.\n",
    "dL = 0.32\n",
    "dU = 2.87\n",
    "n = len(countries)\n",
    "DW_results = np.zeros((n,))\n",
    "for i in range(n):\n",
    "    mask = data.country==countries[i]\n",
    "    mask = mask.to_list()\n",
    "    res = train_y[mask] - post_unit_mean[mask]\n",
    "    DW_results[i] = durbin_watson(res.detach().numpy())\n",
    "\n",
    "print(\"{} out of {} residuals are positively correlated.\\n\".format(np.sum(DW_results<=dL),n))\n",
    "print(\"{} out of {} residuals are negatively correlated.\\n\".format(np.sum(DW_results>=dU),n))\n",
    "print(\"{} out of {} residuals are not correlated.\\n\".format(np.sum((DW_results>dL) & (DW_results<dU)),n))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot fitted mean trend and CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yahoo/anaconda3/lib/python3.7/site-packages/gpytorch/models/exact_gp.py:275: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  GPInputWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gpr_mean  true_y   gpr_lwr   gpr_upr  year country\n",
      "0  0.248446     0.0 -2.349084  2.845976  1983     USA\n",
      "1  0.314196     0.0 -2.268266  2.896659  1984     USA\n",
      "2  0.742070     1.0 -1.825849  3.309989  1985     USA\n",
      "3  0.458850     1.0 -2.097085  3.014784  1986     USA\n",
      "4  0.837209     0.0 -1.711116  3.385533  1987     USA\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('PIRI_GPR_model.pth'))\n",
    "\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out = likelihood(model(train_x))\n",
    "    mu_f = out.mean.numpy()\n",
    "    lower, upper = out.confidence_region()\n",
    "\n",
    "results = pd.DataFrame({\"gpr_mean\":mu_f})\n",
    "results['true_y'] = train_y\n",
    "results['gpr_lwr'] = lower\n",
    "results['gpr_upr'] = upper\n",
    "results['year'] = years[train_x[:,0].numpy().astype(int)]\n",
    "results['country'] = [countries[i] for i in train_x[:,1].numpy().astype(int)]\n",
    "print(results.head())\n",
    "results.to_csv(\"./results/PIRI_fitted_gpr.csv\",index=False) #save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpr_mean</th>\n",
       "      <th>true_y</th>\n",
       "      <th>gpr_lwr</th>\n",
       "      <th>gpr_upr</th>\n",
       "      <th>year</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>5.902513</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.297571</td>\n",
       "      <td>8.507455</td>\n",
       "      <td>1983</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>5.489772</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.891394</td>\n",
       "      <td>8.088150</td>\n",
       "      <td>1984</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>4.833130</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.242814</td>\n",
       "      <td>7.423446</td>\n",
       "      <td>1985</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>4.847446</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.264023</td>\n",
       "      <td>7.430869</td>\n",
       "      <td>1986</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>5.537215</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.961719</td>\n",
       "      <td>8.112711</td>\n",
       "      <td>1987</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>5.611730</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.043957</td>\n",
       "      <td>8.179503</td>\n",
       "      <td>1988</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>6.024374</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.466069</td>\n",
       "      <td>8.582679</td>\n",
       "      <td>1989</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>5.882499</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.329415</td>\n",
       "      <td>8.435583</td>\n",
       "      <td>1990</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>5.672275</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.119013</td>\n",
       "      <td>8.225537</td>\n",
       "      <td>1991</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>5.645671</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.086241</td>\n",
       "      <td>8.205101</td>\n",
       "      <td>1992</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>5.518569</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.952352</td>\n",
       "      <td>8.084786</td>\n",
       "      <td>1993</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>6.428920</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.865259</td>\n",
       "      <td>8.992582</td>\n",
       "      <td>1994</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>6.642138</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.074729</td>\n",
       "      <td>9.209547</td>\n",
       "      <td>1995</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>6.435204</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.863814</td>\n",
       "      <td>9.006594</td>\n",
       "      <td>1996</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>6.074017</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.498391</td>\n",
       "      <td>8.649642</td>\n",
       "      <td>1997</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>5.787875</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.204069</td>\n",
       "      <td>8.371680</td>\n",
       "      <td>1998</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>5.646662</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.052518</td>\n",
       "      <td>8.240805</td>\n",
       "      <td>1999</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>5.683616</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.081009</td>\n",
       "      <td>8.286223</td>\n",
       "      <td>2000</td>\n",
       "      <td>CHINA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gpr_mean  true_y   gpr_lwr   gpr_upr  year country\n",
       "1799  5.902513     6.0  3.297571  8.507455  1983   CHINA\n",
       "1800  5.489772     6.0  2.891394  8.088150  1984   CHINA\n",
       "1801  4.833130     4.0  2.242814  7.423446  1985   CHINA\n",
       "1802  4.847446     5.0  2.264023  7.430869  1986   CHINA\n",
       "1803  5.537215     4.0  2.961719  8.112711  1987   CHINA\n",
       "1804  5.611730     6.0  3.043957  8.179503  1988   CHINA\n",
       "1805  6.024374     8.0  3.466069  8.582679  1989   CHINA\n",
       "1806  5.882499     6.0  3.329415  8.435583  1990   CHINA\n",
       "1807  5.672275     4.0  3.119013  8.225537  1991   CHINA\n",
       "1808  5.645671     5.0  3.086241  8.205101  1992   CHINA\n",
       "1809  5.518569     5.0  2.952352  8.084786  1993   CHINA\n",
       "1810  6.428920     8.0  3.865259  8.992582  1994   CHINA\n",
       "1811  6.642138     8.0  4.074729  9.209547  1995   CHINA\n",
       "1812  6.435204     5.0  3.863814  9.006594  1996   CHINA\n",
       "1813  6.074017     6.0  3.498391  8.649642  1997   CHINA\n",
       "1814  5.787875     6.0  3.204069  8.371680  1998   CHINA\n",
       "1815  5.646662     6.0  3.052518  8.240805  1999   CHINA\n",
       "1816  5.683616     5.0  3.081009  8.286223  2000   CHINA"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results.country==\"CHINA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
