{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration of Gaussian Process Regression (GPR) with multiple-units time series data\n",
    "\n",
    "The dataset we are going to use in this demo is the physical integrity rights index (PIRI) data, which comes from Strezhnev, Anton, Judith G Kelley and Beth A Simmons. 2021. “Testing for Negative Spillovers: Is Promoting Human Rights Really Part of the “Problem”?” International\n",
    "Organization 75(1):71–102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "Gaussian Process Regression (GPR) is a Bayesian non-parameteric machine learning method for learning complex functional relation with uncertainty estimation. We will demonstrate in this tutorial how to apply GPR for Political Science research, and show how to estimate the marginal effects of the treatment *AIshame* and other covariates on *PIRI* under GPR framework. Readers who are not familiar with GPR are encouraged to read Chapter 2 of [Gaussian Processes for Machine Learning](https://gaussianprocess.org/gpml/chapters/RW2.pdf) for a complete introduction. In short, Gaussian process assumes the observations $\\{y_i\\}_{1}^n$ at input locations $\\{x_i\\}_{1}^n$ have a joint Gaussian distribution:\n",
    "$$\n",
    "[y_1,\\dots,y_n]^T \\sim \\mathcal{N}(\\mu,K)\n",
    "$$\n",
    "where the covariance matrix is determined by the kernel $K(x,x')$ so $K_{ij}=K(x_i,x_j)$. Notationwise, GPR can also be written as $f \\sim \\mathcal{GP}(\\mu,K)$. Throughout this demonstration, we will use the RBF kernel (also known as the squared exponential kernel), specified by $K(x,x')=\\rho^2 \\exp(-\\frac{(x-x')^2}{2\\ell^2})$. Here $\\rho^2$ is the output scale that controls the variance of deviation from the mean trend, and $\\ell$ is the length scale that determines how fast two distanced input locations become less correlated. Both of them belong to the set of hyperparameters in GPR, and can either be handcoded based on prior knowledge or optimized based on [maximum-likelihood-ii](https://en.wikipedia.org/wiki/Empirical_Bayes_method).\n",
    "\n",
    "There are many implemenations of GPR in R(GauPro, gausspr), python(gpytorch, scikit-learn) and matlab (gpml). This tutorial is done in the gpytorch, a highly efficient and modular implementation of GPs with GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gpytoch and other packages\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gpytorch\n",
    "from scipy.stats import norm\n",
    "from typing import Optional, Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "from gpytorch.means import LinearMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from statsmodels.stats.stattools import durbin_watson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and setup data\n",
    "\n",
    "In this cell, we set up the PIRI data for this example. The physical integrity rights index (PIRI) is a composite index of repression indices obtained by adding together the scores for the four physical integrity measures: killing, torture, imprisonment, and disappearances. This yields a variable that ranges from 0 (no violations on any of the four measures) to 8 (worst scores on all four measures). The treatment variable is *AIShame*, which indicates whether Amnesty International, an international human rights organization, publicly shames a country about their physical integrity rights. Other covariates include *CAT* and *CCPR* (Convention Against Torture and International Covenant on Civil and Political Rights), measures of economic development (pop and gdp) and national security (war and civil war). Following Strezhnev et al. 2021, we use *PIRILead1* as our DV (dependent variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_PIRI_data():\n",
    "    # read data\n",
    "    # url = \"https://raw.githubusercontent.com/yahoochen97/GP_gradient/main/hb_data_complete.csv\"\n",
    "    # data = pd.read_csv(url, index_col=[0])\n",
    "    data = pd.read_csv(\"./hb_data_complete.csv\" ,index_col=[0])\n",
    "\n",
    "    # all zero PIRI for new zealand and netherland\n",
    "    data = data.loc[~data['country'].isin(['N-ZEAL','NETHERL'])]\n",
    "\n",
    "    countries = sorted(data.country.unique())\n",
    "    years = data.year.unique()\n",
    "    n = len(countries)\n",
    "    m = len(years)\n",
    "\n",
    "    # build data\n",
    "    country_dict = dict(zip(countries, range(n)))\n",
    "    year_dict = dict(zip(years, range(m)))\n",
    "\n",
    "    # x is:\n",
    "    # 1: year number\n",
    "    # 2: country id\n",
    "    # 3: AIShame (treatment indicator)\n",
    "    # 4: cat_rat\n",
    "    # 5: ccpr_rat\n",
    "    # 6: democratic\n",
    "    # 7: log(gdppc)\n",
    "    # 8: log(pop)\n",
    "    # 9: Civilwar2\n",
    "    # 10: War\n",
    "    x = torch.zeros(data.shape[0], 10)\n",
    "    x[:,0] = torch.as_tensor(list(map(year_dict.get, data.year)))\n",
    "    x[:,1] = torch.as_tensor(list(map(country_dict.get, data.country)))\n",
    "    x[:,2] = torch.as_tensor(data.AIShame.to_numpy())\n",
    "    x[:,3] = torch.as_tensor(data.cat_rat.to_numpy())\n",
    "    x[:,4] = torch.as_tensor(data.ccpr_rat.to_numpy())\n",
    "    x[:,5] = torch.as_tensor(data.democratic.to_numpy())\n",
    "    x[:,6] = torch.as_tensor(data.log_gdppc.to_numpy())\n",
    "    x[:,7] = torch.as_tensor(data.log_pop.to_numpy())\n",
    "    x[:,8] = torch.as_tensor(data.Civilwar2.to_numpy())\n",
    "    x[:,9] = torch.as_tensor(data.War.to_numpy())\n",
    "    # x[:,10] = torch.as_tensor(data.PIRI.to_numpy())\n",
    "    y = torch.as_tensor(data.PIRILead1.to_numpy()).double()\n",
    "\n",
    "    unit_means = torch.zeros(n,)\n",
    "    for i in range(n):\n",
    "        unit_means[i] = y[x[:,1]==i].mean()\n",
    "\n",
    "    return x.double(), y.double(), unit_means.double(), data, countries, years\n",
    "\n",
    "train_x, train_y, unit_means, data, countries, years = load_PIRI_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to interpret fixed effects model as GPR\n",
    "\n",
    "Let's see how the baseline fixed effects model works. The fixed effects model regresses *PIRILead1* with the treatment and covariate variables dicussed above with year- and country- level fixed effects:\n",
    "$$\n",
    "\\text{PIRILead1} \\sim \\text{AIShame}  + \\text{democrat} + ... +  \\text{C(year)} + \\text{C(country)} + \\varepsilon\n",
    "$$\n",
    "Mathematically it is equivalent to write DV $y_{it}$ in terms of IV $x_{it}$, slope $\\beta$, time effects $\\alpha_t$ and country effects $b_i$\n",
    "$$\n",
    "y_{it} \\sim \\beta^T x_{it} + \\alpha_t + b_i + \\varepsilon_{it}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      tfe_mean  tfe_lwr  tfe_upr  true_y  year country\n",
      "2132     1.104    0.489    1.719     1.0  1996    FIJI\n",
      "2133     1.216    0.598    1.833     1.0  1997    FIJI\n",
      "2134     1.762    1.142    2.382     1.0  1998    FIJI\n",
      "2135     0.729    0.095    1.364     1.0  1999    FIJI\n",
      "2136     2.088    1.466    2.710     2.0  2000    FIJI\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "\n",
    "# lm = sm.ols('PIRILead1 ~ AIShame  + cat_rat + ccpr_rat \\\n",
    "#             + democratic + log_gdppc + log_pop \\\n",
    "#             + Civilwar2 + War + C(year) + C(country) + PIRI', data).fit()\n",
    "lm = sm.ols('PIRI ~ AIShame  + cat_rat + ccpr_rat \\\n",
    "            + democratic + log_gdppc + log_pop \\\n",
    "            + Civilwar2 + War + C(year) + C(country)', data).fit()\n",
    "\n",
    "# store results\n",
    "predictions = lm.get_prediction(data)\n",
    "prediction_summary = predictions.summary_frame(alpha=0.05)\n",
    "lower = np.round(prediction_summary['mean_ci_lower'],3)\n",
    "upper = np.round(prediction_summary['mean_ci_upper'],3)\n",
    "results = pd.DataFrame({\"tfe_mean\": np.round(prediction_summary[\"mean\"], 3), \n",
    "                        \"tfe_lwr\": lower,\n",
    "                        \"tfe_upr\": upper})\n",
    "results['true_y'] = np.round(train_y, 3)\n",
    "results['year'] = years[train_x[:,0].numpy().astype(int)]\n",
    "results['country'] = [countries[i] for i in train_x[:,1].numpy().astype(int)]\n",
    "print(results.tail())\n",
    "results.to_csv(\"./results/PIRI_fitted_2fe.csv\",index=False) #save to file\n",
    "\n",
    "coefs = lm.params.to_dict()\n",
    "covariate_names = [\"AIShame\" ,\"cat_rat\" , \"ccpr_rat\",\n",
    "           \"democratic\",  \"log_gdppc\", \"log_pop\",\n",
    "            \"Civilwar2\", \"War\"]\n",
    "x_weights = list(map(coefs.get, covariate_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bayesian alternative of this fixed effect model makes two additional prior assumptions. First, a Gaussian prior is put on the slope $\\beta\\sim\\mathcal{N}(0,\\Sigma_{\\beta})$. Second, i.i.d. Gaussian priors are also placed on the fixed effects $\\alpha_t\\sim\\mathcal{N}(0,\\sigma^2_{t})$ and $b_i\\sim\\mathcal{N}(0,\\sigma^2_{i})$ so as to be random effects. In this new Bayesian hierarchical model, we can now compute the corresponding mean and covariance between any two input triples $(x_{it},i,t)$ and $(x_{i't'},i',t')$\n",
    "$$\n",
    "\\begin{gathered}\n",
    "E[y_{it}] = E[\\beta^T] x_{it} + E[\\alpha_{t}] + E[b_{i}] + E[\\varepsilon_{it}] = 0\\\\\n",
    "\\text{cov}[y_{it},y_{i't'}] = x_{it} \\text{cov}[\\beta^T,\\beta^T] x_{i't'} + \\text{cov}[\\alpha_{t},\\alpha_{t'}] + \\text{cov}[b_{i},b_{i'}] = x_{it} \\Sigma_{\\beta} x_{i't'} + \\mathbb{I}[t=t']\\sigma^2_{t} + \\mathbb{I}[i=i']\\sigma^2_{i}\n",
    "\\end{gathered}\n",
    "$$\n",
    "where the indicator function $\\mathbb{I}[i=i']$ is 1 iff the country indices are the same $i=i'$. Hence, we can see that the linear component $\\beta^T x_{it}$ with normal prior on $\\beta$ is equivalent to GPR with a so-called linear kernel, and the random effect is equivalent to GPR with an indicator kernel. This kernel trick also works with basis expansion $\\phi(x_{it})$ such as quadratic terms or interactions, so by designs of kernels GPR can automatically perform feature selection instead of manual transformation. As is well known, the RBF kernel we are going to use corresponds to basis transformation to an infinite dimensional space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customization of mean and kernel\n",
    "\n",
    "One good thing about gpytorch is its flexibility in customization of mean functions and covariance functions, although someone may argue such flexibility is bad for new users who are not used to coding. Therefore, we simply provide two customized mean functions: a constant mean $E[f(x,i,t)]=c_i$ that simply returns a predefined value dependent on index $i$ and a mask mean $E[f(x,i,t)]=E[f(x_{\\text{mask}},i,t)]$ that ignores some dependent variables. The implementation is done by inheriting derived classes from base classes and customizing the functional relation in the `forward()` function, which should be familiar to pytorch users. Customization of kernels is slightly more complicated so we skip them in this demo, but advanced readers are welcome to check source codes from [gpytorch doc](https://docs.gpytorch.ai/en/stable/_modules/gpytorch/kernels/polynomial_kernel.html#PolynomialKernel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantVectorMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(self, d=1, prior=None, batch_shape=torch.Size(), **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_shape = batch_shape\n",
    "        self.register_parameter(name=\"constantvector\",\\\n",
    "                 parameter=torch.nn.Parameter(torch.zeros(*batch_shape, d)))\n",
    "        if prior is not None:\n",
    "            self.register_prior(\"mean_prior\", prior, \"constantvector\")\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.constantvector[input.int().reshape((-1,)).tolist()]\n",
    "    \n",
    "class MaskMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_mean: gpytorch.means.mean.Mean,\n",
    "        active_dims: Optional[Tuple[int, ...]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if active_dims is not None and not torch.is_tensor(active_dims):\n",
    "            active_dims = torch.tensor(active_dims, dtype=torch.long)\n",
    "        self.active_dims = active_dims\n",
    "        self.base_mean = base_mean\n",
    "    \n",
    "    def forward(self, x, **params):\n",
    "        return self.base_mean.forward(x.index_select(-1, self.active_dims), **params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build GPR model for multi-unit time-series data\n",
    "\n",
    "GPR model is in general good for multi-unit time series data for two reasons. First, the use of kernel functions implicitly perform feature transformation and selection so social scientists could no longer worry about missing interaction or non-linearity. Secondly and maybe more importantly, many time series models other than simple fixed effects can be easily integrated into GPR framework. For example, the commonly used autoregression (AR) model is shown to be a Gaussian process with [a composite of kernels](http://herbsusmann.com/2019/08/09/autoregressive-processes-are-gaussian-processes/). For the purpose of illustration, we adopt the following specification of a country-level time trend plus covariate process for the PIRI dataset:\n",
    "$$\n",
    "\\begin{gathered}\n",
    "y_i(t) \\sim u_i(t) + f(x_{it}) + \\varepsilon \\\\\n",
    "f(x_{it}) \\sim GP(0, K_x) \\\\\n",
    "u_i(t) \\sim GP(b_i, K_t)\n",
    "\\end{gathered}\n",
    "$$\n",
    "where the time trends $u_i(t)$s are GPs with mean $b_i$s acting as country-level fixed effects and time kernel $K_t$ acting as time-level random effects but with temporal correlation, and the covariate process is zero-meaned and has kernel $K_x$.\n",
    "\n",
    "The most critical part of GPR in gpytorch codewise is the model class, where both mean and covariance functions are defined. Shortly speaking, users need to define different mean and covariance modules based on their desired model in the `__init__()` function, and sum up those modules for both mean and covariance functions in the `forward()` function. Again, gpytorch website has a detailed walkthrough of [GPR regression](https://docs.gpytorch.ai/en/stable/examples/01_Exact_GPs/Simple_GP_Regression.html) for complete reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model specification: PIRI gp model with unit trends\n",
    "# x_it : AIShame + cat_rat + ccpr_rat \n",
    "#            + democratic + log(gdppc) + log(pop) \n",
    "#            + Civilwar2 + War \n",
    "# y_i(t) ~ u_i(t) + f(x_{it}) + ε\n",
    "# f(x_{it}) ~ GP(0, K_x)\n",
    "# u_i(t) ~ GP(b_i, K_t)\n",
    "\n",
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        # constant country-level mean\n",
    "        self.mean_module = MaskMean(active_dims=1, \\\n",
    "               base_mean=ConstantVectorMean(d=train_x[:,1].unique().size()[0]))\n",
    "        # linear mean for covariates\n",
    "        self.x_mean_module = MaskMean(active_dims=[2,3,4,5,6,7,8,9], base_mean=LinearMean(input_size=8, bias=False))\n",
    "        # year kernel * country kernel\n",
    "        self.unit_covar_module = ScaleKernel(RBFKernel(active_dims=0)*RBFKernel(active_dims=1))\n",
    "        self.x_covar_module = torch.nn.ModuleList([ScaleKernel(RBFKernel(\\\n",
    "            active_dims=(i))) for i in [6,7]])\n",
    "        # cov for categorical covariates\n",
    "        self.binary_covar_module = torch.nn.ModuleList([ScaleKernel(RBFKernel(\\\n",
    "            active_dims=(i))) for i in [3,4,5,8,9]])\n",
    "        self.effect_covar_module = ScaleKernel(RBFKernel(active_dims=2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x) + self.x_mean_module(x)\n",
    "        unit_covar_x = self.unit_covar_module(x)\n",
    "        effect_covar_x = self.effect_covar_module(x)\n",
    "        covar_x = unit_covar_x + effect_covar_x\n",
    "        for i, _ in enumerate(self.x_covar_module):\n",
    "            covar_x += self.x_covar_module[i](x)\n",
    "        for i, _ in enumerate(self.binary_covar_module):\n",
    "            covar_x += self.binary_covar_module[i](x)\n",
    "        \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning and training of GPR model\n",
    "\n",
    "As mentioned previously, GPR is usually associated with a set of hyperparameters either in the mean function (country-level intercept) or the kernel (length scale, output scale). These hypparameters could either be handcoded based on prior belief or be tuned by empirical bayes methods such as maximizing type-ii likelihood. Here we should how to perform MLE-II for GPR. We start by defining and initializing the GP model and likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = GaussianLikelihood()\n",
    "model = GPModel(train_x, train_y, likelihood).double()\n",
    "\n",
    "# initialize model parameters\n",
    "hypers = {\n",
    "    'mean_module.base_mean.constantvector': unit_means,\n",
    "    'x_mean_module.base_mean.weights': torch.tensor(x_weights),\n",
    "    'likelihood.noise_covar.noise': torch.tensor(0.25),\n",
    "    'unit_covar_module.base_kernel.kernels.0.lengthscale': torch.tensor(6),\n",
    "    'unit_covar_module.base_kernel.kernels.1.lengthscale': torch.tensor(0.01),\n",
    "    'unit_covar_module.outputscale': torch.tensor(4),\n",
    "    'x_covar_module.0.outputscale': torch.tensor(1),\n",
    "    'x_covar_module.1.outputscale': torch.tensor(1),\n",
    "    'binary_covar_module.0.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.1.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.2.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.3.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.4.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.0.outputscale': torch.tensor(1),\n",
    "    'binary_covar_module.1.outputscale': torch.tensor(1),\n",
    "    'binary_covar_module.2.outputscale': torch.tensor(1),\n",
    "    'binary_covar_module.3.outputscale': torch.tensor(1),\n",
    "    'binary_covar_module.4.outputscale': torch.tensor(1),\n",
    "    'effect_covar_module.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'effect_covar_module.outputscale': torch.tensor(1)\n",
    "}    \n",
    "\n",
    "model = model.initialize(**hypers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to other machine learning algorithms, training of GPR means to find the optimal set of hyperparameters (or parameters in other ML algorithms) for some objective. For GPR with $f\\sim\\mathcal{GP}(0,K)$, the objective is to maximize the type-ii log likelihood\n",
    "$$\n",
    "\\log(y\\mid x) = -\\frac{1}{2}y^T\\big(K(x,x)+\\sigma^2_{\\text{noise}}\\mathbf{I}\\big)^{-1}y-\\frac{1}{2}\\log\\Big(\\det\\big(K(x,x)+\\sigma^2_{\\text{noise}}\\big)\\Big)-\\frac{n}{2}\\log(2\\pi)\n",
    "$$\n",
    "As gpytorch is built on the top of pytorch, optimization of this objective is very convenient using off-the-shelf optimizers such as [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html). Hyperparameters can also be fixed by simpling excluding them from the optimization procedure. For example, we have used RBF kernel as a shortcut for binary indicator kernel by fixing the length scale to be some small number ($0.01$ in our case), so $\\text{cov}(0,1)=\\exp(-1/2/0.01^2)\\approx 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yahoo/anaconda3/lib/python3.7/site-packages/gpytorch/functions/_pivoted_cholesky.py:118: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2189.)\n",
      "  [L, torch.triangular_solve(Krows[..., m:, :].transpose(-1, -2), L, upper=False)[0].transpose(-1, -2)],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0/100 - Loss: 2.254 \n",
      "Iter 10/100 - Loss: 1.740 \n",
      "Iter 20/100 - Loss: 1.648 \n",
      "Iter 30/100 - Loss: 1.611 \n",
      "Iter 40/100 - Loss: 1.592 \n",
      "Iter 50/100 - Loss: 1.577 \n",
      "Iter 60/100 - Loss: 1.577 \n",
      "Iter 70/100 - Loss: 1.570 \n",
      "Iter 80/100 - Loss: 1.563 \n",
      "Iter 90/100 - Loss: 1.563 \n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "# freeze length scale in the country component in unit covar\n",
    "# freeze constant unit means\n",
    "all_params = set(model.parameters())\n",
    "final_params = list(all_params - \\\n",
    "            {model.unit_covar_module.base_kernel.kernels[1].raw_lengthscale, \\\n",
    "            model.binary_covar_module[0].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[1].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[2].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[3].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[4].base_kernel.raw_lengthscale,\n",
    "            model.effect_covar_module.base_kernel.raw_lengthscale})\n",
    "optimizer = torch.optim.Adam(final_params, lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 100\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    if i % 10 == 0:\n",
    "        print('Iter %d/%d - Loss: %.3f '  % (\n",
    "            i , training_iter, loss.item()\n",
    "        ))\n",
    "    optimizer.step()\n",
    "\n",
    "torch.save(model.state_dict(), \"PIRI_GPR_model.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior of GPR model\n",
    "\n",
    "After hyperparameter tuning, we now discuss how to derive posterior of GPR model. It is worth reminding that hyperparameter tuning is orthogonal to deriving GPR posterior, since the former only finds the set of hyperparameters that best explains the data apriori. Like any other Bayesian method, we still need to condition the GPR prior on observed data and derive its posterior to fully \"learn\" the model. Fortunately, GPR posterior looks extremely nice with Gaussian noise. Assuming $f\\sim\\mathcal{GP}(0,K)$ and we observed $\\mathcal{D}=\\{x,y\\}$, the posterior $f\\mid \\mathcal{D}$ at any test location $x^*$ is\n",
    "$$\n",
    "\\begin{gathered}\n",
    "f(x^*)\\mid \\mathcal{D}\\sim \\mathcal{GP}(\\mu_{f\\mid \\mathcal{D}},K_{f\\mid \\mathcal{D}})\\\\\n",
    "\\mu_{f\\mid \\mathcal{D}}(x^*) = K(x,x^*)\\big(K(x,x)+\\sigma^2_{\\text{noise}}\\mathbf{I}\\big)^{-1}y\\\\\n",
    "K_{f\\mid \\mathcal{D}}(x^*,x^*) = K(x^*,x^*) - K(x^*,x)\\big(K(x,x)+\\sigma^2_{\\text{noise}}\\mathbf{I}\\big)^{-1}K(x,x^*)\n",
    "\\end{gathered}\n",
    "$$\n",
    "Fortunately, gpytorch has implemented the posterior update rule for us so we don't have to manually code them up. The RMSE loss of the fitted GPR model can be computed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yahoo/anaconda3/lib/python3.7/site-packages/gpytorch/models/exact_gp.py:275: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  GPInputWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8740755519284427\n"
     ]
    }
   ],
   "source": [
    "# load the trained model\n",
    "model.load_state_dict(torch.load('PIRI_GPR_model.pth'))\n",
    "\n",
    "# set model and likelihood to evaluation mode, meaning that we are dealing with posterior now\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out = model(train_x)\n",
    "    mu_f = out.mean.numpy()\n",
    "    lower, upper = out.confidence_region()\n",
    "\n",
    "# store results\n",
    "results = pd.DataFrame({\"gpr_mean\":mu_f})\n",
    "results['true_y'] = train_y\n",
    "results['gpr_lwr'] = lower\n",
    "results['gpr_upr'] = upper\n",
    "results['year'] = years[train_x[:,0].numpy().astype(int)]\n",
    "results['country'] = [countries[i] for i in train_x[:,1].numpy().astype(int)]\n",
    "results.to_csv(\"./results/PIRI_fitted_gpr.csv\",index=False) #save to file\n",
    "\n",
    "# print RMSE\n",
    "RMSE = np.square((out.mean - train_y).detach().numpy()).mean()**0.5\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a default warning from gpytorch because gpytorch expects a test input tensor other than the training input tensor in posterior mode. However, we are exactly interested in the fitted values at training input locations by letting $x^*=x$. We are very aware that by calling `eval()` functions we are computing the posterior rather than the prior, so we do not need to call `train()` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect estimation of binary treatment\n",
    "\n",
    "Supervised machine learning algorithms are predictive models, meaning that they are interested in getting as close as possible to the ground truth for new unseen observations. On the contrary, social scientists are interested in the treatment effect on the targeted DV, assuming collected data well respresent the whole population. Using the potential outcome framework, the conditional treatment effect for a binary treatment is defined as\n",
    "$$\n",
    "\\tau(x) = \\mathbb{E}[y^{(1)}-y^{(0)}\\mid x] \\approx \\mathbb{E}[y_{it}\\mid x_{it}, D_{it}=1] - \\mathbb{E}[y_{it}\\mid x_{it}, D_{it}=0]\n",
    "$$\n",
    "In GPR, estimation of $\\tau(x)$ is straight-forward as the model allows us to do inference on unobserved counterfactuals. Suppose we already have the posterior of a general model $f(z;x)\\sim \\mathcal{GP}(\\mu,K)$ for the joint of treatment $z$ and covariate $x$ (we could append country and time indices $i,t$ to $x$), and we can get the posterior of $\\tau(x)$ with MAP and also uncertainty estimation:\n",
    "$$\n",
    "{\\tau}(x) = f(1;x) - f(0;x) \\sim \\mathcal{GP}\\Big(\\mu(1;x)-\\mu(0;x),K([1;x],[1;x])+K([0;x],[0;x])\\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: 0.066 +- 0.054\n",
      "\n",
      "model evidence: -3344.159 \n",
      "\n",
      "BIC: 6787.991 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('PIRI_GPR_model.pth'))\n",
    "\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# copy training tesnor to test tensors and set AIshame to 1 and 0\n",
    "test_x1 = train_x.clone().detach().requires_grad_(False)\n",
    "test_x1[:,2] = 1\n",
    "test_x0 = train_x.clone().detach().requires_grad_(False)\n",
    "test_x0[:,2] = 0\n",
    "\n",
    "# in eval mode the forward() function returns posterioir\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out1 = likelihood(model(test_x1))\n",
    "    out0 = likelihood(model(test_x0))\n",
    "\n",
    "# compute ATE and its uncertainty\n",
    "effect = out1.mean.numpy().mean() - out0.mean.numpy().mean()\n",
    "effect_std = np.sqrt((out1.mean.numpy()[test_x1[:,2] == 1].mean()\\\n",
    "                      +out0.mean.numpy()[test_x1[:,2] == 1].mean())) / np.sqrt(train_x[test_x1[:,2] == 1].size()[0])\n",
    "BIC = (2+4+6+1)*torch.log(torch.tensor(train_x.size()[0])) + 2*loss*train_x.size()[0]\n",
    "print(\"ATE: {:0.3f} +- {:0.3f}\\n\".format(effect, effect_std))\n",
    "print(\"model evidence: {:0.3f} \\n\".format(-loss*train_x.size()[0]))\n",
    "print(\"BIC: {:0.3f} \\n\".format(BIC))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity analysis for autocorrelation\n",
    "\n",
    "One argument we make is that GPR is powerful in building flexible, but structured, models in the presence of clustering and spatio-temporal autocorrelation. For PIRI application, the unobserved time shocks on each country are modeled by GPR with consideration of temporal autocorrelation. We now test this hypothesis by perform Durbin Watson autocorrelation tests for fitted GPR residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 138 residuals are positively correlated.\n",
      "\n",
      "36 out of 138 residuals are negatively correlated.\n",
      "\n",
      "102 out of 138 residuals are not correlated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('PIRI_GPR_model.pth'))\n",
    "\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out = likelihood(model(train_x))\n",
    "    mu_f = out.mean.numpy()\n",
    "\n",
    "# DW-test for sample size = 18 (years) and 8 regressors.\n",
    "dL = 0.407\n",
    "dU = 2.668\n",
    "n = len(countries)\n",
    "DW_results = np.zeros((n,))\n",
    "for i in range(n):\n",
    "    mask = data.country==countries[i]\n",
    "    mask = mask.to_list()\n",
    "    res = train_y[mask] - mu_f[mask] # post_unit_mean[mask]\n",
    "    DW_results[i] = durbin_watson(res.detach().numpy())\n",
    "\n",
    "print(\"{} out of {} residuals are positively correlated.\\n\".format(np.sum(DW_results<=dL),n))\n",
    "print(\"{} out of {} residuals are negatively correlated.\\n\".format(np.sum(DW_results>=dU),n))\n",
    "print(\"{} out of {} residuals are not correlated.\\n\".format(np.sum((DW_results>dL) & (DW_results<dU)),n))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the majority of countries the residuals are not correlated temporally. Hence, GPR better meets the exogeneity assumption than fixed effects by adjusting for temporal correlated unobserved confoundings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimation of marginal effects for covariates\n",
    "Another quantity of interests to social scientists is the averaged marginal effect of covariate variables, which describles on average how change in covariates reflects change in targeted DV. The causal effect of a continuous treatment can also be understood as a certain type of marginal effect, as we can no longer compute the difference in means. For linear regression model, the marginal effects are simply the slopes in front of covariate variables, or the gradients of those variables on DV. Similarly, we can compute the corresponding marginal effects in GPR framework using differentiation of GPR. Luckily, differentiation is a linear operator and by the closure property of Gaussian process under linear transformation, we have a multivariate Gaussian for the joint of $f$ and $\\nabla f$\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\mathbf{f}\\\\\n",
    "    \\nabla \\mathbf{f}\n",
    "\\end{bmatrix}\n",
    "\\sim\n",
    "\\mathcal{GP} \\begin{bmatrix}\n",
    "\\mu_{\\mathbf{f}} \\\\\n",
    "\\nabla \\mu_{\\mathbf{f}} \n",
    " \\end{bmatrix}\n",
    ", \\begin{bmatrix}\n",
    "    \\mathbf{K}_{f} &\\nabla \\mathbf{K}_{f }^T\\\\\n",
    "    \\nabla \\mathbf{K}_{f} & \\nabla^2\\mathbf{K}_{f}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and we can derive the posterior of $\\nabla f$ by conditioning on observation of $f$\n",
    "$$\n",
    "\\begin{gathered}\n",
    " \\nabla {f} \\mid \\mathcal{D}\\sim \\mathcal{GP}(\\mu_{\\nabla f\\mid \\mathcal{D}},K_{\\nabla f\\mid \\mathcal{D}})\\\\\n",
    "\\nabla\\mu_{f\\mid \\mathcal{D}}(x^*) = \\nabla K_{f\\mid \\mathcal{D}}(x,x^*)\\big(\\nabla^2 K_{f\\mid \\mathcal{D}}(x,x)+\\sigma^2_{\\text{noise}}\\mathbf{I}\\big)^{-1}y\\\\\n",
    "\\nabla K_{f\\mid \\mathcal{D}}(x^*,x^*) = \\nabla^2 K_{f\\mid \\mathcal{D}}(x^*,x^*) - \\nabla K_{f\\mid \\mathcal{D}}(x^*,x)\\big(\\nabla^2 K_{f\\mid \\mathcal{D}}(x,x)+\\sigma^2_{\\text{noise}}\\mathbf{I}\\big)^{-1} \\nabla K_{f\\mid \\mathcal{D}}(x,x^*)\n",
    "\\end{gathered}\n",
    "$$\n",
    "Unfortunately, the above posterior has not been implemented by gpytorch yet (they have been working on it but the changes may have not been pushed to latest verision). However, pytorch is known for its fast gradient computation via autogradient, which we are going to exploit. Specifically, we are going to sample from posterior of the function, use `torch.auto.grad` to compute their gradients and estimate the standard deviation using the empirical means and gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('PIRI_GPR_model.pth'))\n",
    "\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "df_std = np.zeros((train_x.size(0),train_x.size(1)))\n",
    "x_grad = np.zeros((train_x.size(0),train_x.size(1)))\n",
    "\n",
    "# number of empirically sample \n",
    "n_samples = 100\n",
    "sampled_dydtest_x = np.zeros((n_samples, train_x.size(0),train_x.size(1)))\n",
    "\n",
    "# we proceed in small batches of size 100 for speed up\n",
    "for i in range(train_x.size(0)//100):\n",
    "    with gpytorch.settings.fast_pred_var():\n",
    "        test_x = train_x[(i*100):(i*100+100)].clone().detach().requires_grad_(True)\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "        dydtest_x = torch.autograd.grad(observed_pred.mean.sum(), test_x, retain_graph=True)[0]\n",
    "        x_grad[(i*100):(i*100+100)] = dydtest_x\n",
    "        loss = mll(observed_pred, train_y[(i*100):(i*100+100)])\n",
    "\n",
    "        sampled_pred = observed_pred.rsample(torch.Size([n_samples]))\n",
    "        sampled_dydtest_x[:,(i*100):(i*100+100),:] = torch.stack([torch.autograd.grad(pred.sum(), \\\n",
    "                                    test_x, retain_graph=True)[0] for pred in sampled_pred])\n",
    "        \n",
    "# last 100 rows\n",
    "with gpytorch.settings.fast_pred_var():\n",
    "    test_x = train_x[(100*i+100):].clone().detach().requires_grad_(True)\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "    dydtest_x = torch.autograd.grad(observed_pred.mean.sum(), test_x, retain_graph=True)[0]\n",
    "    x_grad[(100*i+100):] = dydtest_x\n",
    "    loss = mll(observed_pred, train_y[(100*i+100):])\n",
    "\n",
    "    sampled_pred = observed_pred.rsample(torch.Size([n_samples]))\n",
    "    sampled_dydtest_x[:,(100*i+100):,:] = torch.stack([torch.autograd.grad(pred.sum(),\\\n",
    "                                     test_x, retain_graph=True)[0] for pred in sampled_pred])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With samples from the gradients, we can estimate the marginal effects of all regressors by adding the variance of the gradient means and the mean of the gradient variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           x  est_mean   est_std          t        pvalue\n",
      "0  log_gdppc -0.173369  0.011196 -15.484234  2.216558e-54\n",
      "1    log_pop  0.088603  0.006458  13.720367  3.833842e-43\n"
     ]
    }
   ],
   "source": [
    "est_std = np.sqrt(sampled_dydtest_x.mean(1).var(0) + \\\n",
    "                  sampled_dydtest_x.var(1).mean(0)).round(decimals=15)\n",
    "\n",
    "results = pd.DataFrame({\"x\": covariate_names[4:6], \\\n",
    "                        'est_mean': x_grad.mean(axis=0)[6:8],\n",
    "                        'est_std': est_std[6:8]})\n",
    "results[\"t\"] = results['est_mean'].values/results['est_std'].values\n",
    "results[\"pvalue\"] = norm.cdf(-np.abs(results[\"t\"].values))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_rat: 0.079 +- 0.054\n",
      "\n",
      "t: 1.450, p: 0.074\n",
      "\n",
      "ccpr_rat: 0.394 +- 0.054\n",
      "\n",
      "t: 7.334, p: 0.000\n",
      "\n",
      "democratic: -0.655 +- 0.054\n",
      "\n",
      "t: -12.099, p: 0.000\n",
      "\n",
      "Civilwar2: 0.680 +- 0.057\n",
      "\n",
      "t: 11.993, p: 0.000\n",
      "\n",
      "War: 0.541 +- 0.057\n",
      "\n",
      "t: 9.567, p: 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('PIRI_GPR_model.pth'))\n",
    "\n",
    "for i in [3,4,5,8,9]:\n",
    "    # copy training tesnor to test tensors and set AIshame to 1 and 0\n",
    "    test_x1 = train_x.clone().detach().requires_grad_(False)\n",
    "    test_x1[:,i] = 1\n",
    "    test_x0 = train_x.clone().detach().requires_grad_(False)\n",
    "    test_x0[:,i] = 0\n",
    "\n",
    "    # in eval mode the forward() function returns posterioir\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        out1 = likelihood(model(test_x1))\n",
    "        out0 = likelihood(model(test_x0))\n",
    "\n",
    "    # compute ATE and its uncertainty\n",
    "    effect = out1.mean.numpy().mean() - out0.mean.numpy().mean()\n",
    "    effect_std = np.sqrt((out1.mean.numpy().mean()+out0.mean.numpy().mean())) / np.sqrt(train_x.size()[0])\n",
    "    print(\"{}: {:0.3f} +- {:0.3f}\\n\".format(covariate_names[i-2],effect, effect_std))\n",
    "    print(\"t: {:0.3f}, p: {:0.3f}\\n\".format(effect/effect_std, norm.cdf(-np.abs(effect/effect_std))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.-1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
