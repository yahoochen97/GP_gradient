{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of GPR for PIRI data with unit trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import torch\n",
    "import pandas as pd\n",
    "import gpytorch\n",
    "from typing import Optional, Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "from gpytorch.means import LinearMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement constant mean module and mask mean module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantVectorMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(self, d=1, prior=None, batch_shape=torch.Size(), **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_shape = batch_shape\n",
    "        self.register_parameter(name=\"constantvector\",\\\n",
    "                 parameter=torch.nn.Parameter(torch.zeros(*batch_shape, d)))\n",
    "        if prior is not None:\n",
    "            self.register_prior(\"mean_prior\", prior, \"constantvector\")\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.constantvector[input.int().reshape((-1,)).tolist()]\n",
    "    \n",
    "class MaskMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_mean: gpytorch.means.mean.Mean,\n",
    "        active_dims: Optional[Tuple[int, ...]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if active_dims is not None and not torch.is_tensor(active_dims):\n",
    "            active_dims = torch.tensor(active_dims, dtype=torch.long)\n",
    "        self.active_dims = active_dims\n",
    "        self.base_mean = base_mean\n",
    "    \n",
    "    def forward(self, x, **params):\n",
    "        return self.base_mean.forward(x.index_select(-1, self.active_dims), **params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_PIRI_data():\n",
    "    # read data\n",
    "    data = pd.read_csv(\"hb_data_complete.csv\", index_col=[0])\n",
    "\n",
    "    # all zero PIRI for new zealand and netherland\n",
    "    data = data.loc[~data['country'].isin(['N-ZEAL','NETHERL'])]\n",
    "\n",
    "    countries = sorted(data.country.unique())\n",
    "    years = data.year.unique()\n",
    "    n = len(countries)\n",
    "    m = len(years)\n",
    "\n",
    "    # build data\n",
    "    country_dict = dict(zip(countries, range(n)))\n",
    "    year_dict = dict(zip(years, range(m)))\n",
    "\n",
    "    # x is:\n",
    "    # 1: year number\n",
    "    # 2: country id\n",
    "    # 3: AIShame (treatment indicator)\n",
    "    # 4: cat_rat\n",
    "    # 5: ccpr_rat\n",
    "    # 6: democratic\n",
    "    # 7: log(gdppc)\n",
    "    # 8: log(pop)\n",
    "    # 9: Civilwar2\n",
    "    # 10: War\n",
    "    x = torch.zeros(data.shape[0], 10)\n",
    "    x[:,0] = torch.as_tensor(list(map(year_dict.get, data.year)))\n",
    "    x[:,1] = torch.as_tensor(list(map(country_dict.get, data.country)))\n",
    "    x[:,2] = torch.as_tensor(data.AIShame.to_numpy())\n",
    "    x[:,3] = torch.as_tensor(data.cat_rat.to_numpy())\n",
    "    x[:,4] = torch.as_tensor(data.ccpr_rat.to_numpy())\n",
    "    x[:,5] = torch.as_tensor(data.democratic.to_numpy())\n",
    "    x[:,6] = torch.as_tensor(data.log_gdppc.to_numpy())\n",
    "    x[:,7] = torch.as_tensor(data.log_pop.to_numpy())\n",
    "    x[:,8] = torch.as_tensor(data.Civilwar2.to_numpy())\n",
    "    x[:,9] = torch.as_tensor(data.War.to_numpy())\n",
    "    y = torch.as_tensor(data.PIRI.to_numpy()).double()\n",
    "\n",
    "    unit_means = torch.zeros(n,)\n",
    "    for i in range(n):\n",
    "        unit_means[i] = y[x[:,1]==i].mean()\n",
    "\n",
    "    return x.double(), y.double(), unit_means.double()\n",
    "\n",
    "train_x, train_y, unit_means = load_PIRI_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build GPR model with unit trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model specification: PIRI gp model with unit trends\n",
    "# PIRI ~ AIShame + u_i(t) + cat_rat + ccpr_rat \n",
    "#            + democratic + log(gdppc) + log(pop) \n",
    "#            + Civilwar2 + War\n",
    "# u_i(t) ~ GP(b_i, K_t)\n",
    "\n",
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, ard_num_dim=None):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = MaskMean(active_dims=1, \\\n",
    "                base_mean=ConstantVectorMean(d=train_x[:,1].unique().size()[0]))\n",
    "        # year kernel * country kernel\n",
    "        self.unit_covar_module = ScaleKernel(RBFKernel(active_dims=0)*RBFKernel(active_dims=1))\n",
    "        self.x_covar_module = torch.nn.ModuleList([ScaleKernel(RBFKernel(\\\n",
    "            active_dims=(i))) for i in [6,7]])\n",
    "        self.binary_covar_module = torch.nn.ModuleList([ScaleKernel(RBFKernel(\\\n",
    "            active_dims=(i))) for i in [3,4,5,8,9]])\n",
    "        self.effect_covar_module = ScaleKernel(RBFKernel(active_dims=2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        unit_covar_x = self.unit_covar_module(x)\n",
    "        effect_covar_x = self.effect_covar_module(x)\n",
    "        covar_x = unit_covar_x + effect_covar_x\n",
    "        for i, _ in enumerate(self.x_covar_module):\n",
    "            covar_x += self.x_covar_module[i](x)\n",
    "        for i, _ in enumerate(self.binary_covar_module):\n",
    "            covar_x += self.binary_covar_module[i](x)\n",
    "        \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPModel(\n",
       "  (likelihood): GaussianLikelihood(\n",
       "    (noise_covar): HomoskedasticNoise(\n",
       "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "    )\n",
       "  )\n",
       "  (mean_module): MaskMean(\n",
       "    (base_mean): ConstantVectorMean()\n",
       "  )\n",
       "  (unit_covar_module): ScaleKernel(\n",
       "    (base_kernel): ProductKernel(\n",
       "      (kernels): ModuleList(\n",
       "        (0): RBFKernel(\n",
       "          (raw_lengthscale_constraint): Positive()\n",
       "        )\n",
       "        (1): RBFKernel(\n",
       "          (raw_lengthscale_constraint): Positive()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (raw_outputscale_constraint): Positive()\n",
       "  )\n",
       "  (x_covar_module): ModuleList(\n",
       "    (0): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (1): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "  )\n",
       "  (binary_covar_module): ModuleList(\n",
       "    (0): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (1): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (2): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (3): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (4): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "  )\n",
       "  (effect_covar_module): ScaleKernel(\n",
       "    (base_kernel): RBFKernel(\n",
       "      (raw_lengthscale_constraint): Positive()\n",
       "    )\n",
       "    (raw_outputscale_constraint): Positive()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood = GaussianLikelihood()\n",
    "model = GPModel(train_x, train_y, likelihood).double()\n",
    "\n",
    "# initialize model parameters\n",
    "hypers = {\n",
    "    'mean_module.base_mean.constantvector': unit_means,\n",
    "    'likelihood.noise_covar.noise': torch.tensor(0.25),\n",
    "    'unit_covar_module.base_kernel.kernels.0.lengthscale': torch.tensor(4),\n",
    "    'unit_covar_module.base_kernel.kernels.1.lengthscale': torch.tensor(0.01),\n",
    "    'unit_covar_module.outputscale': torch.tensor(0.25),\n",
    "    'x_covar_module.0.outputscale': torch.tensor(0.25),\n",
    "    'x_covar_module.1.outputscale': torch.tensor(0.25),\n",
    "    'binary_covar_module.0.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.1.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.2.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.3.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'binary_covar_module.4.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'effect_covar_module.base_kernel.lengthscale': torch.tensor(0.01),\n",
    "    'effect_covar_module.outputscale': torch.tensor(0.25)\n",
    "}    \n",
    "\n",
    "model.initialize(**hypers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model by optimizing hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 2.310 \n",
      "Iter 2/50 - Loss: 2.189 \n",
      "Iter 3/50 - Loss: 2.085 \n",
      "Iter 4/50 - Loss: 1.997 \n",
      "Iter 5/50 - Loss: 1.907 \n",
      "Iter 6/50 - Loss: 1.842 \n",
      "Iter 7/50 - Loss: 1.791 \n",
      "Iter 8/50 - Loss: 1.747 \n",
      "Iter 9/50 - Loss: 1.706 \n",
      "Iter 10/50 - Loss: 1.675 \n",
      "Iter 11/50 - Loss: 1.655 \n",
      "Iter 12/50 - Loss: 1.638 \n",
      "Iter 13/50 - Loss: 1.622 \n",
      "Iter 14/50 - Loss: 1.602 \n",
      "Iter 15/50 - Loss: 1.592 \n",
      "Iter 16/50 - Loss: 1.584 \n",
      "Iter 17/50 - Loss: 1.582 \n",
      "Iter 18/50 - Loss: 1.574 \n",
      "Iter 19/50 - Loss: 1.574 \n",
      "Iter 20/50 - Loss: 1.574 \n",
      "Iter 21/50 - Loss: 1.573 \n",
      "Iter 22/50 - Loss: 1.570 \n",
      "Iter 23/50 - Loss: 1.565 \n",
      "Iter 24/50 - Loss: 1.571 \n",
      "Iter 25/50 - Loss: 1.568 \n",
      "Iter 26/50 - Loss: 1.573 \n",
      "Iter 27/50 - Loss: 1.577 \n",
      "Iter 28/50 - Loss: 1.576 \n",
      "Iter 29/50 - Loss: 1.574 \n",
      "Iter 30/50 - Loss: 1.570 \n",
      "Iter 31/50 - Loss: 1.569 \n",
      "Iter 32/50 - Loss: 1.572 \n",
      "Iter 33/50 - Loss: 1.578 \n",
      "Iter 34/50 - Loss: 1.570 \n",
      "Iter 35/50 - Loss: 1.574 \n",
      "Iter 36/50 - Loss: 1.573 \n",
      "Iter 37/50 - Loss: 1.570 \n",
      "Iter 38/50 - Loss: 1.574 \n",
      "Iter 39/50 - Loss: 1.571 \n",
      "Iter 40/50 - Loss: 1.577 \n",
      "Iter 41/50 - Loss: 1.571 \n",
      "Iter 42/50 - Loss: 1.568 \n",
      "Iter 43/50 - Loss: 1.576 \n",
      "Iter 44/50 - Loss: 1.569 \n",
      "Iter 45/50 - Loss: 1.570 \n",
      "Iter 46/50 - Loss: 1.570 \n",
      "Iter 47/50 - Loss: 1.572 \n",
      "Iter 48/50 - Loss: 1.570 \n",
      "Iter 49/50 - Loss: 1.560 \n",
      "Iter 50/50 - Loss: 1.567 \n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "# freeze length scale in the country component in unit covar\n",
    "# freeze constant unit means\n",
    "all_params = set(model.parameters())\n",
    "final_params = list(all_params - \\\n",
    "            {model.unit_covar_module.base_kernel.kernels[1].raw_lengthscale, \\\n",
    "            model.mean_module.base_mean.constantvector, \\\n",
    "        #   model.x_covar_module[0].raw_outputscale,\n",
    "        #   model.x_covar_module[1].raw_outputscale,\n",
    "            model.binary_covar_module[0].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[1].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[2].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[3].base_kernel.raw_lengthscale,\n",
    "            model.binary_covar_module[4].base_kernel.raw_lengthscale})\n",
    "        #   model.effect_covar_module.base_kernel.raw_lengthscale,\n",
    "        #   model.effect_covar_module.raw_outputscale})\n",
    "optimizer = torch.optim.Adam(final_params, lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 50\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f '  % (\n",
    "        i + 1, training_iter, loss.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate posterior of PIRI effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effect: 0.348 +- 0.148\n",
      "\n",
      "model evidence: -3348.776 \n",
      "\n",
      "BIC: 6789.559 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_model_state = model.state_dict()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out = likelihood(model(train_x))\n",
    "    mu_f = out.mean\n",
    "    V = out.covariance_matrix\n",
    "    L = torch.linalg.cholesky(V, upper=False)\n",
    "\n",
    "# model.load_state_dict(full_model_state)\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    model.unit_covar_module.outputscale = 0\n",
    "    for i,_ in enumerate(model.x_covar_module):\n",
    "        model.x_covar_module[i].outputscale = 0\n",
    "    for i,_ in enumerate(model.binary_covar_module):\n",
    "        model.binary_covar_module[i].outputscale = 0\n",
    "    effect_covar = model(train_x).covariance_matrix\n",
    "\n",
    "# get posterior effect mean\n",
    "alpha = torch.linalg.solve(L.t(),torch.linalg.solve(L,train_y-mu_f))\n",
    "tmp = torch.linalg.solve(L, effect_covar)\n",
    "post_effect_mean = effect_covar @ alpha\n",
    "# get posterior effect covariance\n",
    "post_effect_covar = effect_covar - tmp.t() @ tmp\n",
    "\n",
    "effect = post_effect_mean[train_x[:,2]==1].mean() - post_effect_mean[train_x[:,2]==0].mean()\n",
    "effect_std = post_effect_covar.diag().mean().sqrt()\n",
    "BIC = (2+3+6+1)*torch.log(torch.tensor(train_x.size()[0])) + 2*loss*train_x.size()[0]\n",
    "print(\"effect: {:0.3f} +- {:0.3f}\\n\".format(effect, effect_std))\n",
    "print(\"model evidence: {:0.3f} \\n\".format(-loss*train_x.size()[0]))\n",
    "print(\"BIC: {:0.3f} \\n\".format(BIC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
