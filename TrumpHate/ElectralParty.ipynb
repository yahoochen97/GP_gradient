{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Electoral parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd69db045a0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load gpytoch and other packages\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gpytorch\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt\n",
    "from gpytorch.means import ZeroMean, LinearMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from datetime import datetime\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data = pd.read_stata(\"./data/rep_clark_2006a.dta\")\n",
    "data = data[[\"enep1\", \"eneg\", \"logmag\", \"uppertier\", \"enpres\", \"proximity1\"]]\n",
    "\n",
    "# train_x: eneg, logmag, uppertier, enpres, proximity1\n",
    "train_x = torch.tensor((data.eneg.values, data.logmag.values, data.uppertier.values,\\\n",
    "    data.enpres.values, data.proximity1.values, data.eneg.values*data.logmag.values,\\\n",
    "    data.uppertier.values*data.eneg.values, data.proximity1.values*data.eneg.values)).t().double()\n",
    "train_y = torch.tensor(data.enep1.values).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import ExactGP\n",
    "\n",
    "class GPModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = LinearMean(input_size=train_x.size(1), bias=True)\n",
    "        self.covar_module = ScaleKernel(RBFKernel(ard_num_dims=train_x.size(1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0/500 - Loss: 200.988 \n",
      "Iter 50/500 - Loss: 2.575 \n",
      "Iter 100/500 - Loss: 1.867 \n",
      "Iter 150/500 - Loss: 1.710 \n",
      "Iter 200/500 - Loss: 1.640 \n",
      "Iter 250/500 - Loss: 1.602 \n",
      "Iter 300/500 - Loss: 1.575 \n",
      "Iter 350/500 - Loss: 1.554 \n",
      "Iter 400/500 - Loss: 1.525 \n",
      "Iter 450/500 - Loss: 1.505 \n"
     ]
    }
   ],
   "source": [
    "likelihood = GaussianLikelihood()\n",
    "model = GPModel(train_x, train_y, likelihood).double()\n",
    "\n",
    "# train model\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "all_params = set(model.parameters())\n",
    "optimizer = torch.optim.Adam(all_params, lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 500\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    if i % 50 == 0:\n",
    "        print('Iter %d/%d - Loss: %.3f '  % (\n",
    "            i , training_iter, loss.item()\n",
    "        ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yahoo/anaconda3/lib/python3.7/site-packages/gpytorch/models/exact_gp.py:275: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  GPInputWarning,\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# number of empirically sample \n",
    "n_samples = 100\n",
    "\n",
    "with gpytorch.settings.fast_pred_var():\n",
    "    test_x = train_x.clone().detach().requires_grad_(True)\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "    x_grad = torch.autograd.grad(observed_pred.mean.sum(), test_x, retain_graph=True)[0]\n",
    "\n",
    "    sampled_pred = observed_pred.rsample(torch.Size([n_samples]))\n",
    "    sampled_dydtest_x = torch.stack([torch.autograd.grad(pred.sum(),\\\n",
    "                                     test_x, retain_graph=True)[0] for pred in sampled_pred])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 x  est_mean   est_std         t        pvalue\n",
      "0             eneg -1.835348  14.27166 -0.128601  4.488367e-01\n",
      "1           logmag  0.283421   0.23720  1.194860  1.160709e-01\n",
      "2        uppertier -0.032908   0.05732 -0.574119  2.829436e-01\n",
      "3           enpres  0.496657   0.28705  1.730209  4.179645e-02\n",
      "4       proximity1 -0.727712   0.11130 -6.538296  3.111178e-11\n",
      "5      logmag:eneg  0.224086   0.22973  0.975434  1.646726e-01\n",
      "6   uppertier:eneg  0.058874   0.08464  0.695576  2.433472e-01\n",
      "7  proximity1:eneg -0.046701   0.18241 -0.256024  3.989660e-01\n"
     ]
    }
   ],
   "source": [
    "est_std = np.sqrt(sampled_dydtest_x.mean(1).var(0) + \\\n",
    "                  sampled_dydtest_x.var(1).mean(0)).round(decimals=5)\n",
    "covariate_names = [\"eneg\", \"logmag\", \"uppertier\", \"enpres\", \"proximity1\",\\\n",
    "                   \"logmag:eneg\",\"uppertier:eneg\",\"proximity1:eneg\"]\n",
    "results = pd.DataFrame({\"x\": covariate_names, \\\n",
    "                        'est_mean': x_grad.mean(axis=0),\n",
    "                        'est_std': est_std})\n",
    "results[\"t\"] = results['est_mean'].values/results['est_std'].values\n",
    "results[\"pvalue\"] = 1 - norm.cdf(np.abs(results[\"t\"].values))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model evidence: -725.102 \n",
      "\n",
      "BIC: 1524.462 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "BIC = (5+5+1+1)*torch.log(torch.tensor(train_x.size()[0])) + 2*loss*train_x.size()[0]\n",
    "print(\"model evidence: {:0.3f} \\n\".format(-loss*train_x.size()[0]))\n",
    "print(\"BIC: {:0.3f} \\n\".format(BIC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  enep1   R-squared:                       0.397\n",
      "Model:                            OLS   Adj. R-squared:                  0.387\n",
      "Method:                 Least Squares   F-statistic:                     39.28\n",
      "Date:                Tue, 31 Oct 2023   Prob (F-statistic):           5.38e-48\n",
      "Time:                        14:17:50   Log-Likelihood:                -826.23\n",
      "No. Observations:                 487   AIC:                             1670.\n",
      "Df Residuals:                     478   BIC:                             1708.\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Intercept             2.9157      0.176     16.605      0.000       2.571       3.261\n",
      "eneg                  0.1116      0.071      1.563      0.119      -0.029       0.252\n",
      "logmag                0.0780      0.116      0.673      0.501      -0.150       0.306\n",
      "uppertier            -0.0566      0.020     -2.803      0.005      -0.096      -0.017\n",
      "enpres                0.2638      0.064      4.103      0.000       0.137       0.390\n",
      "proximity1           -3.0976      0.352     -8.791      0.000      -3.790      -2.405\n",
      "logmag:eneg           0.2637      0.067      3.915      0.000       0.131       0.396\n",
      "uppertier:eneg        0.0592      0.014      4.141      0.000       0.031       0.087\n",
      "proximity1:enpres     0.6832      0.137      4.976      0.000       0.413       0.953\n",
      "==============================================================================\n",
      "Omnibus:                      174.993   Durbin-Watson:                   0.908\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              647.096\n",
      "Skew:                           1.625   Prob(JB):                    3.05e-141\n",
      "Kurtosis:                       7.618   Cond. No.                         115.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "lm = sm.ols('enep1 ~ eneg+logmag+uppertier+enpres+proximity1+\\\n",
    "        logmag:eneg+uppertier:eneg+proximity1:enpres', data).fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
