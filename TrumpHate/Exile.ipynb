{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exile on Online Opposition\n",
    "\n",
    "Examine the effect of exile on percentage of tweets' harsh criticism of Venezuela government.\n",
    "\n",
    "ESBERG, J., & SIEGEL, A. (2023). How Exile Shapes Online Opposition: Evidence from Venezuela. American Political Science Review, 117(4), 1361-1378."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7faa573055a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load gpytoch and other packages\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gpytorch\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt\n",
    "from gpytorch.means import ZeroMean, LinearMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from datetime import datetime\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we first implement GPR for right panel of figure 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data = pd.read_csv(\"./data/exile.csv\")\n",
    "data = data[[\"perc_harsh_criticism\", \"lead_lags\",\"month\",\"num_tweets\", \"date_of_exile\", \"actor.id\"]]\n",
    "data = data[~data.lead_lags.isna()]\n",
    "\n",
    "def diff_month(d1, d2):\n",
    "    d1 = datetime.strptime(d1,\"%Y-%m-%d\")\n",
    "    d2 = datetime.strptime(d2,\"%Y-%m-%d\")\n",
    "    return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
    "\n",
    "# xs: unit id, month, log_num_tweets, dummies for lead_lags\n",
    "xs = data.month.apply(lambda x: diff_month(x,\"2013-01-01\"))\n",
    "xs = torch.tensor(np.hstack((data[\"actor.id\"].astype('category').cat.codes.values.reshape((-1,1)),\\\n",
    "                    xs.values.reshape((-1,1)),\n",
    "                    np.log(data.num_tweets.values+1).reshape((-1,1)), \\\n",
    "                    pd.get_dummies(data['lead_lags']).values))).double()\n",
    "ys = torch.tensor(data.perc_harsh_criticism.values).double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we build a Gaussian process regression model with a linear mean function and an automatic relevance determination RBF kernel:\n",
    "$$\n",
    "f(t,D) \\sim\\mathcal{GP}(\\beta x,K)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.means import Mean\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "class ConstantVectorMean(Mean):\n",
    "    def __init__(self, n, prior=None, batch_shape=torch.Size(), **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_shape = batch_shape\n",
    "        self.register_parameter(name=\"constantvector\",\\\n",
    "                 parameter=torch.nn.Parameter(torch.zeros(*batch_shape, n)))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.constantvector[input.long()]\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=False)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        # linear mean\n",
    "        self.mean_module = LinearMean(input_size=(inducing_points.size(1)-2), bias=False)\n",
    "        self.covar_module = ScaleKernel(RBFKernel(ard_num_dims=(inducing_points.size(1)-2)))\n",
    "        self.t_covar_module = ScaleKernel(RBFKernel(active_dims=[0])*RBFKernel(active_dims=[1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x[:,2:])\n",
    "        covar_x = self.covar_module(x[:,2:]) + self.t_covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 2 - Loss: 189.349\n",
      "Epoch 1 Iter 3 - Loss: 171.356\n",
      "Epoch 1 Iter 4 - Loss: 180.147\n",
      "Epoch 1 Iter 5 - Loss: 153.898\n",
      "Epoch 1 Iter 6 - Loss: 123.403\n",
      "Epoch 1 Iter 7 - Loss: 135.199\n",
      "Epoch 1 Iter 8 - Loss: 133.921\n",
      "Epoch 1 Iter 9 - Loss: 108.711\n",
      "Epoch 1 Iter 10 - Loss: 117.488\n",
      "Epoch 1 Iter 11 - Loss: 99.816\n",
      "Epoch 1 Iter 12 - Loss: 106.587\n",
      "Epoch 1 Iter 13 - Loss: 98.036\n",
      "Epoch 1 Iter 14 - Loss: 83.112\n",
      "Epoch 1 Iter 15 - Loss: 88.746\n",
      "Epoch 1 Iter 16 - Loss: 69.555\n",
      "Epoch 1 Iter 17 - Loss: 86.583\n",
      "Epoch 1 Iter 18 - Loss: 76.788\n",
      "Epoch 1 Iter 19 - Loss: 74.607\n",
      "Epoch 1 Iter 20 - Loss: 68.791\n",
      "Epoch 1 Iter 21 - Loss: 79.606\n",
      "Epoch 1 Iter 22 - Loss: 73.140\n",
      "Epoch 1 Iter 23 - Loss: 93.630\n",
      "Epoch 2 Iter 2 - Loss: 61.623\n",
      "Epoch 2 Iter 3 - Loss: 67.125\n",
      "Epoch 2 Iter 4 - Loss: 80.656\n",
      "Epoch 2 Iter 5 - Loss: 62.133\n",
      "Epoch 2 Iter 6 - Loss: 59.033\n",
      "Epoch 2 Iter 7 - Loss: 68.869\n",
      "Epoch 2 Iter 8 - Loss: 56.350\n",
      "Epoch 2 Iter 9 - Loss: 58.138\n",
      "Epoch 2 Iter 10 - Loss: 60.868\n",
      "Epoch 2 Iter 11 - Loss: 59.588\n",
      "Epoch 2 Iter 12 - Loss: 47.268\n",
      "Epoch 2 Iter 13 - Loss: 61.337\n",
      "Epoch 2 Iter 14 - Loss: 53.275\n",
      "Epoch 2 Iter 15 - Loss: 59.892\n",
      "Epoch 2 Iter 16 - Loss: 55.469\n",
      "Epoch 2 Iter 17 - Loss: 56.861\n",
      "Epoch 2 Iter 18 - Loss: 52.741\n",
      "Epoch 2 Iter 19 - Loss: 48.950\n",
      "Epoch 2 Iter 20 - Loss: 50.337\n",
      "Epoch 2 Iter 21 - Loss: 48.558\n",
      "Epoch 2 Iter 22 - Loss: 51.288\n",
      "Epoch 2 Iter 23 - Loss: 41.961\n",
      "Epoch 3 Iter 2 - Loss: 44.793\n",
      "Epoch 3 Iter 3 - Loss: 53.371\n",
      "Epoch 3 Iter 4 - Loss: 53.746\n",
      "Epoch 3 Iter 5 - Loss: 41.694\n",
      "Epoch 3 Iter 6 - Loss: 48.739\n",
      "Epoch 3 Iter 7 - Loss: 53.243\n",
      "Epoch 3 Iter 8 - Loss: 45.440\n",
      "Epoch 3 Iter 9 - Loss: 46.852\n",
      "Epoch 3 Iter 10 - Loss: 49.413\n",
      "Epoch 3 Iter 11 - Loss: 55.188\n",
      "Epoch 3 Iter 12 - Loss: 41.275\n",
      "Epoch 3 Iter 13 - Loss: 43.510\n",
      "Epoch 3 Iter 14 - Loss: 50.020\n",
      "Epoch 3 Iter 15 - Loss: 48.559\n",
      "Epoch 3 Iter 16 - Loss: 41.392\n",
      "Epoch 3 Iter 17 - Loss: 39.140\n",
      "Epoch 3 Iter 18 - Loss: 38.500\n",
      "Epoch 3 Iter 19 - Loss: 35.644\n",
      "Epoch 3 Iter 20 - Loss: 36.297\n",
      "Epoch 3 Iter 21 - Loss: 43.707\n",
      "Epoch 3 Iter 22 - Loss: 37.134\n",
      "Epoch 3 Iter 23 - Loss: 36.251\n",
      "Epoch 4 Iter 2 - Loss: 41.287\n",
      "Epoch 4 Iter 3 - Loss: 37.654\n",
      "Epoch 4 Iter 4 - Loss: 42.679\n",
      "Epoch 4 Iter 5 - Loss: 37.068\n",
      "Epoch 4 Iter 6 - Loss: 41.002\n",
      "Epoch 4 Iter 7 - Loss: 38.035\n",
      "Epoch 4 Iter 8 - Loss: 39.266\n",
      "Epoch 4 Iter 9 - Loss: 37.376\n",
      "Epoch 4 Iter 10 - Loss: 36.013\n",
      "Epoch 4 Iter 11 - Loss: 37.379\n",
      "Epoch 4 Iter 12 - Loss: 38.246\n",
      "Epoch 4 Iter 13 - Loss: 38.008\n",
      "Epoch 4 Iter 14 - Loss: 37.263\n",
      "Epoch 4 Iter 15 - Loss: 32.008\n",
      "Epoch 4 Iter 16 - Loss: 37.268\n",
      "Epoch 4 Iter 17 - Loss: 33.541\n",
      "Epoch 4 Iter 18 - Loss: 38.035\n",
      "Epoch 4 Iter 19 - Loss: 39.629\n",
      "Epoch 4 Iter 20 - Loss: 38.125\n",
      "Epoch 4 Iter 21 - Loss: 30.332\n",
      "Epoch 4 Iter 22 - Loss: 39.072\n",
      "Epoch 4 Iter 23 - Loss: 40.309\n",
      "Epoch 5 Iter 2 - Loss: 34.850\n",
      "Epoch 5 Iter 3 - Loss: 35.771\n",
      "Epoch 5 Iter 4 - Loss: 35.479\n",
      "Epoch 5 Iter 5 - Loss: 36.980\n",
      "Epoch 5 Iter 6 - Loss: 31.040\n",
      "Epoch 5 Iter 7 - Loss: 32.950\n",
      "Epoch 5 Iter 8 - Loss: 46.175\n",
      "Epoch 5 Iter 9 - Loss: 30.917\n",
      "Epoch 5 Iter 10 - Loss: 35.087\n",
      "Epoch 5 Iter 11 - Loss: 30.333\n",
      "Epoch 5 Iter 12 - Loss: 29.726\n",
      "Epoch 5 Iter 13 - Loss: 31.625\n",
      "Epoch 5 Iter 14 - Loss: 38.649\n",
      "Epoch 5 Iter 15 - Loss: 33.612\n",
      "Epoch 5 Iter 16 - Loss: 30.612\n",
      "Epoch 5 Iter 17 - Loss: 36.639\n",
      "Epoch 5 Iter 18 - Loss: 35.450\n",
      "Epoch 5 Iter 19 - Loss: 32.706\n",
      "Epoch 5 Iter 20 - Loss: 27.682\n",
      "Epoch 5 Iter 21 - Loss: 27.857\n",
      "Epoch 5 Iter 22 - Loss: 31.392\n",
      "Epoch 5 Iter 23 - Loss: 36.723\n",
      "Epoch 6 Iter 2 - Loss: 35.198\n",
      "Epoch 6 Iter 3 - Loss: 28.089\n",
      "Epoch 6 Iter 4 - Loss: 31.817\n",
      "Epoch 6 Iter 5 - Loss: 30.249\n",
      "Epoch 6 Iter 6 - Loss: 34.348\n",
      "Epoch 6 Iter 7 - Loss: 31.242\n",
      "Epoch 6 Iter 8 - Loss: 30.825\n",
      "Epoch 6 Iter 9 - Loss: 29.725\n",
      "Epoch 6 Iter 10 - Loss: 27.649\n",
      "Epoch 6 Iter 11 - Loss: 27.503\n",
      "Epoch 6 Iter 12 - Loss: 29.672\n",
      "Epoch 6 Iter 13 - Loss: 26.898\n",
      "Epoch 6 Iter 14 - Loss: 30.078\n",
      "Epoch 6 Iter 15 - Loss: 29.426\n",
      "Epoch 6 Iter 16 - Loss: 32.384\n",
      "Epoch 6 Iter 17 - Loss: 27.683\n",
      "Epoch 6 Iter 18 - Loss: 28.130\n",
      "Epoch 6 Iter 19 - Loss: 30.748\n",
      "Epoch 6 Iter 20 - Loss: 34.868\n",
      "Epoch 6 Iter 21 - Loss: 31.762\n",
      "Epoch 6 Iter 22 - Loss: 27.758\n",
      "Epoch 6 Iter 23 - Loss: 28.739\n",
      "Epoch 7 Iter 2 - Loss: 29.081\n",
      "Epoch 7 Iter 3 - Loss: 30.587\n",
      "Epoch 7 Iter 4 - Loss: 33.057\n",
      "Epoch 7 Iter 5 - Loss: 23.551\n",
      "Epoch 7 Iter 6 - Loss: 29.280\n",
      "Epoch 7 Iter 7 - Loss: 29.097\n",
      "Epoch 7 Iter 8 - Loss: 30.503\n",
      "Epoch 7 Iter 9 - Loss: 28.526\n",
      "Epoch 7 Iter 10 - Loss: 25.958\n",
      "Epoch 7 Iter 11 - Loss: 25.953\n",
      "Epoch 7 Iter 12 - Loss: 27.153\n",
      "Epoch 7 Iter 13 - Loss: 29.681\n",
      "Epoch 7 Iter 14 - Loss: 25.742\n",
      "Epoch 7 Iter 15 - Loss: 34.333\n",
      "Epoch 7 Iter 16 - Loss: 28.017\n",
      "Epoch 7 Iter 17 - Loss: 33.754\n",
      "Epoch 7 Iter 18 - Loss: 25.203\n",
      "Epoch 7 Iter 19 - Loss: 26.930\n",
      "Epoch 7 Iter 20 - Loss: 21.454\n",
      "Epoch 7 Iter 21 - Loss: 26.030\n",
      "Epoch 7 Iter 22 - Loss: 25.107\n",
      "Epoch 7 Iter 23 - Loss: 25.762\n",
      "Epoch 8 Iter 2 - Loss: 26.680\n",
      "Epoch 8 Iter 3 - Loss: 27.045\n",
      "Epoch 8 Iter 4 - Loss: 22.606\n",
      "Epoch 8 Iter 5 - Loss: 32.886\n",
      "Epoch 8 Iter 6 - Loss: 25.420\n",
      "Epoch 8 Iter 7 - Loss: 28.199\n",
      "Epoch 8 Iter 8 - Loss: 27.391\n",
      "Epoch 8 Iter 9 - Loss: 20.888\n",
      "Epoch 8 Iter 10 - Loss: 29.242\n",
      "Epoch 8 Iter 11 - Loss: 26.713\n",
      "Epoch 8 Iter 12 - Loss: 24.108\n",
      "Epoch 8 Iter 13 - Loss: 29.830\n",
      "Epoch 8 Iter 14 - Loss: 21.647\n",
      "Epoch 8 Iter 15 - Loss: 25.081\n",
      "Epoch 8 Iter 16 - Loss: 27.167\n",
      "Epoch 8 Iter 17 - Loss: 24.913\n",
      "Epoch 8 Iter 18 - Loss: 25.207\n",
      "Epoch 8 Iter 19 - Loss: 26.251\n",
      "Epoch 8 Iter 20 - Loss: 23.645\n",
      "Epoch 8 Iter 21 - Loss: 26.076\n",
      "Epoch 8 Iter 22 - Loss: 23.898\n",
      "Epoch 8 Iter 23 - Loss: 13.718\n",
      "Epoch 9 Iter 2 - Loss: 26.748\n",
      "Epoch 9 Iter 3 - Loss: 23.496\n",
      "Epoch 9 Iter 4 - Loss: 27.761\n",
      "Epoch 9 Iter 5 - Loss: 24.603\n",
      "Epoch 9 Iter 6 - Loss: 22.835\n",
      "Epoch 9 Iter 7 - Loss: 26.292\n",
      "Epoch 9 Iter 8 - Loss: 24.957\n",
      "Epoch 9 Iter 9 - Loss: 24.907\n",
      "Epoch 9 Iter 10 - Loss: 25.280\n",
      "Epoch 9 Iter 11 - Loss: 23.155\n",
      "Epoch 9 Iter 12 - Loss: 25.753\n",
      "Epoch 9 Iter 13 - Loss: 25.954\n",
      "Epoch 9 Iter 14 - Loss: 19.584\n",
      "Epoch 9 Iter 15 - Loss: 24.855\n",
      "Epoch 9 Iter 16 - Loss: 26.955\n",
      "Epoch 9 Iter 17 - Loss: 21.962\n",
      "Epoch 9 Iter 18 - Loss: 21.879\n",
      "Epoch 9 Iter 19 - Loss: 24.117\n",
      "Epoch 9 Iter 20 - Loss: 21.014\n",
      "Epoch 9 Iter 21 - Loss: 24.430\n",
      "Epoch 9 Iter 22 - Loss: 23.600\n",
      "Epoch 9 Iter 23 - Loss: 19.341\n",
      "Epoch 10 Iter 2 - Loss: 23.902\n",
      "Epoch 10 Iter 3 - Loss: 22.326\n",
      "Epoch 10 Iter 4 - Loss: 22.700\n",
      "Epoch 10 Iter 5 - Loss: 19.706\n",
      "Epoch 10 Iter 6 - Loss: 22.326\n",
      "Epoch 10 Iter 7 - Loss: 20.244\n",
      "Epoch 10 Iter 8 - Loss: 23.134\n",
      "Epoch 10 Iter 9 - Loss: 23.745\n",
      "Epoch 10 Iter 10 - Loss: 22.377\n",
      "Epoch 10 Iter 11 - Loss: 22.828\n",
      "Epoch 10 Iter 12 - Loss: 25.233\n",
      "Epoch 10 Iter 13 - Loss: 26.257\n",
      "Epoch 10 Iter 14 - Loss: 23.003\n",
      "Epoch 10 Iter 15 - Loss: 20.714\n",
      "Epoch 10 Iter 16 - Loss: 23.399\n",
      "Epoch 10 Iter 17 - Loss: 21.527\n",
      "Epoch 10 Iter 18 - Loss: 22.418\n",
      "Epoch 10 Iter 19 - Loss: 20.041\n",
      "Epoch 10 Iter 20 - Loss: 25.519\n",
      "Epoch 10 Iter 21 - Loss: 24.445\n",
      "Epoch 10 Iter 22 - Loss: 22.164\n",
      "Epoch 10 Iter 23 - Loss: 15.548\n",
      "Epoch 11 Iter 2 - Loss: 24.750\n",
      "Epoch 11 Iter 3 - Loss: 21.896\n",
      "Epoch 11 Iter 4 - Loss: 20.566\n",
      "Epoch 11 Iter 5 - Loss: 17.670\n",
      "Epoch 11 Iter 6 - Loss: 20.315\n",
      "Epoch 11 Iter 7 - Loss: 19.763\n",
      "Epoch 11 Iter 8 - Loss: 19.836\n",
      "Epoch 11 Iter 9 - Loss: 22.612\n",
      "Epoch 11 Iter 10 - Loss: 21.839\n",
      "Epoch 11 Iter 11 - Loss: 15.517\n",
      "Epoch 11 Iter 12 - Loss: 20.478\n",
      "Epoch 11 Iter 13 - Loss: 24.911\n",
      "Epoch 11 Iter 14 - Loss: 21.458\n",
      "Epoch 11 Iter 15 - Loss: 23.841\n",
      "Epoch 11 Iter 16 - Loss: 23.671\n",
      "Epoch 11 Iter 17 - Loss: 24.194\n",
      "Epoch 11 Iter 18 - Loss: 19.018\n",
      "Epoch 11 Iter 19 - Loss: 21.940\n",
      "Epoch 11 Iter 20 - Loss: 19.756\n",
      "Epoch 11 Iter 21 - Loss: 20.639\n",
      "Epoch 11 Iter 22 - Loss: 23.720\n",
      "Epoch 11 Iter 23 - Loss: 20.188\n",
      "Epoch 12 Iter 2 - Loss: 19.830\n",
      "Epoch 12 Iter 3 - Loss: 21.374\n",
      "Epoch 12 Iter 4 - Loss: 21.180\n",
      "Epoch 12 Iter 5 - Loss: 20.867\n",
      "Epoch 12 Iter 6 - Loss: 18.938\n",
      "Epoch 12 Iter 7 - Loss: 18.365\n",
      "Epoch 12 Iter 8 - Loss: 21.600\n",
      "Epoch 12 Iter 9 - Loss: 21.325\n",
      "Epoch 12 Iter 10 - Loss: 19.591\n",
      "Epoch 12 Iter 11 - Loss: 21.225\n",
      "Epoch 12 Iter 12 - Loss: 21.013\n",
      "Epoch 12 Iter 13 - Loss: 19.122\n",
      "Epoch 12 Iter 14 - Loss: 18.820\n",
      "Epoch 12 Iter 15 - Loss: 21.158\n",
      "Epoch 12 Iter 16 - Loss: 15.988\n",
      "Epoch 12 Iter 17 - Loss: 19.808\n",
      "Epoch 12 Iter 18 - Loss: 21.540\n",
      "Epoch 12 Iter 19 - Loss: 21.916\n",
      "Epoch 12 Iter 20 - Loss: 16.839\n",
      "Epoch 12 Iter 21 - Loss: 18.835\n",
      "Epoch 12 Iter 22 - Loss: 24.242\n",
      "Epoch 12 Iter 23 - Loss: 14.103\n",
      "Epoch 13 Iter 2 - Loss: 19.860\n",
      "Epoch 13 Iter 3 - Loss: 19.279\n",
      "Epoch 13 Iter 4 - Loss: 22.059\n",
      "Epoch 13 Iter 5 - Loss: 20.967\n",
      "Epoch 13 Iter 6 - Loss: 20.872\n",
      "Epoch 13 Iter 7 - Loss: 19.375\n",
      "Epoch 13 Iter 8 - Loss: 22.053\n",
      "Epoch 13 Iter 9 - Loss: 22.208\n",
      "Epoch 13 Iter 10 - Loss: 20.300\n",
      "Epoch 13 Iter 11 - Loss: 17.539\n",
      "Epoch 13 Iter 12 - Loss: 18.999\n",
      "Epoch 13 Iter 13 - Loss: 16.739\n",
      "Epoch 13 Iter 14 - Loss: 19.345\n",
      "Epoch 13 Iter 15 - Loss: 16.322\n",
      "Epoch 13 Iter 16 - Loss: 22.817\n",
      "Epoch 13 Iter 17 - Loss: 18.769\n",
      "Epoch 13 Iter 18 - Loss: 14.956\n",
      "Epoch 13 Iter 19 - Loss: 18.962\n",
      "Epoch 13 Iter 20 - Loss: 17.379\n",
      "Epoch 13 Iter 21 - Loss: 18.829\n",
      "Epoch 13 Iter 22 - Loss: 15.525\n",
      "Epoch 13 Iter 23 - Loss: 17.662\n",
      "Epoch 14 Iter 2 - Loss: 19.111\n",
      "Epoch 14 Iter 3 - Loss: 17.621\n",
      "Epoch 14 Iter 4 - Loss: 20.054\n",
      "Epoch 14 Iter 5 - Loss: 16.296\n",
      "Epoch 14 Iter 6 - Loss: 15.310\n",
      "Epoch 14 Iter 7 - Loss: 20.370\n",
      "Epoch 14 Iter 8 - Loss: 18.214\n",
      "Epoch 14 Iter 9 - Loss: 18.101\n",
      "Epoch 14 Iter 10 - Loss: 18.457\n",
      "Epoch 14 Iter 11 - Loss: 17.607\n",
      "Epoch 14 Iter 12 - Loss: 17.607\n",
      "Epoch 14 Iter 13 - Loss: 20.519\n",
      "Epoch 14 Iter 14 - Loss: 19.487\n",
      "Epoch 14 Iter 15 - Loss: 16.105\n",
      "Epoch 14 Iter 16 - Loss: 17.883\n",
      "Epoch 14 Iter 17 - Loss: 19.053\n",
      "Epoch 14 Iter 18 - Loss: 15.797\n",
      "Epoch 14 Iter 19 - Loss: 20.188\n",
      "Epoch 14 Iter 20 - Loss: 17.469\n",
      "Epoch 14 Iter 21 - Loss: 20.989\n",
      "Epoch 14 Iter 22 - Loss: 20.318\n",
      "Epoch 14 Iter 23 - Loss: 12.613\n",
      "Epoch 15 Iter 2 - Loss: 15.815\n",
      "Epoch 15 Iter 3 - Loss: 14.917\n",
      "Epoch 15 Iter 4 - Loss: 17.006\n",
      "Epoch 15 Iter 5 - Loss: 17.691\n",
      "Epoch 15 Iter 6 - Loss: 18.795\n",
      "Epoch 15 Iter 7 - Loss: 17.166\n",
      "Epoch 15 Iter 8 - Loss: 20.129\n",
      "Epoch 15 Iter 9 - Loss: 18.809\n",
      "Epoch 15 Iter 10 - Loss: 16.702\n",
      "Epoch 15 Iter 11 - Loss: 16.645\n",
      "Epoch 15 Iter 12 - Loss: 16.330\n",
      "Epoch 15 Iter 13 - Loss: 17.831\n",
      "Epoch 15 Iter 14 - Loss: 20.028\n",
      "Epoch 15 Iter 15 - Loss: 19.680\n",
      "Epoch 15 Iter 16 - Loss: 16.700\n",
      "Epoch 15 Iter 17 - Loss: 16.008\n",
      "Epoch 15 Iter 18 - Loss: 17.018\n",
      "Epoch 15 Iter 19 - Loss: 17.285\n",
      "Epoch 15 Iter 20 - Loss: 18.419\n",
      "Epoch 15 Iter 21 - Loss: 15.125\n",
      "Epoch 15 Iter 22 - Loss: 18.921\n",
      "Epoch 15 Iter 23 - Loss: 17.593\n",
      "Epoch 16 Iter 2 - Loss: 16.613\n",
      "Epoch 16 Iter 3 - Loss: 17.635\n",
      "Epoch 16 Iter 4 - Loss: 19.837\n",
      "Epoch 16 Iter 5 - Loss: 17.202\n",
      "Epoch 16 Iter 6 - Loss: 15.282\n",
      "Epoch 16 Iter 7 - Loss: 15.359\n",
      "Epoch 16 Iter 8 - Loss: 14.904\n",
      "Epoch 16 Iter 9 - Loss: 16.603\n",
      "Epoch 16 Iter 10 - Loss: 18.308\n",
      "Epoch 16 Iter 11 - Loss: 19.349\n",
      "Epoch 16 Iter 12 - Loss: 18.629\n",
      "Epoch 16 Iter 13 - Loss: 14.753\n",
      "Epoch 16 Iter 14 - Loss: 18.317\n",
      "Epoch 16 Iter 15 - Loss: 16.845\n",
      "Epoch 16 Iter 16 - Loss: 16.690\n",
      "Epoch 16 Iter 17 - Loss: 15.733\n",
      "Epoch 16 Iter 18 - Loss: 16.037\n",
      "Epoch 16 Iter 19 - Loss: 20.147\n",
      "Epoch 16 Iter 20 - Loss: 14.625\n",
      "Epoch 16 Iter 21 - Loss: 14.672\n",
      "Epoch 16 Iter 22 - Loss: 17.851\n",
      "Epoch 16 Iter 23 - Loss: 24.604\n",
      "Epoch 17 Iter 2 - Loss: 14.542\n",
      "Epoch 17 Iter 3 - Loss: 12.427\n",
      "Epoch 17 Iter 4 - Loss: 15.829\n",
      "Epoch 17 Iter 5 - Loss: 15.181\n",
      "Epoch 17 Iter 6 - Loss: 16.914\n",
      "Epoch 17 Iter 7 - Loss: 18.832\n",
      "Epoch 17 Iter 8 - Loss: 17.853\n",
      "Epoch 17 Iter 9 - Loss: 18.654\n",
      "Epoch 17 Iter 10 - Loss: 16.751\n",
      "Epoch 17 Iter 11 - Loss: 13.964\n",
      "Epoch 17 Iter 12 - Loss: 16.391\n",
      "Epoch 17 Iter 13 - Loss: 15.994\n",
      "Epoch 17 Iter 14 - Loss: 14.730\n",
      "Epoch 17 Iter 15 - Loss: 14.194\n",
      "Epoch 17 Iter 16 - Loss: 13.196\n",
      "Epoch 17 Iter 17 - Loss: 17.600\n",
      "Epoch 17 Iter 18 - Loss: 16.857\n",
      "Epoch 17 Iter 19 - Loss: 15.490\n",
      "Epoch 17 Iter 20 - Loss: 19.057\n",
      "Epoch 17 Iter 21 - Loss: 16.076\n",
      "Epoch 17 Iter 22 - Loss: 15.295\n",
      "Epoch 17 Iter 23 - Loss: 41.013\n",
      "Epoch 18 Iter 2 - Loss: 13.502\n",
      "Epoch 18 Iter 3 - Loss: 15.930\n",
      "Epoch 18 Iter 4 - Loss: 16.119\n",
      "Epoch 18 Iter 5 - Loss: 14.184\n",
      "Epoch 18 Iter 6 - Loss: 15.372\n",
      "Epoch 18 Iter 7 - Loss: 18.127\n",
      "Epoch 18 Iter 8 - Loss: 16.732\n",
      "Epoch 18 Iter 9 - Loss: 16.044\n",
      "Epoch 18 Iter 10 - Loss: 14.470\n",
      "Epoch 18 Iter 11 - Loss: 14.260\n",
      "Epoch 18 Iter 12 - Loss: 15.973\n",
      "Epoch 18 Iter 13 - Loss: 16.311\n",
      "Epoch 18 Iter 14 - Loss: 13.781\n",
      "Epoch 18 Iter 15 - Loss: 15.819\n",
      "Epoch 18 Iter 16 - Loss: 17.820\n",
      "Epoch 18 Iter 17 - Loss: 16.992\n",
      "Epoch 18 Iter 18 - Loss: 16.526\n",
      "Epoch 18 Iter 19 - Loss: 16.773\n",
      "Epoch 18 Iter 20 - Loss: 16.559\n",
      "Epoch 18 Iter 21 - Loss: 14.659\n",
      "Epoch 18 Iter 22 - Loss: 15.327\n",
      "Epoch 18 Iter 23 - Loss: 15.650\n",
      "Epoch 19 Iter 2 - Loss: 16.792\n",
      "Epoch 19 Iter 3 - Loss: 14.406\n",
      "Epoch 19 Iter 4 - Loss: 14.922\n",
      "Epoch 19 Iter 5 - Loss: 15.001\n",
      "Epoch 19 Iter 6 - Loss: 13.794\n",
      "Epoch 19 Iter 7 - Loss: 14.800\n",
      "Epoch 19 Iter 8 - Loss: 15.257\n",
      "Epoch 19 Iter 9 - Loss: 13.810\n",
      "Epoch 19 Iter 10 - Loss: 15.369\n",
      "Epoch 19 Iter 11 - Loss: 15.603\n",
      "Epoch 19 Iter 12 - Loss: 16.847\n",
      "Epoch 19 Iter 13 - Loss: 14.904\n",
      "Epoch 19 Iter 14 - Loss: 14.764\n",
      "Epoch 19 Iter 15 - Loss: 14.499\n",
      "Epoch 19 Iter 16 - Loss: 16.829\n",
      "Epoch 19 Iter 17 - Loss: 15.887\n",
      "Epoch 19 Iter 18 - Loss: 14.800\n",
      "Epoch 19 Iter 19 - Loss: 17.212\n",
      "Epoch 19 Iter 20 - Loss: 16.292\n",
      "Epoch 19 Iter 21 - Loss: 13.537\n",
      "Epoch 19 Iter 22 - Loss: 15.171\n",
      "Epoch 19 Iter 23 - Loss: 14.243\n",
      "Epoch 20 Iter 2 - Loss: 14.553\n",
      "Epoch 20 Iter 3 - Loss: 14.905\n",
      "Epoch 20 Iter 4 - Loss: 14.564\n",
      "Epoch 20 Iter 5 - Loss: 15.471\n",
      "Epoch 20 Iter 6 - Loss: 13.323\n",
      "Epoch 20 Iter 7 - Loss: 13.154\n",
      "Epoch 20 Iter 8 - Loss: 16.702\n",
      "Epoch 20 Iter 9 - Loss: 15.320\n",
      "Epoch 20 Iter 10 - Loss: 19.411\n",
      "Epoch 20 Iter 11 - Loss: 13.154\n",
      "Epoch 20 Iter 12 - Loss: 15.329\n",
      "Epoch 20 Iter 13 - Loss: 13.732\n",
      "Epoch 20 Iter 14 - Loss: 12.458\n",
      "Epoch 20 Iter 15 - Loss: 16.137\n",
      "Epoch 20 Iter 16 - Loss: 13.939\n",
      "Epoch 20 Iter 17 - Loss: 14.067\n",
      "Epoch 20 Iter 18 - Loss: 15.555\n",
      "Epoch 20 Iter 19 - Loss: 16.169\n",
      "Epoch 20 Iter 20 - Loss: 15.909\n",
      "Epoch 20 Iter 21 - Loss: 13.155\n",
      "Epoch 20 Iter 22 - Loss: 12.799\n",
      "Epoch 20 Iter 23 - Loss: 13.716\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "inducing_points = xs[np.random.choice(xs.size(0),1000,replace=False),:]\n",
    "model = GPModel(inducing_points=inducing_points).double()\n",
    "likelihood = GaussianLikelihood().double()\n",
    "\n",
    "train_dataset = TensorDataset(xs, ys)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# initialize model parameters\n",
    "model.t_covar_module.base_kernel.kernels[0].raw_lengthscale.require_grad = False\n",
    "model.t_covar_module.base_kernel.kernels[0].lengthscale = 0.1\n",
    "model.t_covar_module.raw_outputscale.require_grad = False\n",
    "model.t_covar_module.outputscale = 1.\n",
    "model.covar_module.raw_outputscale.requires_grad = False\n",
    "model.covar_module.outputscale = 1.\n",
    "likelihood.noise = 1.\n",
    "\n",
    "# train model\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': list(set(model.parameters()) - \\\n",
    "                {model.t_covar_module.base_kernel.kernels[0].raw_lengthscale,\\\n",
    "                model.t_covar_module.raw_outputscale,\\\n",
    "                model.covar_module.raw_outputscale})},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.05)\n",
    "\n",
    "# \"Loss\" for GPs\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=ys.size(0))\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    for j, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = -mll(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if j % 50:\n",
    "            print('Epoch %d Iter %d - Loss: %.3f' % (i + 1, j+1, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model and likelihood to evaluation mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out = model(xs)\n",
    "    mu_f = out.mean.numpy()\n",
    "    lower, upper = out.confidence_region()\n",
    "\n",
    "# store results\n",
    "results = pd.DataFrame({\"gpr_mean\":mu_f})\n",
    "results['true_y'] = ys\n",
    "results['gpr_lwr'] = lower\n",
    "results['gpr_upr'] = upper\n",
    "results['month'] = xs[:,1].numpy().astype(int)\n",
    "results['unit'] = xs[:,0].numpy().astype(int)\n",
    "results.to_csv(\"./results/exile_fitted_gpr.csv\",index=False) #save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# number of empirically sample \n",
    "n_samples = 100\n",
    "x_grad = np.zeros((xs.size(0),xs.size(1)))\n",
    "sampled_dydtest_x = np.zeros((n_samples, xs.size(0),xs.size(1)))\n",
    "\n",
    "# we proceed in small batches of size 100 for speed up\n",
    "for i in range(xs.size(0)//100):\n",
    "    with gpytorch.settings.fast_pred_var():\n",
    "        test_x = xs[(i*100):(i*100+100)].clone().detach().requires_grad_(True)\n",
    "        observed_pred = model(test_x)\n",
    "        dydtest_x = torch.autograd.grad(observed_pred.mean.sum(), test_x, retain_graph=True)[0]\n",
    "        x_grad[(i*100):(i*100+100)] = dydtest_x\n",
    "\n",
    "        sampled_pred = observed_pred.rsample(torch.Size([n_samples]))\n",
    "        sampled_dydtest_x[:,(i*100):(i*100+100),:] = torch.stack([torch.autograd.grad(pred.sum(), \\\n",
    "                                    test_x, retain_graph=True)[0] for pred in sampled_pred])\n",
    "        \n",
    "# last 100 rows\n",
    "with gpytorch.settings.fast_pred_var():\n",
    "    test_x = xs[(100*i+100):].clone().detach().requires_grad_(True)\n",
    "    observed_pred = model(test_x)\n",
    "    dydtest_x = torch.autograd.grad(observed_pred.mean.sum(), test_x, retain_graph=True)[0]\n",
    "    x_grad[(100*i+100):] = dydtest_x\n",
    "\n",
    "    sampled_pred = observed_pred.rsample(torch.Size([n_samples]))\n",
    "    sampled_dydtest_x[:,(100*i+100):,:] = torch.stack([torch.autograd.grad(pred.sum(),\\\n",
    "                                     test_x, retain_graph=True)[0] for pred in sampled_pred])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    x   est_mean   est_std         t    pvalue\n",
      "0                time   0.021465   1.79999  0.011925  0.495243\n",
      "1      log_num_tweets   0.297033   2.61566  0.113559  0.454794\n",
      "2             beta -2   0.551435   2.58243  0.213533  0.415455\n",
      "3             beta -3   1.874428   5.91589  0.316846  0.375680\n",
      "4             beta -4   1.328554   5.20095  0.255444  0.399190\n",
      "5             beta -5   1.461899   5.79166  0.252414  0.400360\n",
      "6             beta -6   2.128063   3.79795  0.560319  0.287631\n",
      "7              beta 0   3.488480   4.95499  0.704034  0.240706\n",
      "8              beta 1   4.633176   4.63900  0.998745  0.158959\n",
      "9             beta 10   6.279680   5.86278  1.071110  0.142060\n",
      "10            beta 11   3.861292   2.48533  1.553633  0.060136\n",
      "11             beta 2   3.535981   3.49856  1.010696  0.156081\n",
      "12             beta 3   4.474892   3.31384  1.350364  0.088450\n",
      "13             beta 4   3.196683   2.28952  1.396224  0.081324\n",
      "14             beta 5   3.399454   2.67746  1.269656  0.102104\n",
      "15             beta 6   4.159886   3.25331  1.278663  0.100508\n",
      "16             beta 7   5.449884   2.66379  2.045914  0.020382\n",
      "17             beta 8   2.798574   3.16668  0.883756  0.188414\n",
      "18             beta 9   3.609877   1.63431  2.208808  0.013594\n",
      "19  beta month_before   1.886267   4.22132  0.446843  0.327494\n",
      "20          beta post   1.502518   5.17568  0.290304  0.385792\n",
      "21           beta pre -10.502754  13.28911 -0.790328  0.214668\n"
     ]
    }
   ],
   "source": [
    "est_std = np.sqrt(sampled_dydtest_x.mean(1).var(0) + \\\n",
    "                  sampled_dydtest_x.var(1).mean(0)).round(decimals=5)\n",
    "covariate_names = [\"time\",\"log_num_tweets\"]\n",
    "for tmp in pd.get_dummies(data['lead_lags']).columns.tolist():\n",
    "    covariate_names.append(\"beta \"+tmp)\n",
    "results = pd.DataFrame({\"x\": covariate_names, \\\n",
    "                        'est_mean': x_grad.mean(axis=0)[1:],\n",
    "                        'est_std': est_std[1:]})\n",
    "results[\"t\"] = results['est_mean'].values/results['est_std'].values\n",
    "results[\"pvalue\"] = 1 - norm.cdf(np.abs(results[\"t\"].values))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "plot_est_mean = []\n",
    "plot_est_std = []\n",
    "for i in range(-6,12):\n",
    "    if i==-7:\n",
    "        plot_est_mean.append(results[results.x==(\"beta pre\")].est_mean.values[0])\n",
    "        plot_est_std.append(results[results.x==(\"beta pre\")].est_std.values[0])\n",
    "    elif i==12:\n",
    "        plot_est_mean.append(results[results.x==(\"beta post\")].est_mean.values[0])\n",
    "        plot_est_std.append(results[results.x==(\"beta post\")].est_std.values[0])\n",
    "    elif i==-1:\n",
    "        plot_est_mean.append(results[results.x==(\"beta month_before\")].est_mean.values[0])\n",
    "        plot_est_std.append(results[results.x==(\"beta month_before\")].est_std.values[0])\n",
    "    else:\n",
    "        plot_est_mean.append(results[results.x==(\"beta \"+str(i))].est_mean.values[0])\n",
    "        plot_est_std.append(results[results.x==(\"beta \"+str(i))].est_std.values[0])\n",
    "ax.errorbar([i  for i in range(-6,12)], np.array(plot_est_mean)-plot_est_mean[5], yerr=plot_est_std, fmt=\"k\", ecolor=\"k\")\n",
    "ax.axvline(x=-0.5, ls=\"--\", color=\"gray\")\n",
    "ax.axhline(y=0, ls=\"--\", color=\"gray\")\n",
    "ax.set_xlabel(\"Months Since Exile\")\n",
    "ax.set_ylabel(\"Change in % Tweets on Harsh Criticism\")\n",
    "ax.set_xticks(range(-5,10,5))\n",
    "ax.spines['top'].set_color('gray')\n",
    "ax.spines['right'].set_color('gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we estimate effect of exile on percentage of harsh criticism for left panel of figure 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data = pd.read_csv(\"./data/exile.csv\")\n",
    "data = data[[\"perc_harsh_criticism\", \"exile\", \"month\",\"num_tweets\", \"date_of_exile\", \"actor.id\"]]\n",
    "\n",
    "def diff_month(d1, d2):\n",
    "    d1 = datetime.strptime(d1,\"%Y-%m-%d\")\n",
    "    d2 = datetime.strptime(d2,\"%Y-%m-%d\")\n",
    "    return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
    "\n",
    "# xs: unit id, month, log_num_tweets, exile\n",
    "xs = data.month.apply(lambda x: diff_month(x,\"2013-01-01\"))\n",
    "xs = torch.tensor(np.hstack((data[\"actor.id\"].astype('category').cat.codes.values.reshape((-1,1)),\\\n",
    "                    xs.values.reshape((-1,1)),\n",
    "                    np.log(data.num_tweets.values+1).reshape((-1,1)), \\\n",
    "                    data['exile'].values.reshape((-1,1))==\"yes\")))\n",
    "ys = torch.tensor(data.perc_harsh_criticism.values).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.means import Mean\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=False)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        # linear mean\n",
    "        self.mean_module = LinearMean(input_size=(inducing_points.size(1)-2), bias=False)\n",
    "        self.covar_module = ScaleKernel(RBFKernel(ard_num_dims=(inducing_points.size(1)-2)))\n",
    "        self.t_covar_module = ScaleKernel(RBFKernel(active_dims=[0])*RBFKernel(active_dims=[1,2]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x[:,2:])\n",
    "        covar_x =  self.t_covar_module(x) # self.covar_module(x[:,2:]) +\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 2 - Loss: 521.803\n",
      "Epoch 1 Iter 3 - Loss: 477.685\n",
      "Epoch 1 Iter 4 - Loss: 457.833\n",
      "Epoch 1 Iter 5 - Loss: 449.835\n",
      "Epoch 1 Iter 6 - Loss: 409.039\n",
      "Epoch 1 Iter 7 - Loss: 405.476\n",
      "Epoch 1 Iter 8 - Loss: 377.295\n",
      "Epoch 1 Iter 9 - Loss: 374.548\n",
      "Epoch 1 Iter 10 - Loss: 344.834\n",
      "Epoch 1 Iter 11 - Loss: 339.551\n",
      "Epoch 1 Iter 12 - Loss: 304.868\n",
      "Epoch 1 Iter 13 - Loss: 301.306\n",
      "Epoch 1 Iter 14 - Loss: 308.258\n",
      "Epoch 1 Iter 15 - Loss: 281.179\n",
      "Epoch 1 Iter 16 - Loss: 287.761\n",
      "Epoch 1 Iter 17 - Loss: 273.210\n",
      "Epoch 1 Iter 18 - Loss: 272.289\n",
      "Epoch 1 Iter 19 - Loss: 261.264\n",
      "Epoch 1 Iter 20 - Loss: 258.386\n",
      "Epoch 1 Iter 21 - Loss: 232.067\n",
      "Epoch 1 Iter 22 - Loss: 230.230\n",
      "Epoch 1 Iter 23 - Loss: 190.588\n",
      "Epoch 2 Iter 2 - Loss: 195.023\n",
      "Epoch 2 Iter 3 - Loss: 205.237\n",
      "Epoch 2 Iter 4 - Loss: 210.496\n",
      "Epoch 2 Iter 5 - Loss: 186.999\n",
      "Epoch 2 Iter 6 - Loss: 181.785\n",
      "Epoch 2 Iter 7 - Loss: 185.955\n",
      "Epoch 2 Iter 8 - Loss: 185.339\n",
      "Epoch 2 Iter 9 - Loss: 191.544\n",
      "Epoch 2 Iter 10 - Loss: 164.100\n",
      "Epoch 2 Iter 11 - Loss: 166.658\n",
      "Epoch 2 Iter 12 - Loss: 155.477\n",
      "Epoch 2 Iter 13 - Loss: 163.328\n",
      "Epoch 2 Iter 14 - Loss: 158.054\n",
      "Epoch 2 Iter 15 - Loss: 141.372\n",
      "Epoch 2 Iter 16 - Loss: 155.654\n",
      "Epoch 2 Iter 17 - Loss: 149.114\n",
      "Epoch 2 Iter 18 - Loss: 139.729\n",
      "Epoch 2 Iter 19 - Loss: 141.944\n",
      "Epoch 2 Iter 20 - Loss: 130.370\n",
      "Epoch 2 Iter 21 - Loss: 134.488\n",
      "Epoch 2 Iter 22 - Loss: 130.380\n",
      "Epoch 2 Iter 23 - Loss: 117.807\n",
      "Epoch 3 Iter 2 - Loss: 123.207\n",
      "Epoch 3 Iter 3 - Loss: 126.914\n",
      "Epoch 3 Iter 4 - Loss: 123.041\n",
      "Epoch 3 Iter 5 - Loss: 108.163\n",
      "Epoch 3 Iter 6 - Loss: 119.720\n",
      "Epoch 3 Iter 7 - Loss: 114.083\n",
      "Epoch 3 Iter 8 - Loss: 105.647\n",
      "Epoch 3 Iter 9 - Loss: 106.338\n",
      "Epoch 3 Iter 10 - Loss: 107.517\n",
      "Epoch 3 Iter 11 - Loss: 104.732\n",
      "Epoch 3 Iter 12 - Loss: 109.986\n",
      "Epoch 3 Iter 13 - Loss: 94.372\n",
      "Epoch 3 Iter 14 - Loss: 96.195\n",
      "Epoch 3 Iter 15 - Loss: 98.656\n",
      "Epoch 3 Iter 16 - Loss: 93.474\n",
      "Epoch 3 Iter 17 - Loss: 90.977\n",
      "Epoch 3 Iter 18 - Loss: 98.191\n",
      "Epoch 3 Iter 19 - Loss: 107.709\n",
      "Epoch 3 Iter 20 - Loss: 98.046\n",
      "Epoch 3 Iter 21 - Loss: 85.676\n",
      "Epoch 3 Iter 22 - Loss: 88.005\n",
      "Epoch 3 Iter 23 - Loss: 101.808\n",
      "Epoch 4 Iter 2 - Loss: 77.410\n",
      "Epoch 4 Iter 3 - Loss: 71.408\n",
      "Epoch 4 Iter 4 - Loss: 88.261\n",
      "Epoch 4 Iter 5 - Loss: 78.066\n",
      "Epoch 4 Iter 6 - Loss: 83.096\n",
      "Epoch 4 Iter 7 - Loss: 84.586\n",
      "Epoch 4 Iter 8 - Loss: 74.391\n",
      "Epoch 4 Iter 9 - Loss: 85.588\n",
      "Epoch 4 Iter 10 - Loss: 79.543\n",
      "Epoch 4 Iter 11 - Loss: 75.887\n",
      "Epoch 4 Iter 12 - Loss: 70.338\n",
      "Epoch 4 Iter 13 - Loss: 69.330\n",
      "Epoch 4 Iter 14 - Loss: 78.329\n",
      "Epoch 4 Iter 15 - Loss: 69.910\n",
      "Epoch 4 Iter 16 - Loss: 72.719\n",
      "Epoch 4 Iter 17 - Loss: 75.463\n",
      "Epoch 4 Iter 18 - Loss: 69.364\n",
      "Epoch 4 Iter 19 - Loss: 74.724\n",
      "Epoch 4 Iter 20 - Loss: 61.050\n",
      "Epoch 4 Iter 21 - Loss: 72.658\n",
      "Epoch 4 Iter 22 - Loss: 66.450\n",
      "Epoch 4 Iter 23 - Loss: 36.034\n",
      "Epoch 5 Iter 2 - Loss: 67.311\n",
      "Epoch 5 Iter 3 - Loss: 69.837\n",
      "Epoch 5 Iter 4 - Loss: 63.178\n",
      "Epoch 5 Iter 5 - Loss: 63.735\n",
      "Epoch 5 Iter 6 - Loss: 58.825\n",
      "Epoch 5 Iter 7 - Loss: 60.431\n",
      "Epoch 5 Iter 8 - Loss: 53.693\n",
      "Epoch 5 Iter 9 - Loss: 58.965\n",
      "Epoch 5 Iter 10 - Loss: 56.661\n",
      "Epoch 5 Iter 11 - Loss: 60.139\n",
      "Epoch 5 Iter 12 - Loss: 55.354\n",
      "Epoch 5 Iter 13 - Loss: 55.941\n",
      "Epoch 5 Iter 14 - Loss: 58.879\n",
      "Epoch 5 Iter 15 - Loss: 65.824\n",
      "Epoch 5 Iter 16 - Loss: 57.778\n",
      "Epoch 5 Iter 17 - Loss: 54.425\n",
      "Epoch 5 Iter 18 - Loss: 53.531\n",
      "Epoch 5 Iter 19 - Loss: 52.330\n",
      "Epoch 5 Iter 20 - Loss: 52.262\n",
      "Epoch 5 Iter 21 - Loss: 51.638\n",
      "Epoch 5 Iter 22 - Loss: 53.190\n",
      "Epoch 5 Iter 23 - Loss: 47.919\n",
      "Epoch 6 Iter 2 - Loss: 56.011\n",
      "Epoch 6 Iter 3 - Loss: 43.323\n",
      "Epoch 6 Iter 4 - Loss: 46.667\n",
      "Epoch 6 Iter 5 - Loss: 50.262\n",
      "Epoch 6 Iter 6 - Loss: 47.511\n",
      "Epoch 6 Iter 7 - Loss: 43.605\n",
      "Epoch 6 Iter 8 - Loss: 46.231\n",
      "Epoch 6 Iter 9 - Loss: 52.005\n",
      "Epoch 6 Iter 10 - Loss: 50.553\n",
      "Epoch 6 Iter 11 - Loss: 47.879\n",
      "Epoch 6 Iter 12 - Loss: 44.777\n",
      "Epoch 6 Iter 13 - Loss: 48.495\n",
      "Epoch 6 Iter 14 - Loss: 44.934\n",
      "Epoch 6 Iter 15 - Loss: 50.065\n",
      "Epoch 6 Iter 16 - Loss: 49.393\n",
      "Epoch 6 Iter 17 - Loss: 46.564\n",
      "Epoch 6 Iter 18 - Loss: 46.655\n",
      "Epoch 6 Iter 19 - Loss: 52.003\n",
      "Epoch 6 Iter 20 - Loss: 42.655\n",
      "Epoch 6 Iter 21 - Loss: 45.482\n",
      "Epoch 6 Iter 22 - Loss: 44.251\n",
      "Epoch 6 Iter 23 - Loss: 34.557\n",
      "Epoch 7 Iter 2 - Loss: 48.011\n",
      "Epoch 7 Iter 3 - Loss: 37.942\n",
      "Epoch 7 Iter 4 - Loss: 37.646\n",
      "Epoch 7 Iter 5 - Loss: 43.533\n",
      "Epoch 7 Iter 6 - Loss: 49.075\n",
      "Epoch 7 Iter 7 - Loss: 39.628\n",
      "Epoch 7 Iter 8 - Loss: 44.757\n",
      "Epoch 7 Iter 9 - Loss: 42.315\n",
      "Epoch 7 Iter 10 - Loss: 41.261\n",
      "Epoch 7 Iter 11 - Loss: 34.467\n",
      "Epoch 7 Iter 12 - Loss: 34.431\n",
      "Epoch 7 Iter 13 - Loss: 45.336\n",
      "Epoch 7 Iter 14 - Loss: 42.266\n",
      "Epoch 7 Iter 15 - Loss: 36.955\n",
      "Epoch 7 Iter 16 - Loss: 35.904\n",
      "Epoch 7 Iter 17 - Loss: 42.248\n",
      "Epoch 7 Iter 18 - Loss: 40.985\n",
      "Epoch 7 Iter 19 - Loss: 40.466\n",
      "Epoch 7 Iter 20 - Loss: 38.324\n",
      "Epoch 7 Iter 21 - Loss: 37.068\n",
      "Epoch 7 Iter 22 - Loss: 43.259\n",
      "Epoch 7 Iter 23 - Loss: 23.518\n",
      "Epoch 8 Iter 2 - Loss: 35.924\n",
      "Epoch 8 Iter 3 - Loss: 34.362\n",
      "Epoch 8 Iter 4 - Loss: 32.700\n",
      "Epoch 8 Iter 5 - Loss: 36.885\n",
      "Epoch 8 Iter 6 - Loss: 36.145\n",
      "Epoch 8 Iter 7 - Loss: 36.231\n",
      "Epoch 8 Iter 8 - Loss: 36.949\n",
      "Epoch 8 Iter 9 - Loss: 36.035\n",
      "Epoch 8 Iter 10 - Loss: 35.943\n",
      "Epoch 8 Iter 11 - Loss: 43.385\n",
      "Epoch 8 Iter 12 - Loss: 36.114\n",
      "Epoch 8 Iter 13 - Loss: 33.567\n",
      "Epoch 8 Iter 14 - Loss: 43.389\n",
      "Epoch 8 Iter 15 - Loss: 34.614\n",
      "Epoch 8 Iter 16 - Loss: 37.231\n",
      "Epoch 8 Iter 17 - Loss: 33.633\n",
      "Epoch 8 Iter 18 - Loss: 33.463\n",
      "Epoch 8 Iter 19 - Loss: 33.755\n",
      "Epoch 8 Iter 20 - Loss: 38.880\n",
      "Epoch 8 Iter 21 - Loss: 26.148\n",
      "Epoch 8 Iter 22 - Loss: 31.784\n",
      "Epoch 8 Iter 23 - Loss: 33.053\n",
      "Epoch 9 Iter 2 - Loss: 28.924\n",
      "Epoch 9 Iter 3 - Loss: 38.170\n",
      "Epoch 9 Iter 4 - Loss: 36.464\n",
      "Epoch 9 Iter 5 - Loss: 35.850\n",
      "Epoch 9 Iter 6 - Loss: 36.369\n",
      "Epoch 9 Iter 7 - Loss: 28.700\n",
      "Epoch 9 Iter 8 - Loss: 36.098\n",
      "Epoch 9 Iter 9 - Loss: 33.895\n",
      "Epoch 9 Iter 10 - Loss: 31.275\n",
      "Epoch 9 Iter 11 - Loss: 33.576\n",
      "Epoch 9 Iter 12 - Loss: 29.119\n",
      "Epoch 9 Iter 13 - Loss: 26.486\n",
      "Epoch 9 Iter 14 - Loss: 34.977\n",
      "Epoch 9 Iter 15 - Loss: 28.967\n",
      "Epoch 9 Iter 16 - Loss: 32.695\n",
      "Epoch 9 Iter 17 - Loss: 28.236\n",
      "Epoch 9 Iter 18 - Loss: 28.566\n",
      "Epoch 9 Iter 19 - Loss: 33.697\n",
      "Epoch 9 Iter 20 - Loss: 32.379\n",
      "Epoch 9 Iter 21 - Loss: 33.223\n",
      "Epoch 9 Iter 22 - Loss: 30.476\n",
      "Epoch 9 Iter 23 - Loss: 23.018\n",
      "Epoch 10 Iter 2 - Loss: 30.036\n",
      "Epoch 10 Iter 3 - Loss: 26.614\n",
      "Epoch 10 Iter 4 - Loss: 26.539\n",
      "Epoch 10 Iter 5 - Loss: 29.825\n",
      "Epoch 10 Iter 6 - Loss: 30.960\n",
      "Epoch 10 Iter 7 - Loss: 27.194\n",
      "Epoch 10 Iter 8 - Loss: 29.147\n",
      "Epoch 10 Iter 9 - Loss: 33.905\n",
      "Epoch 10 Iter 10 - Loss: 32.440\n",
      "Epoch 10 Iter 11 - Loss: 30.556\n",
      "Epoch 10 Iter 12 - Loss: 30.396\n",
      "Epoch 10 Iter 13 - Loss: 26.748\n",
      "Epoch 10 Iter 14 - Loss: 27.454\n",
      "Epoch 10 Iter 15 - Loss: 29.630\n",
      "Epoch 10 Iter 16 - Loss: 31.082\n",
      "Epoch 10 Iter 17 - Loss: 33.793\n",
      "Epoch 10 Iter 18 - Loss: 28.378\n",
      "Epoch 10 Iter 19 - Loss: 29.097\n",
      "Epoch 10 Iter 20 - Loss: 27.597\n",
      "Epoch 10 Iter 21 - Loss: 26.103\n",
      "Epoch 10 Iter 22 - Loss: 28.453\n",
      "Epoch 10 Iter 23 - Loss: 24.498\n"
     ]
    }
   ],
   "source": [
    "inducing_points = xs[np.random.choice(xs.size(0),1000,replace=False),:]\n",
    "model = GPModel(inducing_points=inducing_points).double()\n",
    "likelihood = GaussianLikelihood().double()\n",
    "\n",
    "train_dataset = TensorDataset(xs, ys)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# initialize model parameters\n",
    "model.t_covar_module.base_kernel.kernels[0].raw_lengthscale.require_grad = False\n",
    "model.t_covar_module.base_kernel.kernels[0].lengthscale = 0.1\n",
    "model.t_covar_module.outputscale = 1.\n",
    "likelihood.noise = 1.\n",
    "\n",
    "# train model\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': list(set(model.parameters()) - \\\n",
    "                {model.t_covar_module.base_kernel.kernels[0].raw_lengthscale,\\\n",
    "                model.t_covar_module.raw_outputscale})},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.05)\n",
    "\n",
    "# \"Loss\" for GPs\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=ys.size(0))\n",
    "\n",
    "num_epochs = 10\n",
    "for i in range(num_epochs):\n",
    "    for j, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = -mll(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if j % 50:\n",
    "            print('Epoch %d Iter %d - Loss: %.3f' % (i + 1, j+1, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model and likelihood to evaluation mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out = model(xs)\n",
    "    mu_f = out.mean.numpy()\n",
    "    lower, upper = out.confidence_region()\n",
    "\n",
    "# store results\n",
    "results = pd.DataFrame({\"gpr_mean\":mu_f})\n",
    "results['true_y'] = ys\n",
    "results['gpr_lwr'] = lower\n",
    "results['gpr_upr'] = upper\n",
    "results['month'] = xs[:,1].numpy().astype(int)\n",
    "results['unit'] = xs[:,0].numpy().astype(int)\n",
    "results.to_csv(\"./results/exile_fitted_gpr.csv\",index=False) #save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# number of empirically sample \n",
    "n_samples = 100\n",
    "x_grad = np.zeros((xs.size(0),xs.size(1)))\n",
    "sampled_dydtest_x = np.zeros((n_samples, xs.size(0),xs.size(1)))\n",
    "\n",
    "# we proceed in small batches of size 100 for speed up\n",
    "for i in range(xs.size(0)//100):\n",
    "    with gpytorch.settings.fast_pred_var():\n",
    "        test_x = xs[(i*100):(i*100+100)].clone().detach().requires_grad_(True)\n",
    "        observed_pred = model(test_x)\n",
    "        dydtest_x = torch.autograd.grad(observed_pred.mean.sum(), test_x, retain_graph=True)[0]\n",
    "        x_grad[(i*100):(i*100+100)] = dydtest_x\n",
    "\n",
    "        sampled_pred = observed_pred.rsample(torch.Size([n_samples]))\n",
    "        sampled_dydtest_x[:,(i*100):(i*100+100),:] = torch.stack([torch.autograd.grad(pred.sum(), \\\n",
    "                                    test_x, retain_graph=True)[0] for pred in sampled_pred])\n",
    "        \n",
    "# last 100 rows\n",
    "with gpytorch.settings.fast_pred_var():\n",
    "    test_x = xs[(100*i+100):].clone().detach().requires_grad_(True)\n",
    "    observed_pred = model(test_x)\n",
    "    dydtest_x = torch.autograd.grad(observed_pred.mean.sum(), test_x, retain_graph=True)[0]\n",
    "    x_grad[(100*i+100):] = dydtest_x\n",
    "\n",
    "    sampled_pred = observed_pred.rsample(torch.Size([n_samples]))\n",
    "    sampled_dydtest_x[:,(100*i+100):,:] = torch.stack([torch.autograd.grad(pred.sum(),\\\n",
    "                                     test_x, retain_graph=True)[0] for pred in sampled_pred])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                x  est_mean  est_std         t    pvalue\n",
      "0            time  0.012651  2.30062  0.005499  0.497806\n",
      "1  log_num_tweets  1.385515  2.89643  0.478353  0.316200\n"
     ]
    }
   ],
   "source": [
    "est_std = np.sqrt(sampled_dydtest_x.mean(1).var(0) + \\\n",
    "                  sampled_dydtest_x.var(1).mean(0)).round(decimals=5)\n",
    "covariate_names = [\"time\",\"log_num_tweets\"]\n",
    "results = pd.DataFrame({\"x\": covariate_names, \\\n",
    "                        'est_mean': x_grad.mean(axis=0)[1:3],\n",
    "                        'est_std': est_std[1:3]})\n",
    "results[\"t\"] = results['est_mean'].values/results['est_std'].values\n",
    "results[\"pvalue\"] = 1 - norm.cdf(np.abs(results[\"t\"].values))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: 5.286 +- 0.031\n",
      "\n",
      "model evidence: -542.339 \n",
      "\n",
      "BIC: 1134.822 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# copy training tesnor to test tensors and set exile to 1 and 0\n",
    "test_x1 = xs.clone().detach().requires_grad_(False)\n",
    "test_x1[:,3] = 1\n",
    "test_x0 = xs.clone().detach().requires_grad_(False)\n",
    "test_x0[:,3] = 0\n",
    "\n",
    "# in eval mode the forward() function returns posterioir\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out1 = likelihood(model(test_x1))\n",
    "    out0 = likelihood(model(test_x0))\n",
    "\n",
    "# compute ATE and its uncertainty\n",
    "effect = out1.mean.numpy().mean() - out0.mean.numpy().mean()\n",
    "effect_std = np.sqrt((out1.mean.numpy().mean()+out0.mean.numpy().mean())) / np.sqrt(xs.size()[0])\n",
    "BIC = (1+1+2+1)*\\\n",
    "    torch.log(torch.tensor(xs.size()[0])) + 2*loss*xs.size(0)/1024\n",
    "print(\"ATE: {:0.3f} +- {:0.3f}\\n\".format(effect, effect_std))\n",
    "print(\"model evidence: {:0.3f} \\n\".format(-loss*xs.size(0)/1024))\n",
    "print(\"BIC: {:0.3f} \\n\".format(BIC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
