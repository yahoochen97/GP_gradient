{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exile on Online Opposition\n",
    "\n",
    "Examine the effect of exile on percentage of tweets' harsh criticism of Venezuela government.\n",
    "\n",
    "ESBERG, J., & SIEGEL, A. (2023). How Exile Shapes Online Opposition: Evidence from Venezuela. American Political Science Review, 117(4), 1361-1378."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gpytoch and other packages\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gpytorch\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt\n",
    "from gpytorch.means import ZeroMean, LinearMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from datetime import datetime\n",
    "\n",
    "from gpytorch.means import Mean\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "from typing import Optional, Tuple\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "num_inducing = 3000\n",
    "batch_size = 256\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we build a Gaussian process regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_month(d1, d2):\n",
    "    d1 = datetime.strptime(d1,\"%Y-%m-%d\")\n",
    "    d2 = datetime.strptime(d2,\"%Y-%m-%d\")\n",
    "    return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
    "\n",
    "def to_month(d1):\n",
    "    return datetime(2013 + int(d1 / 12), ((1 +d1) % 12) + 1, 1)\n",
    "\n",
    "class ConstantVectorMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(self, d=1, prior=None, batch_shape=torch.Size(), **kwargs):\n",
    "        super().__init__()\n",
    "        self.batch_shape = batch_shape\n",
    "        self.register_parameter(name=\"constantvector\",\\\n",
    "                 parameter=torch.nn.Parameter(torch.zeros(*batch_shape, d)))\n",
    "        if prior is not None:\n",
    "            self.register_prior(\"mean_prior\", prior, \"constantvector\")\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.constantvector[input.int().reshape((-1,)).tolist()]\n",
    "    \n",
    "class MaskMean(gpytorch.means.mean.Mean):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_mean: gpytorch.means.mean.Mean,\n",
    "        active_dims: Optional[Tuple[int, ...]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if active_dims is not None and not torch.is_tensor(active_dims):\n",
    "            active_dims = torch.tensor(active_dims, dtype=torch.long)\n",
    "        self.active_dims = active_dims\n",
    "        self.base_mean = base_mean\n",
    "    \n",
    "    def forward(self, x, **params):\n",
    "        return self.base_mean.forward(x.index_select(-1, self.active_dims), **params)\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points, unit_num):\n",
    "        self.unit_num = unit_num\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=False)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "\n",
    "        # linear mean\n",
    "        self.mean_module = LinearMean(input_size=(2), bias=False)\n",
    "        self.unit_mean = torch.nn.ModuleList([LinearMean(input_size=(1),bias=True) for _ in range(unit_num)])\n",
    "        self.covar_module = ScaleKernel(RBFKernel(ard_num_dims=(2), active_dims=[2,3]))\n",
    "        self.t_covar_module = ScaleKernel(RBFKernel(active_dims=[0])*RBFKernel(active_dims=[1]))\n",
    "        self.g_covar_module = ScaleKernel(RBFKernel(active_dims=[1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x[:,2:]) \n",
    "        for i in range(self.unit_num):\n",
    "            mean_x[x[:,0]==i] += self.unit_mean[i](x[i,1].reshape((-1,1)))\n",
    "        covar_x =  self.covar_module(x) + self.t_covar_module(x)  + self.g_covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 1 - Loss: 197.562\n",
      "Epoch 1 Iter 51 - Loss: 12.582\n",
      "Epoch 2 Iter 1 - Loss: 8.022\n",
      "Epoch 2 Iter 51 - Loss: 6.670\n",
      "Epoch 3 Iter 1 - Loss: 7.135\n",
      "Epoch 3 Iter 51 - Loss: 8.003\n",
      "Epoch 4 Iter 1 - Loss: 7.926\n",
      "Epoch 4 Iter 51 - Loss: 6.196\n",
      "Epoch 5 Iter 1 - Loss: 6.049\n",
      "Epoch 5 Iter 51 - Loss: 5.577\n",
      "Epoch 6 Iter 1 - Loss: 5.691\n",
      "Epoch 6 Iter 51 - Loss: 5.827\n",
      "Epoch 7 Iter 1 - Loss: 6.821\n",
      "Epoch 7 Iter 51 - Loss: 5.194\n",
      "Epoch 8 Iter 1 - Loss: 5.854\n",
      "Epoch 8 Iter 51 - Loss: 6.322\n",
      "Epoch 9 Iter 1 - Loss: 5.469\n",
      "Epoch 9 Iter 51 - Loss: 6.256\n",
      "Epoch 10 Iter 1 - Loss: 4.770\n",
      "Epoch 10 Iter 51 - Loss: 5.055\n",
      "Epoch 11 Iter 1 - Loss: 5.630\n",
      "Epoch 11 Iter 51 - Loss: 5.843\n",
      "Epoch 12 Iter 1 - Loss: 4.927\n",
      "Epoch 12 Iter 51 - Loss: 6.528\n",
      "Epoch 13 Iter 1 - Loss: 4.890\n",
      "Epoch 13 Iter 51 - Loss: 5.080\n",
      "Epoch 14 Iter 1 - Loss: 4.886\n",
      "Epoch 14 Iter 51 - Loss: 5.146\n",
      "Epoch 15 Iter 1 - Loss: 4.715\n",
      "Epoch 15 Iter 51 - Loss: 4.876\n",
      "Epoch 16 Iter 1 - Loss: 6.272\n",
      "Epoch 16 Iter 51 - Loss: 4.792\n",
      "Epoch 17 Iter 1 - Loss: 5.779\n",
      "Epoch 17 Iter 51 - Loss: 4.884\n",
      "Epoch 18 Iter 1 - Loss: 5.391\n",
      "Epoch 18 Iter 51 - Loss: 5.846\n",
      "Epoch 19 Iter 1 - Loss: 4.707\n",
      "Epoch 19 Iter 51 - Loss: 5.748\n",
      "Epoch 20 Iter 1 - Loss: 5.102\n",
      "Epoch 20 Iter 51 - Loss: 6.239\n",
      "Epoch 21 Iter 1 - Loss: 4.429\n",
      "Epoch 21 Iter 51 - Loss: 6.045\n",
      "Epoch 22 Iter 1 - Loss: 5.372\n",
      "Epoch 22 Iter 51 - Loss: 5.352\n",
      "Epoch 23 Iter 1 - Loss: 4.756\n",
      "Epoch 23 Iter 51 - Loss: 4.332\n",
      "Epoch 24 Iter 1 - Loss: 4.885\n",
      "Epoch 24 Iter 51 - Loss: 4.808\n",
      "Epoch 25 Iter 1 - Loss: 4.194\n",
      "Epoch 25 Iter 51 - Loss: 5.240\n",
      "Epoch 26 Iter 1 - Loss: 4.452\n",
      "Epoch 26 Iter 51 - Loss: 4.086\n",
      "Epoch 27 Iter 1 - Loss: 5.110\n",
      "Epoch 27 Iter 51 - Loss: 4.205\n",
      "Epoch 28 Iter 1 - Loss: 4.940\n",
      "Epoch 28 Iter 51 - Loss: 4.349\n",
      "Epoch 29 Iter 1 - Loss: 4.534\n",
      "Epoch 29 Iter 51 - Loss: 4.449\n",
      "Epoch 30 Iter 1 - Loss: 4.337\n",
      "Epoch 30 Iter 51 - Loss: 4.722\n",
      "Epoch 31 Iter 1 - Loss: 4.455\n",
      "Epoch 31 Iter 51 - Loss: 4.116\n",
      "Epoch 32 Iter 1 - Loss: 4.595\n",
      "Epoch 32 Iter 51 - Loss: 4.278\n",
      "Epoch 33 Iter 1 - Loss: 4.293\n",
      "Epoch 33 Iter 51 - Loss: 4.206\n",
      "Epoch 34 Iter 1 - Loss: 4.598\n",
      "Epoch 34 Iter 51 - Loss: 4.761\n",
      "Epoch 35 Iter 1 - Loss: 5.017\n",
      "Epoch 35 Iter 51 - Loss: 4.346\n",
      "Epoch 36 Iter 1 - Loss: 4.374\n",
      "Epoch 36 Iter 51 - Loss: 4.738\n",
      "Epoch 37 Iter 1 - Loss: 3.990\n",
      "Epoch 37 Iter 51 - Loss: 4.123\n",
      "Epoch 38 Iter 1 - Loss: 4.898\n",
      "Epoch 38 Iter 51 - Loss: 4.732\n",
      "Epoch 39 Iter 1 - Loss: 4.431\n",
      "Epoch 39 Iter 51 - Loss: 3.968\n",
      "Epoch 40 Iter 1 - Loss: 4.354\n",
      "Epoch 40 Iter 51 - Loss: 4.564\n",
      "Epoch 41 Iter 1 - Loss: 4.029\n",
      "Epoch 41 Iter 51 - Loss: 4.290\n",
      "Epoch 42 Iter 1 - Loss: 3.982\n",
      "Epoch 42 Iter 51 - Loss: 4.200\n",
      "Epoch 43 Iter 1 - Loss: 4.328\n",
      "Epoch 43 Iter 51 - Loss: 4.280\n",
      "Epoch 44 Iter 1 - Loss: 5.016\n",
      "Epoch 44 Iter 51 - Loss: 4.292\n",
      "Epoch 45 Iter 1 - Loss: 4.401\n",
      "Epoch 45 Iter 51 - Loss: 4.237\n",
      "Epoch 46 Iter 1 - Loss: 4.408\n",
      "Epoch 46 Iter 51 - Loss: 4.196\n",
      "Epoch 47 Iter 1 - Loss: 4.916\n",
      "Epoch 47 Iter 51 - Loss: 4.275\n",
      "Epoch 48 Iter 1 - Loss: 4.171\n",
      "Epoch 48 Iter 51 - Loss: 4.289\n",
      "Epoch 49 Iter 1 - Loss: 4.313\n",
      "Epoch 49 Iter 51 - Loss: 4.643\n",
      "Epoch 50 Iter 1 - Loss: 4.266\n",
      "Epoch 50 Iter 51 - Loss: 4.078\n",
      "Epoch 51 Iter 1 - Loss: 4.361\n",
      "Epoch 51 Iter 51 - Loss: 4.400\n",
      "Epoch 52 Iter 1 - Loss: 4.113\n",
      "Epoch 52 Iter 51 - Loss: 4.233\n",
      "Epoch 53 Iter 1 - Loss: 3.989\n",
      "Epoch 53 Iter 51 - Loss: 4.173\n",
      "Epoch 54 Iter 1 - Loss: 3.928\n",
      "Epoch 54 Iter 51 - Loss: 4.193\n",
      "Epoch 55 Iter 1 - Loss: 3.962\n",
      "Epoch 55 Iter 51 - Loss: 4.607\n",
      "Epoch 56 Iter 1 - Loss: 4.277\n",
      "Epoch 56 Iter 51 - Loss: 4.374\n",
      "Epoch 57 Iter 1 - Loss: 4.149\n",
      "Epoch 57 Iter 51 - Loss: 4.002\n",
      "Epoch 58 Iter 1 - Loss: 3.830\n",
      "Epoch 58 Iter 51 - Loss: 3.886\n",
      "Epoch 59 Iter 1 - Loss: 3.994\n",
      "Epoch 59 Iter 51 - Loss: 4.552\n",
      "Epoch 60 Iter 1 - Loss: 4.123\n",
      "Epoch 60 Iter 51 - Loss: 4.452\n",
      "Epoch 61 Iter 1 - Loss: 3.958\n",
      "Epoch 61 Iter 51 - Loss: 4.060\n",
      "Epoch 62 Iter 1 - Loss: 3.922\n",
      "Epoch 62 Iter 51 - Loss: 4.648\n",
      "Epoch 63 Iter 1 - Loss: 4.252\n",
      "Epoch 63 Iter 51 - Loss: 3.761\n",
      "Epoch 64 Iter 1 - Loss: 4.101\n",
      "Epoch 64 Iter 51 - Loss: 4.164\n",
      "Epoch 65 Iter 1 - Loss: 4.206\n",
      "Epoch 65 Iter 51 - Loss: 4.227\n",
      "Epoch 66 Iter 1 - Loss: 4.252\n",
      "Epoch 66 Iter 51 - Loss: 4.075\n",
      "Epoch 67 Iter 1 - Loss: 4.029\n",
      "Epoch 67 Iter 51 - Loss: 4.162\n",
      "Epoch 68 Iter 1 - Loss: 4.289\n",
      "Epoch 68 Iter 51 - Loss: 4.031\n",
      "Epoch 69 Iter 1 - Loss: 3.960\n",
      "Epoch 69 Iter 51 - Loss: 4.298\n",
      "Epoch 70 Iter 1 - Loss: 4.159\n",
      "Epoch 70 Iter 51 - Loss: 4.308\n",
      "Epoch 71 Iter 1 - Loss: 4.458\n",
      "Epoch 71 Iter 51 - Loss: 4.324\n",
      "Epoch 72 Iter 1 - Loss: 3.864\n",
      "Epoch 72 Iter 51 - Loss: 4.403\n",
      "Epoch 73 Iter 1 - Loss: 4.194\n",
      "Epoch 73 Iter 51 - Loss: 4.034\n",
      "Epoch 74 Iter 1 - Loss: 4.088\n",
      "Epoch 74 Iter 51 - Loss: 3.862\n",
      "Epoch 75 Iter 1 - Loss: 4.115\n",
      "Epoch 75 Iter 51 - Loss: 4.267\n",
      "Epoch 76 Iter 1 - Loss: 3.678\n",
      "Epoch 76 Iter 51 - Loss: 4.057\n",
      "Epoch 77 Iter 1 - Loss: 4.101\n",
      "Epoch 77 Iter 51 - Loss: 4.167\n",
      "Epoch 78 Iter 1 - Loss: 3.867\n",
      "Epoch 78 Iter 51 - Loss: 3.930\n",
      "Epoch 79 Iter 1 - Loss: 3.998\n",
      "Epoch 79 Iter 51 - Loss: 3.798\n",
      "Epoch 80 Iter 1 - Loss: 4.090\n",
      "Epoch 80 Iter 51 - Loss: 4.186\n",
      "Epoch 81 Iter 1 - Loss: 4.091\n",
      "Epoch 81 Iter 51 - Loss: 4.300\n",
      "Epoch 82 Iter 1 - Loss: 4.001\n",
      "Epoch 82 Iter 51 - Loss: 4.368\n",
      "Epoch 83 Iter 1 - Loss: 3.748\n",
      "Epoch 83 Iter 51 - Loss: 4.207\n",
      "Epoch 84 Iter 1 - Loss: 3.987\n",
      "Epoch 84 Iter 51 - Loss: 3.695\n",
      "Epoch 85 Iter 1 - Loss: 4.199\n",
      "Epoch 85 Iter 51 - Loss: 4.247\n",
      "Epoch 86 Iter 1 - Loss: 4.177\n",
      "Epoch 86 Iter 51 - Loss: 3.966\n",
      "Epoch 87 Iter 1 - Loss: 4.053\n",
      "Epoch 87 Iter 51 - Loss: 3.993\n",
      "Epoch 88 Iter 1 - Loss: 4.057\n",
      "Epoch 88 Iter 51 - Loss: 4.007\n",
      "Epoch 89 Iter 1 - Loss: 4.169\n",
      "Epoch 89 Iter 51 - Loss: 4.146\n",
      "Epoch 90 Iter 1 - Loss: 3.863\n",
      "Epoch 90 Iter 51 - Loss: 3.739\n",
      "Epoch 91 Iter 1 - Loss: 3.689\n",
      "Epoch 91 Iter 51 - Loss: 3.957\n",
      "Epoch 92 Iter 1 - Loss: 3.928\n",
      "Epoch 92 Iter 51 - Loss: 4.135\n",
      "Epoch 93 Iter 1 - Loss: 3.985\n",
      "Epoch 93 Iter 51 - Loss: 4.011\n",
      "Epoch 94 Iter 1 - Loss: 3.747\n",
      "Epoch 94 Iter 51 - Loss: 3.908\n",
      "Epoch 95 Iter 1 - Loss: 3.908\n",
      "Epoch 95 Iter 51 - Loss: 3.769\n",
      "Epoch 96 Iter 1 - Loss: 3.879\n",
      "Epoch 96 Iter 51 - Loss: 3.718\n",
      "Epoch 97 Iter 1 - Loss: 3.834\n",
      "Epoch 97 Iter 51 - Loss: 4.209\n",
      "Epoch 98 Iter 1 - Loss: 4.002\n",
      "Epoch 98 Iter 51 - Loss: 4.017\n",
      "Epoch 99 Iter 1 - Loss: 3.974\n",
      "Epoch 99 Iter 51 - Loss: 4.191\n",
      "Epoch 100 Iter 1 - Loss: 3.916\n",
      "Epoch 100 Iter 51 - Loss: 3.745\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "Y_name = \"perc_harsh_criticism\" # \"perc_repression\"\n",
    "data = pd.read_csv(\"./data/exile.csv\")\n",
    "data = data[[Y_name, \"tweeted_exile\", \"month\",\"num_tweets\", \"actor.id\"]]\n",
    "\n",
    "# xs: unit id, month, log_num_tweets, tweeted_exile\n",
    "xs = data.month.apply(lambda x: diff_month(x,\"2013-01-01\"))\n",
    "xs = torch.tensor(np.array([data[\"actor.id\"].astype('category').cat.codes.values.reshape((-1,)),\\\n",
    "            xs.values.reshape((-1,)),\n",
    "            np.log(data.num_tweets.values+1).reshape((-1,)), \\\n",
    "            data['tweeted_exile'].values.reshape((-1,))]).T)\n",
    "# xs = torch.cat((xs, (xs[:, 1] * xs[:, -1]).reshape(-1,1)), dim=1)\n",
    "ys = torch.tensor(data[Y_name].values).double()\n",
    "\n",
    "to_unit = dict(enumerate(data[\"actor.id\"].astype('category').cat.categories))\n",
    "del data\n",
    "\n",
    "# define inducing points and learn\n",
    "inducing_points = xs[np.random.choice(xs.size(0),num_inducing,replace=False),:]\n",
    "# inducing_points = xs[xs[:,1] % 10==0]\n",
    "model = GPModel(inducing_points=inducing_points, unit_num=xs[:,0].unique().size()[0]).double()\n",
    "likelihood = GaussianLikelihood().double()\n",
    "del inducing_points\n",
    "\n",
    "train_dataset = TensorDataset(xs, ys)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "hypers = {\n",
    "'mean_module.weights': torch.tensor([0, 5]),\n",
    "'covar_module.outputscale': 9,\n",
    "'covar_module.base_kernel.lengthscale': torch.std(xs[:,2:4],axis=0),\n",
    "'t_covar_module.base_kernel.kernels.1.lengthscale': torch.tensor([12]),\n",
    "'t_covar_module.outputscale': 4,\n",
    "'g_covar_module.base_kernel.lengthscale': torch.tensor([24]),\n",
    "'g_covar_module.outputscale': 9\n",
    "}    \n",
    "\n",
    "model = model.initialize(**hypers)\n",
    "\n",
    "# initialize model parameters\n",
    "model.t_covar_module.base_kernel.kernels[0].raw_lengthscale.requires_grad_(False)\n",
    "model.t_covar_module.base_kernel.kernels[0].lengthscale = 0.01\n",
    "# model.t_covar_module.base_kernel.kernels[1].raw_lengthscale.requires_grad_(False)\n",
    "# model.covar_module.base_kernel.raw_lengthscale.requires_grad_(False)\n",
    "likelihood.noise = 9.\n",
    "\n",
    "# train model\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': list(set(model.parameters()) \\\n",
    "                - {model.t_covar_module.base_kernel.kernels[0].raw_lengthscale,\\\n",
    "                })},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=ys.size(0))\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    for j, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = -mll(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if j % 50 == 0:\n",
    "            print('Epoch %d Iter %d - Loss: %.3f' % (i + 1, j+1, loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate predictive values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model and likelihood to evaluation mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out = model(xs)\n",
    "    mll.combine_terms = True\n",
    "    loss = -mll(out, ys)\n",
    "    mu_f = out.mean.numpy()\n",
    "    lower, upper = out.confidence_region()\n",
    "\n",
    "# store results\n",
    "results = pd.DataFrame({\"gpr_mean\":mu_f})\n",
    "results['true_y'] = ys\n",
    "results['gpr_lwr'] = lower\n",
    "results['gpr_upr'] = upper\n",
    "results['month'] = np.array([to_month(x) for x in xs[:,1].numpy().astype(int)])\n",
    "results['unit'] = np.array([to_unit[x] for x in xs[:,0].numpy().astype(int)])\n",
    "results['exile'] = xs[:,3].numpy().astype(int)\n",
    "\n",
    "test_x0 = xs.clone().detach().requires_grad_(False)\n",
    "test_x0[:,3] = 0\n",
    "\n",
    "# in eval mode the forward() function returns posterioir\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out0 = model(test_x0)\n",
    "    lower, upper = out0.confidence_region()\n",
    "\n",
    "results['cf'] = out0.mean.numpy()\n",
    "results['cf_lower'] = lower\n",
    "results['cf_upper'] = upper\n",
    "\n",
    "if Y_name == \"perc_harsh_criticism\":\n",
    "    abbr = \"crit\"\n",
    "else:\n",
    "    abbr = \"repr\"\n",
    "results.to_csv(\"./results/exile_{}_fitted_gpr.csv\".format(abbr),index=False) #save to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: 5.216 +- 2.934\n",
      "\n",
      "model evidence: -87763.991 \n",
      "\n",
      "BIC: 175588.154 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# copy training tesnor to test tensors and set exile to 1 and 0\n",
    "test_x1 = xs.clone().detach().requires_grad_(False)\n",
    "test_x1[:,3] = 1\n",
    "test_x0 = xs.clone().detach().requires_grad_(False)\n",
    "test_x0[:,3] = 0\n",
    "\n",
    "# in eval mode the forward() function returns posterioir\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    out = model(xs)\n",
    "    mll.combine_terms = False\n",
    "    loss, _ , _ = mll(out, ys)\n",
    "    loss = -loss*out.event_shape[0]\n",
    "    out1 = model(test_x1)\n",
    "    out0 = model(test_x0)\n",
    "\n",
    "# compute ATE and its uncertainty\n",
    "effect = out1.mean.numpy()[xs[:,3]==1].mean() - out0.mean.numpy()[xs[:,3]==1].mean()\n",
    "effect_std = np.sqrt((out1.variance.detach().numpy()[xs[:,3]==1].mean()\\\n",
    "                    +out0.variance.detach().numpy()[xs[:,3]==1].mean()))\n",
    "BIC = (3+2+1)*\\\n",
    "    torch.log(torch.tensor(xs.size()[0])) + 2*loss # *xs.size(0)/batch_size\n",
    "print(\"ATE: {:0.3f} +- {:0.3f}\\n\".format(effect, effect_std))\n",
    "print(\"model evidence: {:0.3f} \\n\".format(-loss))\n",
    "print(\"BIC: {:0.3f} \\n\".format(BIC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
